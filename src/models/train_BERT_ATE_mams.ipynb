{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BERT_ATE import BERT_ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = '../data/mams_atsa.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>opinions</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[{'target': 'decor', 'polarity': 'negative', '...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[{'target': 'decor', 'polarity': 'negative', '...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[{'target': 'decor', 'polarity': 'negative', '...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[{'target': 'tables', 'polarity': 'neutral', '...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[{'target': 'tables', 'polarity': 'neutral', '...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                            opinions  \\\n",
       "0  [{'target': 'decor', 'polarity': 'negative', '...   \n",
       "1  [{'target': 'decor', 'polarity': 'negative', '...   \n",
       "2  [{'target': 'decor', 'polarity': 'negative', '...   \n",
       "3  [{'target': 'tables', 'polarity': 'neutral', '...   \n",
       "4  [{'target': 'tables', 'polarity': 'neutral', '...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "TRAIN_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_BERT_MODEL_VARIANT = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (11186, 4)\n",
      "TRAIN Dataset: (8949, 4)\n",
      "TEST Dataset: (2237, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = df.sample(frac = TRAIN_SPLIT)\n",
    "test_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_ATM(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.df['tokens'].iloc[idx]\n",
    "        tags = self.df['iob_aspect_tags'].iloc[idx]\n",
    "\n",
    "        bert_tokens = []\n",
    "        bert_tags = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            t = self.tokenizer.tokenize(tokens[i])\n",
    "            bert_tokens += t\n",
    "            bert_tags += [int(tags[i])] * len(t)\n",
    "        \n",
    "        bert_ids = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "\n",
    "        ids_tensor = torch.tensor(bert_ids)\n",
    "        tags_tensor = torch.tensor(bert_tags)\n",
    "\n",
    "        return bert_tokens, ids_tensor, tags_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_BERT_MODEL_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset_ATM(train_df, tokenizer)\n",
    "test_ds = dataset_ATM(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(DEVICE)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(DEVICE)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(DEVICE)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(DEVICE)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    sampler = RandomSampler(train_ds),\n",
    "    batch_size = TRAIN_BATCH_SIZE,\n",
    "    drop_last = True,\n",
    "    collate_fn=create_mini_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    sampler = SequentialSampler(test_ds),\n",
    "    batch_size = VALID_BATCH_SIZE,\n",
    "    drop_last = True,\n",
    "    collate_fn=create_mini_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_ATE = BERT_ATE(PRETRAINED_BERT_MODEL_VARIANT).to(DEVICE)\n",
    "optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch cuda clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    for _, data in enumerate(dataloader, 0):\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "        ids_tensors = ids_tensors.to(DEVICE)\n",
    "        tags_tensors = tags_tensors.to(DEVICE)\n",
    "        masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if _ % 250 == 0:\n",
    "            print(f'Epoch: {epoch}, Step: {_}, Loss:  {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.0352542400360107\n",
      "Epoch: 0, Step: 250, Loss:  0.09849945455789566\n",
      "Epoch: 0, Step: 500, Loss:  0.13399481773376465\n",
      "Epoch: 0, Step: 750, Loss:  0.08224043250083923\n",
      "Epoch: 0, Step: 1000, Loss:  0.16053709387779236\n",
      "Epoch: 0, Step: 1250, Loss:  0.0873182937502861\n",
      "Epoch: 0, Step: 1500, Loss:  0.27753540873527527\n",
      "Epoch: 0, Step: 1750, Loss:  0.14635694026947021\n",
      "Epoch: 0, Step: 2000, Loss:  0.13162505626678467\n",
      "Epoch: 1, Step: 0, Loss:  0.14286555349826813\n",
      "Epoch: 1, Step: 250, Loss:  0.07746236026287079\n",
      "Epoch: 1, Step: 500, Loss:  0.1597706377506256\n",
      "Epoch: 1, Step: 750, Loss:  0.06139006093144417\n",
      "Epoch: 1, Step: 1000, Loss:  0.05829762667417526\n",
      "Epoch: 1, Step: 1250, Loss:  0.10498248040676117\n",
      "Epoch: 1, Step: 1500, Loss:  0.05226742476224899\n",
      "Epoch: 1, Step: 1750, Loss:  0.06537162512540817\n",
      "Epoch: 1, Step: 2000, Loss:  0.09267237037420273\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, model_ATE, torch.nn.CrossEntropyLoss(), optimizer_ATE, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model_ATE, dataloader):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model_ATE(ids_tensors, masks_tensors)\n",
    "\n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            x += list([int(j) for i in predictions for j in i ])\n",
    "            y += list([int(j) for i in tags_tensors for j in i ])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = validate(model_ATE, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     86543\n",
      "           1       0.91      0.81      0.86      8751\n",
      "           2       0.88      0.84      0.86      2862\n",
      "\n",
      "    accuracy                           0.97     98156\n",
      "   macro avg       0.92      0.88      0.90     98156\n",
      "weighted avg       0.97      0.97      0.97     98156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ATE, '../results/ate/models/model_ATE_mams.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ate_mams_stats = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.04682195186615\n",
      "Epoch: 0, Step: 250, Loss:  0.19200390577316284\n",
      "Epoch: 0, Step: 500, Loss:  0.07802700996398926\n",
      "Epoch: 0, Step: 750, Loss:  0.17389942705631256\n",
      "Epoch: 0, Step: 1000, Loss:  0.24707427620887756\n",
      "Epoch: 0, Step: 1250, Loss:  0.03138164430856705\n",
      "Epoch: 0, Step: 1500, Loss:  0.08674127608537674\n",
      "Epoch: 0, Step: 1750, Loss:  0.08171983808279037\n",
      "Epoch: 0, Step: 2000, Loss:  0.09621835500001907\n",
      "Epoch: 1, Step: 0, Loss:  0.15668734908103943\n",
      "Epoch: 1, Step: 250, Loss:  0.03916433826088905\n",
      "Epoch: 1, Step: 500, Loss:  0.11243865638971329\n",
      "Epoch: 1, Step: 750, Loss:  0.05369211360812187\n",
      "Epoch: 1, Step: 1000, Loss:  0.14048659801483154\n",
      "Epoch: 1, Step: 1250, Loss:  0.037970416247844696\n",
      "Epoch: 1, Step: 1500, Loss:  0.09522941708564758\n",
      "Epoch: 1, Step: 1750, Loss:  0.04681490734219551\n",
      "Epoch: 1, Step: 2000, Loss:  0.0465279296040535\n",
      "Run 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.042677879333496\n",
      "Epoch: 0, Step: 250, Loss:  0.2508992850780487\n",
      "Epoch: 0, Step: 500, Loss:  0.3565571904182434\n",
      "Epoch: 0, Step: 750, Loss:  0.1943647861480713\n",
      "Epoch: 0, Step: 1000, Loss:  0.14645105600357056\n",
      "Epoch: 0, Step: 1250, Loss:  0.10630898922681808\n",
      "Epoch: 0, Step: 1500, Loss:  0.20444121956825256\n",
      "Epoch: 0, Step: 1750, Loss:  0.11980200558900833\n",
      "Epoch: 0, Step: 2000, Loss:  0.08928913623094559\n",
      "Epoch: 1, Step: 0, Loss:  0.12784157693386078\n",
      "Epoch: 1, Step: 250, Loss:  0.14071038365364075\n",
      "Epoch: 1, Step: 500, Loss:  0.12431848794221878\n",
      "Epoch: 1, Step: 750, Loss:  0.06743177771568298\n",
      "Epoch: 1, Step: 1000, Loss:  0.09688041359186172\n",
      "Epoch: 1, Step: 1250, Loss:  0.004303648602217436\n",
      "Epoch: 1, Step: 1500, Loss:  0.026872806251049042\n",
      "Epoch: 1, Step: 1750, Loss:  0.03608439117670059\n",
      "Epoch: 1, Step: 2000, Loss:  0.10498127341270447\n",
      "Run 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.4639230966567993\n",
      "Epoch: 0, Step: 250, Loss:  0.17172792553901672\n",
      "Epoch: 0, Step: 500, Loss:  0.3060568571090698\n",
      "Epoch: 0, Step: 750, Loss:  0.17148350179195404\n",
      "Epoch: 0, Step: 1000, Loss:  0.10625799745321274\n",
      "Epoch: 0, Step: 1250, Loss:  0.143313929438591\n",
      "Epoch: 0, Step: 1500, Loss:  0.11178508400917053\n",
      "Epoch: 0, Step: 1750, Loss:  0.07423533499240875\n",
      "Epoch: 0, Step: 2000, Loss:  0.08512452244758606\n",
      "Epoch: 1, Step: 0, Loss:  0.11205387860536575\n",
      "Epoch: 1, Step: 250, Loss:  0.11749545484781265\n",
      "Epoch: 1, Step: 500, Loss:  0.05825150012969971\n",
      "Epoch: 1, Step: 750, Loss:  0.12245792150497437\n",
      "Epoch: 1, Step: 1000, Loss:  0.10984472930431366\n",
      "Epoch: 1, Step: 1250, Loss:  0.05265340209007263\n",
      "Epoch: 1, Step: 1500, Loss:  0.05907595530152321\n",
      "Epoch: 1, Step: 1750, Loss:  0.019220130518078804\n",
      "Epoch: 1, Step: 2000, Loss:  0.24343609809875488\n",
      "Run 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.255709171295166\n",
      "Epoch: 0, Step: 250, Loss:  0.1497087925672531\n",
      "Epoch: 0, Step: 500, Loss:  0.20537877082824707\n",
      "Epoch: 0, Step: 750, Loss:  0.14908242225646973\n",
      "Epoch: 0, Step: 1000, Loss:  0.13232392072677612\n",
      "Epoch: 0, Step: 1250, Loss:  0.05879543721675873\n",
      "Epoch: 0, Step: 1500, Loss:  0.162367582321167\n",
      "Epoch: 0, Step: 1750, Loss:  0.13506710529327393\n",
      "Epoch: 0, Step: 2000, Loss:  0.11573159694671631\n",
      "Epoch: 1, Step: 0, Loss:  0.10732629150152206\n",
      "Epoch: 1, Step: 250, Loss:  0.07997918128967285\n",
      "Epoch: 1, Step: 500, Loss:  0.20562845468521118\n",
      "Epoch: 1, Step: 750, Loss:  0.04574575647711754\n",
      "Epoch: 1, Step: 1000, Loss:  0.07322009652853012\n",
      "Epoch: 1, Step: 1250, Loss:  0.026874329894781113\n",
      "Epoch: 1, Step: 1500, Loss:  0.02503211796283722\n",
      "Epoch: 1, Step: 1750, Loss:  0.19568537175655365\n",
      "Epoch: 1, Step: 2000, Loss:  0.02873862534761429\n",
      "Run 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.075918197631836\n",
      "Epoch: 0, Step: 250, Loss:  0.09127195924520493\n",
      "Epoch: 0, Step: 500, Loss:  0.2927808463573456\n",
      "Epoch: 0, Step: 750, Loss:  0.058256831020116806\n",
      "Epoch: 0, Step: 1000, Loss:  0.08308476954698563\n",
      "Epoch: 0, Step: 1250, Loss:  0.1396861970424652\n",
      "Epoch: 0, Step: 1500, Loss:  0.19522491097450256\n",
      "Epoch: 0, Step: 1750, Loss:  0.24821855127811432\n",
      "Epoch: 0, Step: 2000, Loss:  0.13012632727622986\n",
      "Epoch: 1, Step: 0, Loss:  0.09614209085702896\n",
      "Epoch: 1, Step: 250, Loss:  0.11614061892032623\n",
      "Epoch: 1, Step: 500, Loss:  0.054327499121427536\n",
      "Epoch: 1, Step: 750, Loss:  0.0684332326054573\n",
      "Epoch: 1, Step: 1000, Loss:  0.13350075483322144\n",
      "Epoch: 1, Step: 1250, Loss:  0.027979597449302673\n",
      "Epoch: 1, Step: 1500, Loss:  0.13507843017578125\n",
      "Epoch: 1, Step: 1750, Loss:  0.03768206760287285\n",
      "Epoch: 1, Step: 2000, Loss:  0.042921144515275955\n",
      "Run 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.2783429622650146\n",
      "Epoch: 0, Step: 250, Loss:  0.29739198088645935\n",
      "Epoch: 0, Step: 500, Loss:  0.22795255482196808\n",
      "Epoch: 0, Step: 750, Loss:  0.07380405813455582\n",
      "Epoch: 0, Step: 1000, Loss:  0.0766105055809021\n",
      "Epoch: 0, Step: 1250, Loss:  0.11717953532934189\n",
      "Epoch: 0, Step: 1500, Loss:  0.11925826221704483\n",
      "Epoch: 0, Step: 1750, Loss:  0.17991042137145996\n",
      "Epoch: 0, Step: 2000, Loss:  0.0789104625582695\n",
      "Epoch: 1, Step: 0, Loss:  0.12950313091278076\n",
      "Epoch: 1, Step: 250, Loss:  0.11741800606250763\n",
      "Epoch: 1, Step: 500, Loss:  0.06505125761032104\n",
      "Epoch: 1, Step: 750, Loss:  0.03482092171907425\n",
      "Epoch: 1, Step: 1000, Loss:  0.14196595549583435\n",
      "Epoch: 1, Step: 1250, Loss:  0.08183784782886505\n",
      "Epoch: 1, Step: 1500, Loss:  0.034466080367565155\n",
      "Epoch: 1, Step: 1750, Loss:  0.10144148021936417\n",
      "Epoch: 1, Step: 2000, Loss:  0.06534934043884277\n",
      "Run 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.2830792665481567\n",
      "Epoch: 0, Step: 250, Loss:  0.20837797224521637\n",
      "Epoch: 0, Step: 500, Loss:  0.11199929565191269\n",
      "Epoch: 0, Step: 750, Loss:  0.14493127167224884\n",
      "Epoch: 0, Step: 1000, Loss:  0.11649476736783981\n",
      "Epoch: 0, Step: 1250, Loss:  0.20733001828193665\n",
      "Epoch: 0, Step: 1500, Loss:  0.17347349226474762\n",
      "Epoch: 0, Step: 1750, Loss:  0.19006729125976562\n",
      "Epoch: 0, Step: 2000, Loss:  0.11420568823814392\n",
      "Epoch: 1, Step: 0, Loss:  0.09217513352632523\n",
      "Epoch: 1, Step: 250, Loss:  0.10300259292125702\n",
      "Epoch: 1, Step: 500, Loss:  0.08441643416881561\n",
      "Epoch: 1, Step: 750, Loss:  0.027663564309477806\n",
      "Epoch: 1, Step: 1000, Loss:  0.17444555461406708\n",
      "Epoch: 1, Step: 1250, Loss:  0.06588095426559448\n",
      "Epoch: 1, Step: 1500, Loss:  0.039876554161310196\n",
      "Epoch: 1, Step: 1750, Loss:  0.06674695760011673\n",
      "Epoch: 1, Step: 2000, Loss:  0.01662570796906948\n",
      "Run 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.0995115041732788\n",
      "Epoch: 0, Step: 250, Loss:  0.1791851818561554\n",
      "Epoch: 0, Step: 500, Loss:  0.13415180146694183\n",
      "Epoch: 0, Step: 750, Loss:  0.13288573920726776\n",
      "Epoch: 0, Step: 1000, Loss:  0.08960212767124176\n",
      "Epoch: 0, Step: 1250, Loss:  0.29862144589424133\n",
      "Epoch: 0, Step: 1500, Loss:  0.10156542807817459\n",
      "Epoch: 0, Step: 1750, Loss:  0.05764281749725342\n",
      "Epoch: 0, Step: 2000, Loss:  0.09285144507884979\n",
      "Epoch: 1, Step: 0, Loss:  0.06406483799219131\n",
      "Epoch: 1, Step: 250, Loss:  0.1233275756239891\n",
      "Epoch: 1, Step: 500, Loss:  0.10617289692163467\n",
      "Epoch: 1, Step: 750, Loss:  0.06566299498081207\n",
      "Epoch: 1, Step: 1000, Loss:  0.06824783235788345\n",
      "Epoch: 1, Step: 1250, Loss:  0.12758894264698029\n",
      "Epoch: 1, Step: 1500, Loss:  0.08792734891176224\n",
      "Epoch: 1, Step: 1750, Loss:  0.06153273954987526\n",
      "Epoch: 1, Step: 2000, Loss:  0.04510731250047684\n",
      "Run 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.0557465553283691\n",
      "Epoch: 0, Step: 250, Loss:  0.27305927872657776\n",
      "Epoch: 0, Step: 500, Loss:  0.26642027497291565\n",
      "Epoch: 0, Step: 750, Loss:  0.20242908596992493\n",
      "Epoch: 0, Step: 1000, Loss:  0.04250514879822731\n",
      "Epoch: 0, Step: 1250, Loss:  0.2864929139614105\n",
      "Epoch: 0, Step: 1500, Loss:  0.1269216686487198\n",
      "Epoch: 0, Step: 1750, Loss:  0.06666672229766846\n",
      "Epoch: 0, Step: 2000, Loss:  0.09615769982337952\n",
      "Epoch: 1, Step: 0, Loss:  0.23873209953308105\n",
      "Epoch: 1, Step: 250, Loss:  0.04499483481049538\n",
      "Epoch: 1, Step: 500, Loss:  0.1462191343307495\n",
      "Epoch: 1, Step: 750, Loss:  0.1594005674123764\n",
      "Epoch: 1, Step: 1000, Loss:  0.10446750372648239\n",
      "Epoch: 1, Step: 1250, Loss:  0.021205337718129158\n",
      "Epoch: 1, Step: 1500, Loss:  0.04783220589160919\n",
      "Epoch: 1, Step: 1750, Loss:  0.03491629287600517\n",
      "Epoch: 1, Step: 2000, Loss:  0.04675896465778351\n",
      "Run 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.264002799987793\n",
      "Epoch: 0, Step: 250, Loss:  0.11675743013620377\n",
      "Epoch: 0, Step: 500, Loss:  0.09582889080047607\n",
      "Epoch: 0, Step: 750, Loss:  0.18508930504322052\n",
      "Epoch: 0, Step: 1000, Loss:  0.10759922862052917\n",
      "Epoch: 0, Step: 1250, Loss:  0.0682109072804451\n",
      "Epoch: 0, Step: 1500, Loss:  0.1189827248454094\n",
      "Epoch: 0, Step: 1750, Loss:  0.05768073350191116\n",
      "Epoch: 0, Step: 2000, Loss:  0.14141815900802612\n",
      "Epoch: 1, Step: 0, Loss:  0.15549761056900024\n",
      "Epoch: 1, Step: 250, Loss:  0.13506411015987396\n",
      "Epoch: 1, Step: 500, Loss:  0.09423185139894485\n",
      "Epoch: 1, Step: 750, Loss:  0.14729222655296326\n",
      "Epoch: 1, Step: 1000, Loss:  0.1365109384059906\n",
      "Epoch: 1, Step: 1250, Loss:  0.025700869038701057\n",
      "Epoch: 1, Step: 1500, Loss:  0.02778894081711769\n",
      "Epoch: 1, Step: 1750, Loss:  0.08371781557798386\n",
      "Epoch: 1, Step: 2000, Loss:  0.029140857979655266\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/10\")\n",
    "\n",
    "    train_df = df.sample(frac = TRAIN_SPLIT)\n",
    "    test_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    train_ds = dataset_ATM(train_df, tokenizer)\n",
    "    test_ds = dataset_ATM(test_df, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        sampler = RandomSampler(train_ds),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_ds,\n",
    "        sampler = SequentialSampler(test_ds),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model_ATE_run = BERT_ATE(PRETRAINED_BERT_MODEL_VARIANT).to(DEVICE)\n",
    "    optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    model_ATE_run = BERT_ATE(PRETRAINED_BERT_MODEL_VARIANT).to(DEVICE)\n",
    "    optimizer_ATE_run = torch.optim.Adam(model_ATE_run.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model_ATE_run, torch.nn.CrossEntropyLoss(), optimizer_ATE_run, train_dataloader)\n",
    "\n",
    "    x, y = validate(model_ATE, test_dataloader)\n",
    "\n",
    "    accuracy = accuracy_score(y, x)\n",
    "    precision_score_micro = precision_score(y, x, average='micro')\n",
    "    precision_score_macro = precision_score(y, x, average='macro')\n",
    "    recall_score_micro = recall_score(y, x, average='micro')\n",
    "    recall_score_macro = recall_score(y, x, average='macro')\n",
    "    f1_score_micro = f1_score(y, x, average='micro')\n",
    "    f1_score_macro = f1_score(y, x, average='macro')\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    bert_ate_mams_stats.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    del train_df\n",
    "    del test_df\n",
    "    del train_ds\n",
    "    del test_ds\n",
    "    del train_dataloader\n",
    "    del test_dataloader\n",
    "    del model_ATE_run\n",
    "    del optimizer_ATE\n",
    "    del x\n",
    "    del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.977617</td>\n",
       "      <td>0.977617</td>\n",
       "      <td>0.913309</td>\n",
       "      <td>0.977617</td>\n",
       "      <td>0.949835</td>\n",
       "      <td>0.977617</td>\n",
       "      <td>0.930646</td>\n",
       "      <td>380.570090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.976691</td>\n",
       "      <td>0.976691</td>\n",
       "      <td>0.908525</td>\n",
       "      <td>0.976691</td>\n",
       "      <td>0.944193</td>\n",
       "      <td>0.976691</td>\n",
       "      <td>0.925478</td>\n",
       "      <td>385.877049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.976331</td>\n",
       "      <td>0.976331</td>\n",
       "      <td>0.906979</td>\n",
       "      <td>0.976331</td>\n",
       "      <td>0.944357</td>\n",
       "      <td>0.976331</td>\n",
       "      <td>0.924718</td>\n",
       "      <td>385.869864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.975579</td>\n",
       "      <td>0.975579</td>\n",
       "      <td>0.904484</td>\n",
       "      <td>0.975579</td>\n",
       "      <td>0.943600</td>\n",
       "      <td>0.975579</td>\n",
       "      <td>0.923100</td>\n",
       "      <td>385.640436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.977371</td>\n",
       "      <td>0.977371</td>\n",
       "      <td>0.907026</td>\n",
       "      <td>0.977371</td>\n",
       "      <td>0.947159</td>\n",
       "      <td>0.977371</td>\n",
       "      <td>0.926139</td>\n",
       "      <td>392.682886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.976435</td>\n",
       "      <td>0.976435</td>\n",
       "      <td>0.910136</td>\n",
       "      <td>0.976435</td>\n",
       "      <td>0.943848</td>\n",
       "      <td>0.976435</td>\n",
       "      <td>0.926211</td>\n",
       "      <td>386.502355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.977456</td>\n",
       "      <td>0.977456</td>\n",
       "      <td>0.910919</td>\n",
       "      <td>0.977456</td>\n",
       "      <td>0.949450</td>\n",
       "      <td>0.977456</td>\n",
       "      <td>0.929207</td>\n",
       "      <td>385.147549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.977087</td>\n",
       "      <td>0.977087</td>\n",
       "      <td>0.912205</td>\n",
       "      <td>0.977087</td>\n",
       "      <td>0.943649</td>\n",
       "      <td>0.977087</td>\n",
       "      <td>0.927186</td>\n",
       "      <td>388.000542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.977539</td>\n",
       "      <td>0.977539</td>\n",
       "      <td>0.910781</td>\n",
       "      <td>0.977539</td>\n",
       "      <td>0.949291</td>\n",
       "      <td>0.977539</td>\n",
       "      <td>0.929086</td>\n",
       "      <td>389.369364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.976292</td>\n",
       "      <td>0.976292</td>\n",
       "      <td>0.902420</td>\n",
       "      <td>0.976292</td>\n",
       "      <td>0.946797</td>\n",
       "      <td>0.976292</td>\n",
       "      <td>0.923407</td>\n",
       "      <td>390.278677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.977617               0.977617               0.913309            0.977617   \n",
       "1  0.976691               0.976691               0.908525            0.976691   \n",
       "2  0.976331               0.976331               0.906979            0.976331   \n",
       "3  0.975579               0.975579               0.904484            0.975579   \n",
       "4  0.977371               0.977371               0.907026            0.977371   \n",
       "5  0.976435               0.976435               0.910136            0.976435   \n",
       "6  0.977456               0.977456               0.910919            0.977456   \n",
       "7  0.977087               0.977087               0.912205            0.977087   \n",
       "8  0.977539               0.977539               0.910781            0.977539   \n",
       "9  0.976292               0.976292               0.902420            0.976292   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.949835        0.977617        0.930646      380.570090  \n",
       "1            0.944193        0.976691        0.925478      385.877049  \n",
       "2            0.944357        0.976331        0.924718      385.869864  \n",
       "3            0.943600        0.975579        0.923100      385.640436  \n",
       "4            0.947159        0.977371        0.926139      392.682886  \n",
       "5            0.943848        0.976435        0.926211      386.502355  \n",
       "6            0.949450        0.977456        0.929207      385.147549  \n",
       "7            0.943649        0.977087        0.927186      388.000542  \n",
       "8            0.949291        0.977539        0.929086      389.369364  \n",
       "9            0.946797        0.976292        0.923407      390.278677  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ate_mams_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ate_mams_stats.to_csv('../results/ate/stats/bert_ate_mams_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
