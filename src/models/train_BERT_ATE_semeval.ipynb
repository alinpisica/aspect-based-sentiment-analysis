{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BERT_ATE import BERT_ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = '../data/semeval16_restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>opinions</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[{'target': 'place', 'category': 'RESTAURANT#G...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[{'target': 'staff', 'category': 'SERVICE#GENE...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[{'target': 'NULL', 'category': 'SERVICE#GENER...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[{'target': 'food', 'category': 'FOOD#QUALITY'...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[{'target': 'food', 'category': 'FOOD#QUALITY'...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                            opinions  \\\n",
       "0  [{'target': 'place', 'category': 'RESTAURANT#G...   \n",
       "1  [{'target': 'staff', 'category': 'SERVICE#GENE...   \n",
       "2  [{'target': 'NULL', 'category': 'SERVICE#GENER...   \n",
       "3  [{'target': 'food', 'category': 'FOOD#QUALITY'...   \n",
       "4  [{'target': 'food', 'category': 'FOOD#QUALITY'...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "TRAIN_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_BERT_MODEL_VARIANT = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (2507, 4)\n",
      "TRAIN Dataset: (2006, 4)\n",
      "TEST Dataset: (501, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = df.sample(frac = TRAIN_SPLIT)\n",
    "test_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_ATM(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.df['tokens'].iloc[idx]\n",
    "        tags = self.df['iob_aspect_tags'].iloc[idx]\n",
    "\n",
    "        bert_tokens = []\n",
    "        bert_tags = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            t = self.tokenizer.tokenize(tokens[i])\n",
    "            bert_tokens += t\n",
    "            bert_tags += [int(tags[i])] * len(t)\n",
    "        \n",
    "        bert_ids = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "\n",
    "        ids_tensor = torch.tensor(bert_ids)\n",
    "        tags_tensor = torch.tensor(bert_tags)\n",
    "\n",
    "        return bert_tokens, ids_tensor, tags_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_BERT_MODEL_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset_ATM(train_df, tokenizer)\n",
    "test_ds = dataset_ATM(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(DEVICE)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(DEVICE)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(DEVICE)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(DEVICE)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    sampler = RandomSampler(train_ds),\n",
    "    batch_size = TRAIN_BATCH_SIZE,\n",
    "    drop_last = True,\n",
    "    collate_fn=create_mini_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    sampler = SequentialSampler(test_ds),\n",
    "    batch_size = VALID_BATCH_SIZE,\n",
    "    drop_last = True,\n",
    "    collate_fn=create_mini_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_ATE = BERT_ATE(PRETRAINED_BERT_MODEL_VARIANT).to(DEVICE)\n",
    "optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch cuda clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    for _, data in enumerate(dataloader, 0):\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "        ids_tensors = ids_tensors.to(DEVICE)\n",
    "        tags_tensors = tags_tensors.to(DEVICE)\n",
    "        masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if _ % 50 == 0:\n",
    "            print(f'Epoch: {epoch}, Step: {_}, Loss:  {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.0022004842758179\n",
      "Epoch: 0, Step: 50, Loss:  0.20004427433013916\n",
      "Epoch: 0, Step: 100, Loss:  0.21648697555065155\n",
      "Epoch: 0, Step: 150, Loss:  0.08073092997074127\n",
      "Epoch: 0, Step: 200, Loss:  0.08740708231925964\n",
      "Epoch: 0, Step: 250, Loss:  0.23410958051681519\n",
      "Epoch: 0, Step: 300, Loss:  0.07421716302633286\n",
      "Epoch: 0, Step: 350, Loss:  0.33796024322509766\n",
      "Epoch: 0, Step: 400, Loss:  0.13609564304351807\n",
      "Epoch: 0, Step: 450, Loss:  0.1776328831911087\n",
      "Epoch: 0, Step: 500, Loss:  0.04585307091474533\n",
      "Epoch: 1, Step: 0, Loss:  0.06842641532421112\n",
      "Epoch: 1, Step: 50, Loss:  0.16960299015045166\n",
      "Epoch: 1, Step: 100, Loss:  0.016538985073566437\n",
      "Epoch: 1, Step: 150, Loss:  0.11822547018527985\n",
      "Epoch: 1, Step: 200, Loss:  0.06756515055894852\n",
      "Epoch: 1, Step: 250, Loss:  0.03745114058256149\n",
      "Epoch: 1, Step: 300, Loss:  0.04986521974205971\n",
      "Epoch: 1, Step: 350, Loss:  0.1965045928955078\n",
      "Epoch: 1, Step: 400, Loss:  0.04075441509485245\n",
      "Epoch: 1, Step: 450, Loss:  0.06539789587259293\n",
      "Epoch: 1, Step: 500, Loss:  0.08537736535072327\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, model_ATE, torch.nn.CrossEntropyLoss(), optimizer_ATE, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model_ATE, dataloader):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model_ATE(ids_tensors, masks_tensors)\n",
    "\n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            x += list([int(j) for i in predictions for j in i ])\n",
    "            y += list([int(j) for i in tags_tensors for j in i ])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = validate(model_ATE, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     13338\n",
      "           1       0.82      0.87      0.84       883\n",
      "           2       0.68      0.90      0.77       311\n",
      "\n",
      "    accuracy                           0.97     14532\n",
      "   macro avg       0.83      0.92      0.87     14532\n",
      "weighted avg       0.98      0.97      0.97     14532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ATE, '../results/ate/models/model_ATE_semeval.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ate_semeval_stats = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.0430035591125488\n",
      "Epoch: 0, Step: 50, Loss:  0.36760538816452026\n",
      "Epoch: 0, Step: 100, Loss:  0.2698538899421692\n",
      "Epoch: 0, Step: 150, Loss:  0.0768975168466568\n",
      "Epoch: 0, Step: 200, Loss:  0.05865037813782692\n",
      "Epoch: 0, Step: 250, Loss:  0.09902628511190414\n",
      "Epoch: 0, Step: 300, Loss:  0.1596120297908783\n",
      "Epoch: 0, Step: 350, Loss:  0.1139257550239563\n",
      "Epoch: 0, Step: 400, Loss:  0.1024770736694336\n",
      "Epoch: 0, Step: 450, Loss:  0.10531380772590637\n",
      "Epoch: 0, Step: 500, Loss:  0.08003353327512741\n",
      "Epoch: 1, Step: 0, Loss:  0.009264320135116577\n",
      "Epoch: 1, Step: 50, Loss:  0.09024758636951447\n",
      "Epoch: 1, Step: 100, Loss:  0.10645774751901627\n",
      "Epoch: 1, Step: 150, Loss:  0.04666516184806824\n",
      "Epoch: 1, Step: 200, Loss:  0.02524683251976967\n",
      "Epoch: 1, Step: 250, Loss:  0.05559393763542175\n",
      "Epoch: 1, Step: 300, Loss:  0.20780722796916962\n",
      "Epoch: 1, Step: 350, Loss:  0.11651113629341125\n",
      "Epoch: 1, Step: 400, Loss:  0.04732119292020798\n",
      "Epoch: 1, Step: 450, Loss:  0.045676928013563156\n",
      "Epoch: 1, Step: 500, Loss:  0.10356743633747101\n",
      "Run 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.2537903785705566\n",
      "Epoch: 0, Step: 50, Loss:  0.44710657000541687\n",
      "Epoch: 0, Step: 100, Loss:  0.4849851727485657\n",
      "Epoch: 0, Step: 150, Loss:  0.2641758620738983\n",
      "Epoch: 0, Step: 200, Loss:  0.10806534439325333\n",
      "Epoch: 0, Step: 250, Loss:  0.12158673256635666\n",
      "Epoch: 0, Step: 300, Loss:  0.18524472415447235\n",
      "Epoch: 0, Step: 350, Loss:  0.10666368901729584\n",
      "Epoch: 0, Step: 400, Loss:  0.05690456181764603\n",
      "Epoch: 0, Step: 450, Loss:  0.08016927540302277\n",
      "Epoch: 0, Step: 500, Loss:  0.0972270742058754\n",
      "Epoch: 1, Step: 0, Loss:  0.09911803901195526\n",
      "Epoch: 1, Step: 50, Loss:  0.12451036274433136\n",
      "Epoch: 1, Step: 100, Loss:  0.06200190633535385\n",
      "Epoch: 1, Step: 150, Loss:  0.11665500700473785\n",
      "Epoch: 1, Step: 200, Loss:  0.04767729714512825\n",
      "Epoch: 1, Step: 250, Loss:  0.102117158472538\n",
      "Epoch: 1, Step: 300, Loss:  0.08810574561357498\n",
      "Epoch: 1, Step: 350, Loss:  0.018795818090438843\n",
      "Epoch: 1, Step: 400, Loss:  0.07390595227479935\n",
      "Epoch: 1, Step: 450, Loss:  0.030220715329051018\n",
      "Epoch: 1, Step: 500, Loss:  0.07039941847324371\n",
      "Run 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.3014229536056519\n",
      "Epoch: 0, Step: 50, Loss:  0.40194207429885864\n",
      "Epoch: 0, Step: 100, Loss:  0.18030965328216553\n",
      "Epoch: 0, Step: 150, Loss:  0.11769424378871918\n",
      "Epoch: 0, Step: 200, Loss:  0.1678420752286911\n",
      "Epoch: 0, Step: 250, Loss:  0.07730333507061005\n",
      "Epoch: 0, Step: 300, Loss:  0.07073739171028137\n",
      "Epoch: 0, Step: 350, Loss:  0.23605643212795258\n",
      "Epoch: 0, Step: 400, Loss:  0.07493675500154495\n",
      "Epoch: 0, Step: 450, Loss:  0.22320862114429474\n",
      "Epoch: 0, Step: 500, Loss:  0.06647665053606033\n",
      "Epoch: 1, Step: 0, Loss:  0.12300381064414978\n",
      "Epoch: 1, Step: 50, Loss:  0.032233450561761856\n",
      "Epoch: 1, Step: 100, Loss:  0.1801566332578659\n",
      "Epoch: 1, Step: 150, Loss:  0.02168879471719265\n",
      "Epoch: 1, Step: 200, Loss:  0.12324973195791245\n",
      "Epoch: 1, Step: 250, Loss:  0.037198591977357864\n",
      "Epoch: 1, Step: 300, Loss:  0.036494478583335876\n",
      "Epoch: 1, Step: 350, Loss:  0.029521457850933075\n",
      "Epoch: 1, Step: 400, Loss:  0.09758556634187698\n",
      "Epoch: 1, Step: 450, Loss:  0.04307110980153084\n",
      "Epoch: 1, Step: 500, Loss:  0.04444534331560135\n",
      "Run 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.0898280143737793\n",
      "Epoch: 0, Step: 50, Loss:  0.2447110414505005\n",
      "Epoch: 0, Step: 100, Loss:  0.4175432324409485\n",
      "Epoch: 0, Step: 150, Loss:  0.06393174082040787\n",
      "Epoch: 0, Step: 200, Loss:  0.11685635894536972\n",
      "Epoch: 0, Step: 250, Loss:  0.20008225739002228\n",
      "Epoch: 0, Step: 300, Loss:  0.15935471653938293\n",
      "Epoch: 0, Step: 350, Loss:  0.1250811666250229\n",
      "Epoch: 0, Step: 400, Loss:  0.18204212188720703\n",
      "Epoch: 0, Step: 450, Loss:  0.16214998066425323\n",
      "Epoch: 0, Step: 500, Loss:  0.1858232319355011\n",
      "Epoch: 1, Step: 0, Loss:  0.08303128927946091\n",
      "Epoch: 1, Step: 50, Loss:  0.04150811582803726\n",
      "Epoch: 1, Step: 100, Loss:  0.06011053919792175\n",
      "Epoch: 1, Step: 150, Loss:  0.08970509469509125\n",
      "Epoch: 1, Step: 200, Loss:  0.08348125219345093\n",
      "Epoch: 1, Step: 250, Loss:  0.06847938150167465\n",
      "Epoch: 1, Step: 300, Loss:  0.008204569108784199\n",
      "Epoch: 1, Step: 350, Loss:  0.1344681829214096\n",
      "Epoch: 1, Step: 400, Loss:  0.09505525231361389\n",
      "Epoch: 1, Step: 450, Loss:  0.01969189941883087\n",
      "Epoch: 1, Step: 500, Loss:  0.1577436327934265\n",
      "Run 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.062132716178894\n",
      "Epoch: 0, Step: 50, Loss:  0.5515051484107971\n",
      "Epoch: 0, Step: 100, Loss:  0.4542858898639679\n",
      "Epoch: 0, Step: 150, Loss:  0.11120535433292389\n",
      "Epoch: 0, Step: 200, Loss:  0.2412242889404297\n",
      "Epoch: 0, Step: 250, Loss:  0.0855952799320221\n",
      "Epoch: 0, Step: 300, Loss:  0.09694837033748627\n",
      "Epoch: 0, Step: 350, Loss:  0.15682408213615417\n",
      "Epoch: 0, Step: 400, Loss:  0.13360382616519928\n",
      "Epoch: 0, Step: 450, Loss:  0.03171464055776596\n",
      "Epoch: 0, Step: 500, Loss:  0.06826537102460861\n",
      "Epoch: 1, Step: 0, Loss:  0.05365700647234917\n",
      "Epoch: 1, Step: 50, Loss:  0.0822128877043724\n",
      "Epoch: 1, Step: 100, Loss:  0.038975562900304794\n",
      "Epoch: 1, Step: 150, Loss:  0.05985653027892113\n",
      "Epoch: 1, Step: 200, Loss:  0.007912320084869862\n",
      "Epoch: 1, Step: 250, Loss:  0.061512332409620285\n",
      "Epoch: 1, Step: 300, Loss:  0.027350952848792076\n",
      "Epoch: 1, Step: 350, Loss:  0.052660319954156876\n",
      "Epoch: 1, Step: 400, Loss:  0.04332687705755234\n",
      "Epoch: 1, Step: 450, Loss:  0.06461235880851746\n",
      "Epoch: 1, Step: 500, Loss:  0.05147451162338257\n",
      "Run 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.0856444835662842\n",
      "Epoch: 0, Step: 50, Loss:  0.20581768453121185\n",
      "Epoch: 0, Step: 100, Loss:  0.1957991123199463\n",
      "Epoch: 0, Step: 150, Loss:  0.12569022178649902\n",
      "Epoch: 0, Step: 200, Loss:  0.20157530903816223\n",
      "Epoch: 0, Step: 250, Loss:  0.13149528205394745\n",
      "Epoch: 0, Step: 300, Loss:  0.13972389698028564\n",
      "Epoch: 0, Step: 350, Loss:  0.3456745743751526\n",
      "Epoch: 0, Step: 400, Loss:  0.10223718732595444\n",
      "Epoch: 0, Step: 450, Loss:  0.06560993194580078\n",
      "Epoch: 0, Step: 500, Loss:  0.03135645017027855\n",
      "Epoch: 1, Step: 0, Loss:  0.11809445917606354\n",
      "Epoch: 1, Step: 50, Loss:  0.030834287405014038\n",
      "Epoch: 1, Step: 100, Loss:  0.09849459677934647\n",
      "Epoch: 1, Step: 150, Loss:  0.08539047837257385\n",
      "Epoch: 1, Step: 200, Loss:  0.03766023740172386\n",
      "Epoch: 1, Step: 250, Loss:  0.019637487828731537\n",
      "Epoch: 1, Step: 300, Loss:  0.02738294005393982\n",
      "Epoch: 1, Step: 350, Loss:  0.1278335154056549\n",
      "Epoch: 1, Step: 400, Loss:  0.04479116201400757\n",
      "Epoch: 1, Step: 450, Loss:  0.036197926849126816\n",
      "Epoch: 1, Step: 500, Loss:  0.05553360655903816\n",
      "Run 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.3412960767745972\n",
      "Epoch: 0, Step: 50, Loss:  0.2535034418106079\n",
      "Epoch: 0, Step: 100, Loss:  0.4011283218860626\n",
      "Epoch: 0, Step: 150, Loss:  0.16236890852451324\n",
      "Epoch: 0, Step: 200, Loss:  0.1214841902256012\n",
      "Epoch: 0, Step: 250, Loss:  0.06271844357252121\n",
      "Epoch: 0, Step: 300, Loss:  0.05705879628658295\n",
      "Epoch: 0, Step: 350, Loss:  0.10836785286664963\n",
      "Epoch: 0, Step: 400, Loss:  0.05862100049853325\n",
      "Epoch: 0, Step: 450, Loss:  0.12642738223075867\n",
      "Epoch: 0, Step: 500, Loss:  0.13753166794776917\n",
      "Epoch: 1, Step: 0, Loss:  0.12718148529529572\n",
      "Epoch: 1, Step: 50, Loss:  0.16934145987033844\n",
      "Epoch: 1, Step: 100, Loss:  0.03909799084067345\n",
      "Epoch: 1, Step: 150, Loss:  0.1451781988143921\n",
      "Epoch: 1, Step: 200, Loss:  0.09871223568916321\n",
      "Epoch: 1, Step: 250, Loss:  0.03597717359662056\n",
      "Epoch: 1, Step: 300, Loss:  0.06970240920782089\n",
      "Epoch: 1, Step: 350, Loss:  0.0950542613863945\n",
      "Epoch: 1, Step: 400, Loss:  0.09138715267181396\n",
      "Epoch: 1, Step: 450, Loss:  0.06733590364456177\n",
      "Epoch: 1, Step: 500, Loss:  0.010968132875859737\n",
      "Run 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.1806660890579224\n",
      "Epoch: 0, Step: 50, Loss:  0.2237490862607956\n",
      "Epoch: 0, Step: 100, Loss:  0.2773565649986267\n",
      "Epoch: 0, Step: 150, Loss:  0.09165164083242416\n",
      "Epoch: 0, Step: 200, Loss:  0.07346244156360626\n",
      "Epoch: 0, Step: 250, Loss:  0.0747537687420845\n",
      "Epoch: 0, Step: 300, Loss:  0.07161753624677658\n",
      "Epoch: 0, Step: 350, Loss:  0.04677466303110123\n",
      "Epoch: 0, Step: 400, Loss:  0.16600003838539124\n",
      "Epoch: 0, Step: 450, Loss:  0.16486799716949463\n",
      "Epoch: 0, Step: 500, Loss:  0.05424050614237785\n",
      "Epoch: 1, Step: 0, Loss:  0.0966126024723053\n",
      "Epoch: 1, Step: 50, Loss:  0.03344816341996193\n",
      "Epoch: 1, Step: 100, Loss:  0.3145105838775635\n",
      "Epoch: 1, Step: 150, Loss:  0.06441684067249298\n",
      "Epoch: 1, Step: 200, Loss:  0.03905321657657623\n",
      "Epoch: 1, Step: 250, Loss:  0.020183682441711426\n",
      "Epoch: 1, Step: 300, Loss:  0.0366407074034214\n",
      "Epoch: 1, Step: 350, Loss:  0.05747498944401741\n",
      "Epoch: 1, Step: 400, Loss:  0.03100842423737049\n",
      "Epoch: 1, Step: 450, Loss:  0.01602890156209469\n",
      "Epoch: 1, Step: 500, Loss:  0.025365903973579407\n",
      "Run 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  0.9639701247215271\n",
      "Epoch: 0, Step: 50, Loss:  0.2821648418903351\n",
      "Epoch: 0, Step: 100, Loss:  0.1693946272134781\n",
      "Epoch: 0, Step: 150, Loss:  0.05498886853456497\n",
      "Epoch: 0, Step: 200, Loss:  0.16234391927719116\n",
      "Epoch: 0, Step: 250, Loss:  0.09449974447488785\n",
      "Epoch: 0, Step: 300, Loss:  0.18932634592056274\n",
      "Epoch: 0, Step: 350, Loss:  0.1320429891347885\n",
      "Epoch: 0, Step: 400, Loss:  0.12235639989376068\n",
      "Epoch: 0, Step: 450, Loss:  0.07171657681465149\n",
      "Epoch: 0, Step: 500, Loss:  0.03225191310048103\n",
      "Epoch: 1, Step: 0, Loss:  0.11923080682754517\n",
      "Epoch: 1, Step: 50, Loss:  0.04148229584097862\n",
      "Epoch: 1, Step: 100, Loss:  0.06723132729530334\n",
      "Epoch: 1, Step: 150, Loss:  0.03943600133061409\n",
      "Epoch: 1, Step: 200, Loss:  0.02137628011405468\n",
      "Epoch: 1, Step: 250, Loss:  0.09542258083820343\n",
      "Epoch: 1, Step: 300, Loss:  0.1124953031539917\n",
      "Epoch: 1, Step: 350, Loss:  0.026850419119000435\n",
      "Epoch: 1, Step: 400, Loss:  0.05960392206907272\n",
      "Epoch: 1, Step: 450, Loss:  0.03837199509143829\n",
      "Epoch: 1, Step: 500, Loss:  0.01371727418154478\n",
      "Run 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss:  1.087335467338562\n",
      "Epoch: 0, Step: 50, Loss:  0.1756507158279419\n",
      "Epoch: 0, Step: 100, Loss:  0.0999547615647316\n",
      "Epoch: 0, Step: 150, Loss:  0.12446015328168869\n",
      "Epoch: 0, Step: 200, Loss:  0.1867896318435669\n",
      "Epoch: 0, Step: 250, Loss:  0.10504032671451569\n",
      "Epoch: 0, Step: 300, Loss:  0.37062010169029236\n",
      "Epoch: 0, Step: 350, Loss:  0.1552312970161438\n",
      "Epoch: 0, Step: 400, Loss:  0.11326158791780472\n",
      "Epoch: 0, Step: 450, Loss:  0.23856714367866516\n",
      "Epoch: 0, Step: 500, Loss:  0.10938013345003128\n",
      "Epoch: 1, Step: 0, Loss:  0.021813208237290382\n",
      "Epoch: 1, Step: 50, Loss:  0.09317556023597717\n",
      "Epoch: 1, Step: 100, Loss:  0.03270767256617546\n",
      "Epoch: 1, Step: 150, Loss:  0.025941254571080208\n",
      "Epoch: 1, Step: 200, Loss:  0.10452130436897278\n",
      "Epoch: 1, Step: 250, Loss:  0.07946568727493286\n",
      "Epoch: 1, Step: 300, Loss:  0.05709370970726013\n",
      "Epoch: 1, Step: 350, Loss:  0.042549505829811096\n",
      "Epoch: 1, Step: 400, Loss:  0.0824536457657814\n",
      "Epoch: 1, Step: 450, Loss:  0.05225629359483719\n",
      "Epoch: 1, Step: 500, Loss:  0.019109785556793213\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/10\")\n",
    "\n",
    "    train_df = df.sample(frac = TRAIN_SPLIT)\n",
    "    test_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    train_ds = dataset_ATM(train_df, tokenizer)\n",
    "    test_ds = dataset_ATM(test_df, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        sampler = RandomSampler(train_ds),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_ds,\n",
    "        sampler = SequentialSampler(test_ds),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model_ATE_run = BERT_ATE(PRETRAINED_BERT_MODEL_VARIANT).to(DEVICE)\n",
    "    optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    model_ATE_run = BERT_ATE(PRETRAINED_BERT_MODEL_VARIANT).to(DEVICE)\n",
    "    optimizer_ATE_run = torch.optim.Adam(model_ATE_run.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model_ATE_run, torch.nn.CrossEntropyLoss(), optimizer_ATE_run, train_dataloader)\n",
    "\n",
    "    x, y = validate(model_ATE, test_dataloader)\n",
    "\n",
    "    accuracy = accuracy_score(y, x)\n",
    "    precision_score_micro = precision_score(y, x, average='micro')\n",
    "    precision_score_macro = precision_score(y, x, average='macro')\n",
    "    recall_score_micro = recall_score(y, x, average='micro')\n",
    "    recall_score_macro = recall_score(y, x, average='macro')\n",
    "    f1_score_micro = f1_score(y, x, average='micro')\n",
    "    f1_score_macro = f1_score(y, x, average='macro')\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    bert_ate_semeval_stats.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    del train_df\n",
    "    del test_df\n",
    "    del train_ds\n",
    "    del test_ds\n",
    "    del train_dataloader\n",
    "    del test_dataloader\n",
    "    del model_ATE_run\n",
    "    del optimizer_ATE\n",
    "    del x\n",
    "    del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984185</td>\n",
       "      <td>0.984185</td>\n",
       "      <td>0.948481</td>\n",
       "      <td>0.984185</td>\n",
       "      <td>0.896852</td>\n",
       "      <td>0.984185</td>\n",
       "      <td>0.921263</td>\n",
       "      <td>90.381280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.983483</td>\n",
       "      <td>0.983483</td>\n",
       "      <td>0.942692</td>\n",
       "      <td>0.983483</td>\n",
       "      <td>0.904133</td>\n",
       "      <td>0.983483</td>\n",
       "      <td>0.922510</td>\n",
       "      <td>91.455228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.982673</td>\n",
       "      <td>0.982673</td>\n",
       "      <td>0.941610</td>\n",
       "      <td>0.982673</td>\n",
       "      <td>0.901140</td>\n",
       "      <td>0.982673</td>\n",
       "      <td>0.920507</td>\n",
       "      <td>91.439573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.984795</td>\n",
       "      <td>0.984795</td>\n",
       "      <td>0.951683</td>\n",
       "      <td>0.984795</td>\n",
       "      <td>0.909210</td>\n",
       "      <td>0.984795</td>\n",
       "      <td>0.929607</td>\n",
       "      <td>90.933346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.952645</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.900382</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.924893</td>\n",
       "      <td>91.098615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.982738</td>\n",
       "      <td>0.982738</td>\n",
       "      <td>0.945367</td>\n",
       "      <td>0.982738</td>\n",
       "      <td>0.886782</td>\n",
       "      <td>0.982738</td>\n",
       "      <td>0.914181</td>\n",
       "      <td>89.598135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.985574</td>\n",
       "      <td>0.985574</td>\n",
       "      <td>0.953635</td>\n",
       "      <td>0.985574</td>\n",
       "      <td>0.912738</td>\n",
       "      <td>0.985574</td>\n",
       "      <td>0.932236</td>\n",
       "      <td>86.936766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.984163</td>\n",
       "      <td>0.984163</td>\n",
       "      <td>0.947839</td>\n",
       "      <td>0.984163</td>\n",
       "      <td>0.909608</td>\n",
       "      <td>0.984163</td>\n",
       "      <td>0.927822</td>\n",
       "      <td>85.269147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.984485</td>\n",
       "      <td>0.984485</td>\n",
       "      <td>0.947195</td>\n",
       "      <td>0.984485</td>\n",
       "      <td>0.902074</td>\n",
       "      <td>0.984485</td>\n",
       "      <td>0.923478</td>\n",
       "      <td>84.893893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.983207</td>\n",
       "      <td>0.983207</td>\n",
       "      <td>0.954255</td>\n",
       "      <td>0.983207</td>\n",
       "      <td>0.897044</td>\n",
       "      <td>0.983207</td>\n",
       "      <td>0.923493</td>\n",
       "      <td>86.076742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.984185               0.984185               0.948481            0.984185   \n",
       "1  0.983483               0.983483               0.942692            0.983483   \n",
       "2  0.982673               0.982673               0.941610            0.982673   \n",
       "3  0.984795               0.984795               0.951683            0.984795   \n",
       "4  0.983838               0.983838               0.952645            0.983838   \n",
       "5  0.982738               0.982738               0.945367            0.982738   \n",
       "6  0.985574               0.985574               0.953635            0.985574   \n",
       "7  0.984163               0.984163               0.947839            0.984163   \n",
       "8  0.984485               0.984485               0.947195            0.984485   \n",
       "9  0.983207               0.983207               0.954255            0.983207   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.896852        0.984185        0.921263       90.381280  \n",
       "1            0.904133        0.983483        0.922510       91.455228  \n",
       "2            0.901140        0.982673        0.920507       91.439573  \n",
       "3            0.909210        0.984795        0.929607       90.933346  \n",
       "4            0.900382        0.983838        0.924893       91.098615  \n",
       "5            0.886782        0.982738        0.914181       89.598135  \n",
       "6            0.912738        0.985574        0.932236       86.936766  \n",
       "7            0.909608        0.984163        0.927822       85.269147  \n",
       "8            0.902074        0.984485        0.923478       84.893893  \n",
       "9            0.897044        0.983207        0.923493       86.076742  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ate_semeval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ate_semeval_stats.to_csv('../results/ate/stats/bert_ate_semeval_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
