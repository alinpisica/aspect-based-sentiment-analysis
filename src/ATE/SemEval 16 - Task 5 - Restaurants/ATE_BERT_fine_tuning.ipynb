{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_Linear import ATE_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_pre_trained_dropout_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_pre_trained_dropout_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3136848211288452\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.4250970184803009\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.23467062413692474\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.20183883607387543\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.22018960118293762\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.1074257642030716\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.16011159121990204\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.1487109661102295\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.08448944985866547\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.10175495594739914\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10951728373765945\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.14191976189613342\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.04640845209360123\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.1718132644891739\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.07284900546073914\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.09145954251289368\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.01075038406997919\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.05614425987005234\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.023991115391254425\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.03584618866443634\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.01741977035999298\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.11627617478370667\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.136258602142334\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.2905256450176239\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.24280525743961334\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.28147873282432556\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.06653360277414322\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.23958812654018402\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.07672208547592163\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.060821015387773514\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.10607057064771652\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.08977189660072327\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.17622728645801544\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.017622927203774452\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.012875115498900414\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.11435408145189285\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.031678058207035065\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.029264137148857117\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.016362104564905167\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.0347614586353302\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.20941907167434692\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.022764472290873528\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.023666486144065857\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.2205961048603058\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.278268814086914\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.31950148940086365\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.21908409893512726\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.2918170392513275\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.12274406105279922\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.1532038152217865\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.2005252093076706\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.21268466114997864\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.06216776743531227\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.21213483810424805\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.019932566210627556\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.08417250961065292\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.10758935660123825\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03817598149180412\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.14309939742088318\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.1162797063589096\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.040688998997211456\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.05468142032623291\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.14082959294319153\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.03975986689329147\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.01032854150980711\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.07229936122894287\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.2973014116287231\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.4189172387123108\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.14972451329231262\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.10906382650136948\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.21809940040111542\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.10297694802284241\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.1738235205411911\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.047799281775951385\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.1266501247882843\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.09223344922065735\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10934265702962875\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.040787696838378906\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.05370314419269562\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.09107746928930283\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.013833428733050823\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.06572207063436508\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.00957905687391758\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.15219825506210327\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.11509300768375397\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01854616403579712\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.07254483550786972\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.26520711183547974\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.9804778099060059\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.15680356323719025\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.21945498883724213\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.08896783739328384\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.11447702348232269\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05345384404063225\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.024073315784335136\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.1265777051448822\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.16478757560253143\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06567460298538208\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10667731612920761\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.10145239531993866\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.008758869022130966\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.06583533436059952\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.10076036304235458\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.0216685701161623\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.062286049127578735\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.1447019875049591\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.13434360921382904\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.048785243183374405\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.027913041412830353\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.07315163314342499\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0196093320846558\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.44502654671669006\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.37642785906791687\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.16296207904815674\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.07757595926523209\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.04725867509841919\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.40204694867134094\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.1793905794620514\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.06734596192836761\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.09421064704656601\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10542501509189606\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.024164769798517227\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.14677631855010986\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03177451342344284\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.059133198112249374\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.13828600943088531\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.072880819439888\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.10655771940946579\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.016458328813314438\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.09696350991725922\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.08577310293912888\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.05818858742713928\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.9909113049507141\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.39360731840133667\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.35410600900650024\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.12689854204654694\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.26228687167167664\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.06272221356630325\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.15702922642230988\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.21936355531215668\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.27105262875556946\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.09816735237836838\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.056709837168455124\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01516616903245449\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.06717275083065033\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.07792099565267563\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.10865366458892822\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.055683914572000504\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.12284044176340103\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.06383173167705536\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.012566866353154182\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.03438206762075424\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.01294923946261406\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.06894814968109131\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.073682427406311\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5618377327919006\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.4204596281051636\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.15919296443462372\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.13991834223270416\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.08512319624423981\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.2254599779844284\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.11197298020124435\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.1440335363149643\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.15205416083335876\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.12395942956209183\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.060161083936691284\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.08871763944625854\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.0755879282951355\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.12278760224580765\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.022409461438655853\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.025007063522934914\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.09122400730848312\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.06868033111095428\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0846320167183876\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.048648651689291\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.22003281116485596\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.8360261917114258\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.26917117834091187\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.21871237456798553\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.11827097833156586\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.24123597145080566\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.2562725841999054\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.14915812015533447\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.24596965312957764\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.34906241297721863\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.1310693323612213\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.11376690864562988\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.10609319061040878\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.028692688792943954\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.035705313086509705\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.1157534122467041\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.012624906376004219\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.042250897735357285\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.04259810969233513\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.015079532749950886\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.02601209469139576\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.030590498819947243\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.09279775619506836\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.2239402532577515\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5934131145477295\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10379642248153687\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.04104812815785408\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.12995003163814545\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.11315907537937164\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.09300221502780914\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.14361616969108582\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.09109779447317123\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.1721923053264618\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10167578607797623\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.02328033372759819\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.12310495227575302\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.059968337416648865\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.08822322636842728\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.02640334889292717\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.01850823499262333\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.050612229853868484\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.07014931738376617\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.04457711800932884\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.03517324849963188\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.011317775584757328\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, BERT_FINE_TUNED_OUTPUT)\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973842</td>\n",
       "      <td>0.973842</td>\n",
       "      <td>0.841771</td>\n",
       "      <td>0.973842</td>\n",
       "      <td>0.941407</td>\n",
       "      <td>0.973842</td>\n",
       "      <td>0.886187</td>\n",
       "      <td>95.567652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.979687</td>\n",
       "      <td>0.979687</td>\n",
       "      <td>0.916294</td>\n",
       "      <td>0.979687</td>\n",
       "      <td>0.901058</td>\n",
       "      <td>0.979687</td>\n",
       "      <td>0.908395</td>\n",
       "      <td>91.334954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.976032</td>\n",
       "      <td>0.976032</td>\n",
       "      <td>0.859331</td>\n",
       "      <td>0.976032</td>\n",
       "      <td>0.925538</td>\n",
       "      <td>0.976032</td>\n",
       "      <td>0.890173</td>\n",
       "      <td>95.048315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.979759</td>\n",
       "      <td>0.979759</td>\n",
       "      <td>0.903937</td>\n",
       "      <td>0.979759</td>\n",
       "      <td>0.914151</td>\n",
       "      <td>0.979759</td>\n",
       "      <td>0.907285</td>\n",
       "      <td>95.601372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978242</td>\n",
       "      <td>0.978242</td>\n",
       "      <td>0.902038</td>\n",
       "      <td>0.978242</td>\n",
       "      <td>0.915560</td>\n",
       "      <td>0.978242</td>\n",
       "      <td>0.905565</td>\n",
       "      <td>96.182762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.972845</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>0.867926</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>0.888331</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>0.877875</td>\n",
       "      <td>96.182066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.868932</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.936461</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.900386</td>\n",
       "      <td>96.276968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.977834</td>\n",
       "      <td>0.977834</td>\n",
       "      <td>0.892837</td>\n",
       "      <td>0.977834</td>\n",
       "      <td>0.889743</td>\n",
       "      <td>0.977834</td>\n",
       "      <td>0.890976</td>\n",
       "      <td>95.795636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.981605</td>\n",
       "      <td>0.981605</td>\n",
       "      <td>0.916278</td>\n",
       "      <td>0.981605</td>\n",
       "      <td>0.908535</td>\n",
       "      <td>0.981605</td>\n",
       "      <td>0.910607</td>\n",
       "      <td>95.937091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.979477</td>\n",
       "      <td>0.979477</td>\n",
       "      <td>0.894592</td>\n",
       "      <td>0.979477</td>\n",
       "      <td>0.917170</td>\n",
       "      <td>0.979477</td>\n",
       "      <td>0.905568</td>\n",
       "      <td>95.837374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.973842               0.973842               0.841771            0.973842   \n",
       "1  0.979687               0.979687               0.916294            0.979687   \n",
       "2  0.976032               0.976032               0.859331            0.976032   \n",
       "3  0.979759               0.979759               0.903937            0.979759   \n",
       "4  0.978242               0.978242               0.902038            0.978242   \n",
       "5  0.972845               0.972845               0.867926            0.972845   \n",
       "6  0.978098               0.978098               0.868932            0.978098   \n",
       "7  0.977834               0.977834               0.892837            0.977834   \n",
       "8  0.981605               0.981605               0.916278            0.981605   \n",
       "9  0.979477               0.979477               0.894592            0.979477   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.941407        0.973842        0.886187       95.567652  \n",
       "1            0.901058        0.979687        0.908395       91.334954  \n",
       "2            0.925538        0.976032        0.890173       95.048315  \n",
       "3            0.914151        0.979759        0.907285       95.601372  \n",
       "4            0.915560        0.978242        0.905565       96.182762  \n",
       "5            0.888331        0.972845        0.877875       96.182066  \n",
       "6            0.936461        0.978098        0.900386       96.276968  \n",
       "7            0.889743        0.977834        0.890976       95.795636  \n",
       "8            0.908535        0.981605        0.910607       95.937091  \n",
       "9            0.917170        0.979477        0.905568       95.837374  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
