{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from models.BERT_Dropout_Linear import BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_pre_trained_dropout_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0914812088012695\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.40233829617500305\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.28810033202171326\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.33813944458961487\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.12924909591674805\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.1765786111354828\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.14727523922920227\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.08501440286636353\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.11077336221933365\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.13617530465126038\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.07167695462703705\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.09104160964488983\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.06506556272506714\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.04870505630970001\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.047677893191576004\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.07685714960098267\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.14508873224258423\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.053749293088912964\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.11230167746543884\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.1350126564502716\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.04976915568113327\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.049922723323106766\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0794651508331299\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.19607895612716675\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.16089491546154022\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.29982316493988037\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.21142379939556122\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.10452722758054733\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.13297918438911438\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.027866559103131294\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0662485733628273\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06205936521291733\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.020787246525287628\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.026319056749343872\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.04583342745900154\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.11144288629293442\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.2031501680612564\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.10628603398799896\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.03293633088469505\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.09434334188699722\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0696331337094307\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0585828572511673\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.06134212762117386\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.016482321545481682\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1107670068740845\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.27047330141067505\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1953950822353363\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.260953813791275\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.3693777918815613\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.13270385563373566\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.09930946677923203\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.12776413559913635\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.4176847040653229\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.07639788836240768\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.19597303867340088\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0829627737402916\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.11692873388528824\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.046346306800842285\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.04460928216576576\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.047834597527980804\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.03225475177168846\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.026246992871165276\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.10706108808517456\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.07061457633972168\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.003526517655700445\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.04755779728293419\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0051289796829224\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.37763792276382446\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.3725273013114929\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.12914444506168365\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.08948598802089691\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.10537448525428772\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.12461768090724945\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.11235135048627853\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.04154007136821747\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.12562675774097443\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.12508173286914825\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.06296946853399277\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.05961255729198456\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.06591272354125977\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.06034937500953674\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.13441772758960724\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.07696305215358734\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.047415509819984436\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.05002284049987793\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.08446996659040451\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.036490216851234436\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.031281303614377975\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.2528573274612427\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.16470064222812653\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.28077173233032227\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.29528680443763733\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.14762981235980988\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.18559785187244415\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.04444674029946327\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.24083158373832703\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.21813037991523743\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.14123137295246124\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.06182809919118881\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.08607674390077591\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.1197667196393013\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03866967186331749\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.08533400297164917\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.08362174034118652\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.0741913765668869\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.0613039992749691\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.14178477227687836\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.02692105434834957\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.011878046207129955\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.10690256953239441\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1486040353775024\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.20943167805671692\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.3579862117767334\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.19988508522510529\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.07676070928573608\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.30234843492507935\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.11449594050645828\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.11601404845714569\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.02137141302227974\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.041177865117788315\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10574693232774734\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.09580506384372711\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.102312371134758\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03580090403556824\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.05846143513917923\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.026503577828407288\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.043703820556402206\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.062496405094861984\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.038610342890024185\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.060682740062475204\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.08037753403186798\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.10357137024402618\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.2503509521484375\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.17178066074848175\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.25047338008880615\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.16603432595729828\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.234750896692276\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.13325661420822144\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.1847105473279953\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.11633536219596863\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.05372117459774017\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.1682959794998169\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.09889686852693558\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.05376579239964485\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.08243021368980408\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.09368903189897537\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.12175864726305008\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.18637247383594513\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.026245003566145897\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.21591119468212128\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.11611459404230118\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.06545871496200562\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.03169004246592522\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.027334902435541153\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1216504573822021\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.1758010983467102\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.14559917151927948\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.25809425115585327\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.11635599285364151\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.1475103348493576\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.08288418501615524\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.13691659271717072\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.08480134606361389\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.12035692483186722\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.13611268997192383\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.06272339075803757\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.07636038959026337\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.13178886473178864\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.04035021364688873\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.035723261535167694\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.11984129995107651\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.017324339598417282\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.05163394287228584\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0921667292714119\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.18726375699043274\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.25790005922317505\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1444646120071411\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.3406839370727539\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.34035494923591614\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.13543643057346344\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.20963987708091736\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05020071193575859\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.21875332295894623\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.13908801972866058\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.08046062290668488\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06207216531038284\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10996730625629425\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.17233827710151672\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.13504773378372192\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.1222030520439148\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.048514679074287415\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.08743622153997421\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.021064452826976776\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.03439878672361374\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.08868750929832458\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0914466604590416\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.008546868339180946\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.10544432699680328\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.2028807401657104\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.43892014026641846\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.36773496866226196\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.18193870782852173\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.1134410873055458\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.09062421321868896\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.2374984622001648\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0752527192234993\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.06721270829439163\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.03595948964357376\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10437432676553726\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0870673730969429\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.049500495195388794\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.06870643049478531\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.020603833720088005\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.1831713318824768\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.38267838954925537\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.06475923210382462\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.06453046947717667\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.22620011866092682\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.071619413793087\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.025159543380141258\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = BERT_Dropout_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.980093</td>\n",
       "      <td>0.980093</td>\n",
       "      <td>0.905790</td>\n",
       "      <td>0.980093</td>\n",
       "      <td>0.909271</td>\n",
       "      <td>0.980093</td>\n",
       "      <td>0.907457</td>\n",
       "      <td>99.004894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.981146</td>\n",
       "      <td>0.981146</td>\n",
       "      <td>0.908735</td>\n",
       "      <td>0.981146</td>\n",
       "      <td>0.918030</td>\n",
       "      <td>0.981146</td>\n",
       "      <td>0.913133</td>\n",
       "      <td>95.168508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.973255</td>\n",
       "      <td>0.973255</td>\n",
       "      <td>0.860948</td>\n",
       "      <td>0.973255</td>\n",
       "      <td>0.899623</td>\n",
       "      <td>0.973255</td>\n",
       "      <td>0.879436</td>\n",
       "      <td>94.875341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.981769</td>\n",
       "      <td>0.981769</td>\n",
       "      <td>0.924386</td>\n",
       "      <td>0.981769</td>\n",
       "      <td>0.896316</td>\n",
       "      <td>0.981769</td>\n",
       "      <td>0.908466</td>\n",
       "      <td>95.501744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.977015</td>\n",
       "      <td>0.977015</td>\n",
       "      <td>0.880359</td>\n",
       "      <td>0.977015</td>\n",
       "      <td>0.906627</td>\n",
       "      <td>0.977015</td>\n",
       "      <td>0.892258</td>\n",
       "      <td>96.837001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.981339</td>\n",
       "      <td>0.981339</td>\n",
       "      <td>0.916477</td>\n",
       "      <td>0.981339</td>\n",
       "      <td>0.901454</td>\n",
       "      <td>0.981339</td>\n",
       "      <td>0.907445</td>\n",
       "      <td>92.344032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.978785</td>\n",
       "      <td>0.978785</td>\n",
       "      <td>0.871879</td>\n",
       "      <td>0.978785</td>\n",
       "      <td>0.930928</td>\n",
       "      <td>0.978785</td>\n",
       "      <td>0.899144</td>\n",
       "      <td>91.709094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.976004</td>\n",
       "      <td>0.976004</td>\n",
       "      <td>0.890628</td>\n",
       "      <td>0.976004</td>\n",
       "      <td>0.892865</td>\n",
       "      <td>0.976004</td>\n",
       "      <td>0.891470</td>\n",
       "      <td>91.271627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.979734</td>\n",
       "      <td>0.979734</td>\n",
       "      <td>0.896081</td>\n",
       "      <td>0.979734</td>\n",
       "      <td>0.909870</td>\n",
       "      <td>0.979734</td>\n",
       "      <td>0.902691</td>\n",
       "      <td>91.441620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.880475</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.936356</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.906869</td>\n",
       "      <td>91.230057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.980093               0.980093               0.905790            0.980093   \n",
       "1  0.981146               0.981146               0.908735            0.981146   \n",
       "2  0.973255               0.973255               0.860948            0.973255   \n",
       "3  0.981769               0.981769               0.924386            0.981769   \n",
       "4  0.977015               0.977015               0.880359            0.977015   \n",
       "5  0.981339               0.981339               0.916477            0.981339   \n",
       "6  0.978785               0.978785               0.871879            0.978785   \n",
       "7  0.976004               0.976004               0.890628            0.976004   \n",
       "8  0.979734               0.979734               0.896081            0.979734   \n",
       "9  0.978947               0.978947               0.880475            0.978947   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.909271        0.980093        0.907457       99.004894  \n",
       "1            0.918030        0.981146        0.913133       95.168508  \n",
       "2            0.899623        0.973255        0.879436       94.875341  \n",
       "3            0.896316        0.981769        0.908466       95.501744  \n",
       "4            0.906627        0.977015        0.892258       96.837001  \n",
       "5            0.901454        0.981339        0.907445       92.344032  \n",
       "6            0.930928        0.978785        0.899144       91.709094  \n",
       "7            0.892865        0.976004        0.891470       91.271627  \n",
       "8            0.909870        0.979734        0.902691       91.441620  \n",
       "9            0.936356        0.978947        0.906869       91.230057  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
