{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_BiLSTM_Linear import ATE_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_pre_trained_dropout_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_pre_trained_dropout_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1784590482711792\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.14135833084583282\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.2543601989746094\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.18058620393276215\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.09585241973400116\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.12657317519187927\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.1151762306690216\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.14240065217018127\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.11094211041927338\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.11946126818656921\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.16318322718143463\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.22438417375087738\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.1883147656917572\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.09735038876533508\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.08541425317525864\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.07892384380102158\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.059401847422122955\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.34586814045906067\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.18798518180847168\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.08363783359527588\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.29065239429473877\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.09547536075115204\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.143061637878418\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.3569719195365906\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.24717426300048828\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.3648395538330078\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.2408205270767212\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.04876437410712242\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.25617659091949463\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.08218849450349808\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.10895130783319473\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.1775078922510147\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.2510964274406433\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.1355341374874115\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.10603475570678711\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.1532357633113861\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.17226733267307281\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.09899844229221344\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.03559381514787674\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.2724757790565491\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.19450458884239197\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.09229491651058197\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.07989548146724701\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.16453582048416138\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.093644142150879\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.40804797410964966\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.2794574201107025\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.32793182134628296\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.2562740743160248\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.07270009070634842\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.17052829265594482\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.16039590537548065\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.1845225840806961\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.08413712680339813\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.05704670771956444\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.12477412074804306\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.2887290120124817\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.2384117990732193\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.13105036318302155\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.11593419313430786\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.10922326147556305\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.08705774694681168\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.06470425426959991\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0914793387055397\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.24138152599334717\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.08132506906986237\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0484341382980347\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.31758084893226624\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.4416247010231018\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.25775960087776184\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.16078655421733856\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.1806965470314026\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.1781245470046997\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.15375401079654694\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.23025870323181152\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.20937608182430267\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.1250305473804474\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.062424689531326294\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.2581467032432556\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.09290909022092819\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.04223077744245529\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.07435054332017899\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.14738193154335022\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.07836800813674927\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.16464199125766754\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0728679969906807\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.07215753942728043\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.09285766631364822\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0139963626861572\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5955489873886108\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.18047142028808594\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.32022884488105774\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.29565054178237915\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.2116808444261551\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.06965956091880798\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.2212553769350052\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.09424342960119247\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0688895732164383\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.1913735270500183\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.12124959379434586\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.26714545488357544\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.11795167624950409\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.13651497662067413\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.14844372868537903\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.19628040492534637\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.09773457050323486\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.08726180344820023\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.10819880664348602\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.08477023988962173\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.06905686855316162\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0396250486373901\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.1725090742111206\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.2816484272480011\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.15728244185447693\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.3239305913448334\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.10766434669494629\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.12785108387470245\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.1252928376197815\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.12369978427886963\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.14697900414466858\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.07025421410799026\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.04744455963373184\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.14912264049053192\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.04190681502223015\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.1914949268102646\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.14270807802677155\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.10038995742797852\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.2226315289735794\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.13483549654483795\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.100102998316288\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.06065314635634422\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.2577725946903229\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.050809621810913\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.41942936182022095\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.5205636024475098\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.4811036288738251\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.3651593029499054\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.16315993666648865\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.29645970463752747\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.11661673337221146\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.14445658028125763\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.14370675384998322\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.11242568492889404\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.14460372924804688\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.05810331180691719\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.11189135164022446\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.042031314224004745\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.18297769129276276\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.08759523183107376\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.12393726408481598\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.08705843985080719\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.05848739668726921\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.20956222712993622\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.13153885304927826\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1277384757995605\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.4966926574707031\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.20877239108085632\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.22172193229198456\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.3375367224216461\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.27053478360176086\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.07925189286470413\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.15640932321548462\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.22677823901176453\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.10361151397228241\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.2046879082918167\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0759374350309372\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.08363723009824753\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.11703740805387497\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.08841021358966827\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.12439887225627899\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.11123625934123993\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.13805140554904938\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.04170200973749161\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.09192853420972824\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.04010447487235069\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.08717014640569687\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1037553548812866\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.1546071618795395\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.4023589789867401\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.3343806266784668\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.40385112166404724\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.1909247636795044\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.47105035185813904\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.13694611191749573\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.08385250717401505\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.08138803392648697\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.18935859203338623\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.09634167701005936\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.17096012830734253\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.11600321531295776\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.15678565204143524\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.11216200143098831\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.11933546513319016\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.0428890623152256\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.07613041996955872\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.24128194153308868\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.023619722574949265\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.10563696175813675\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0545172691345215\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.6506715416908264\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1860901266336441\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.17393654584884644\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.23939061164855957\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.23802441358566284\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.23727108538150787\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.12063430994749069\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.11952327191829681\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0557924285531044\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10389748215675354\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.2849184274673462\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.1514924019575119\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.04381026700139046\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.09314791858196259\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.16543053090572357\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.13397182524204254\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.1558622121810913\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.04507756978273392\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.09127907454967499\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.06904754787683487\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.10668959468603134\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, bilstm_in_features=256, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.965128</td>\n",
       "      <td>0.965128</td>\n",
       "      <td>0.827280</td>\n",
       "      <td>0.965128</td>\n",
       "      <td>0.816515</td>\n",
       "      <td>0.965128</td>\n",
       "      <td>0.816191</td>\n",
       "      <td>104.859754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.967367</td>\n",
       "      <td>0.967367</td>\n",
       "      <td>0.861129</td>\n",
       "      <td>0.967367</td>\n",
       "      <td>0.771498</td>\n",
       "      <td>0.967367</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>102.998258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.969262</td>\n",
       "      <td>0.969262</td>\n",
       "      <td>0.833991</td>\n",
       "      <td>0.969262</td>\n",
       "      <td>0.846882</td>\n",
       "      <td>0.969262</td>\n",
       "      <td>0.839959</td>\n",
       "      <td>101.319661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.965203</td>\n",
       "      <td>0.965203</td>\n",
       "      <td>0.822567</td>\n",
       "      <td>0.965203</td>\n",
       "      <td>0.869725</td>\n",
       "      <td>0.965203</td>\n",
       "      <td>0.844654</td>\n",
       "      <td>101.062379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.967786</td>\n",
       "      <td>0.967786</td>\n",
       "      <td>0.848732</td>\n",
       "      <td>0.967786</td>\n",
       "      <td>0.815888</td>\n",
       "      <td>0.967786</td>\n",
       "      <td>0.827505</td>\n",
       "      <td>100.821145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.961464</td>\n",
       "      <td>0.961464</td>\n",
       "      <td>0.760754</td>\n",
       "      <td>0.961464</td>\n",
       "      <td>0.831703</td>\n",
       "      <td>0.961464</td>\n",
       "      <td>0.793075</td>\n",
       "      <td>100.972017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.971329</td>\n",
       "      <td>0.971329</td>\n",
       "      <td>0.835795</td>\n",
       "      <td>0.971329</td>\n",
       "      <td>0.831296</td>\n",
       "      <td>0.971329</td>\n",
       "      <td>0.831751</td>\n",
       "      <td>101.054660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.964586</td>\n",
       "      <td>0.964586</td>\n",
       "      <td>0.784206</td>\n",
       "      <td>0.964586</td>\n",
       "      <td>0.867238</td>\n",
       "      <td>0.964586</td>\n",
       "      <td>0.821632</td>\n",
       "      <td>101.390086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.963997</td>\n",
       "      <td>0.963997</td>\n",
       "      <td>0.826384</td>\n",
       "      <td>0.963997</td>\n",
       "      <td>0.838007</td>\n",
       "      <td>0.963997</td>\n",
       "      <td>0.825242</td>\n",
       "      <td>101.483335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.967404</td>\n",
       "      <td>0.967404</td>\n",
       "      <td>0.839691</td>\n",
       "      <td>0.967404</td>\n",
       "      <td>0.798402</td>\n",
       "      <td>0.967404</td>\n",
       "      <td>0.806630</td>\n",
       "      <td>100.968574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.965128               0.965128               0.827280            0.965128   \n",
       "1  0.967367               0.967367               0.861129            0.967367   \n",
       "2  0.969262               0.969262               0.833991            0.969262   \n",
       "3  0.965203               0.965203               0.822567            0.965203   \n",
       "4  0.967786               0.967786               0.848732            0.967786   \n",
       "5  0.961464               0.961464               0.760754            0.961464   \n",
       "6  0.971329               0.971329               0.835795            0.971329   \n",
       "7  0.964586               0.964586               0.784206            0.964586   \n",
       "8  0.963997               0.963997               0.826384            0.963997   \n",
       "9  0.967404               0.967404               0.839691            0.967404   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.816515        0.965128        0.816191      104.859754  \n",
       "1            0.771498        0.967367        0.802933      102.998258  \n",
       "2            0.846882        0.969262        0.839959      101.319661  \n",
       "3            0.869725        0.965203        0.844654      101.062379  \n",
       "4            0.815888        0.967786        0.827505      100.821145  \n",
       "5            0.831703        0.961464        0.793075      100.972017  \n",
       "6            0.831296        0.971329        0.831751      101.054660  \n",
       "7            0.867238        0.964586        0.821632      101.390086  \n",
       "8            0.838007        0.963997        0.825242      101.483335  \n",
       "9            0.798402        0.967404        0.806630      100.968574  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
