{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from models.BERT_Dropout_BiLSTM_Linear import BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_pre_trained_dropout_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_pre_trained_dropout_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1339452266693115\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5134332180023193\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.39175689220428467\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.15073518455028534\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.18087317049503326\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.19568811357021332\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.22810862958431244\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.20390141010284424\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.2804506719112396\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.13913506269454956\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.05047709494829178\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.1428849697113037\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.03153345361351967\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.08569502085447311\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.19120566546916962\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.04239630699157715\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.028872821480035782\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.1451580673456192\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.13491784036159515\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.1021563708782196\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.02742697298526764\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.1452019065618515\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0570083856582642\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.39774373173713684\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.3123171925544739\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.2420610785484314\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.26733511686325073\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.12204237282276154\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.1379515528678894\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.1444559395313263\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.18392905592918396\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.08601728826761246\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.15325012803077698\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.16305239498615265\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.10412748903036118\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.04074693098664284\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.05056639760732651\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.14492377638816833\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.06268678605556488\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.08464977890253067\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.32767853140830994\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.12836216390132904\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.08063221722841263\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.05103745311498642\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1331666707992554\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.26790931820869446\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.3308069407939911\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.3375096619129181\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.12387647479772568\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.10878684371709824\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.09006180614233017\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.16875213384628296\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.16250191628932953\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.05485136806964874\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.072776198387146\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.07200010120868683\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.12970754504203796\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.1088695079088211\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.2316262573003769\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.04771772399544716\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.1523752510547638\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.1320594698190689\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.12281427532434464\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.10530123859643936\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.24878785014152527\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.14883965253829956\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0754146575927734\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.2801874577999115\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.33889493346214294\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.2373245507478714\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.15531153976917267\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.17817673087120056\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.13415613770484924\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.16319844126701355\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.1155913695693016\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.11642918735742569\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.09824519604444504\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.139524444937706\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.14683124423027039\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.13377724587917328\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.07294522970914841\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.31999680399894714\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.028436528518795967\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.13516579568386078\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0901261642575264\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.09593862295150757\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.06057702377438545\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.09395968168973923\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1667778491973877\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.35175302624702454\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.5051103234291077\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.09375791996717453\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.20465295016765594\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.3360338807106018\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.20445410907268524\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.07773949205875397\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.13623715937137604\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.1466429978609085\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.18774349987506866\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.24739687144756317\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.057045549154281616\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.1069100871682167\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.09140621870756149\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.16142871975898743\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.1242084875702858\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.05960110202431679\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.203854039311409\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.16819901764392853\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.05667859688401222\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.08526092022657394\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0857168436050415\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.4426450729370117\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.5703697204589844\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.17940036952495575\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.22269907593727112\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.18790192902088165\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.1513093113899231\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.22832246124744415\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.180527001619339\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.24270173907279968\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.23275087773799896\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.17038297653198242\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.055963825434446335\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.17506852746009827\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.07440450042486191\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.15842889249324799\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.05474890023469925\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.05884404852986336\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.08183430135250092\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.1808449625968933\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.07208483666181564\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.0727328360080719\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1677175760269165\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.36708804965019226\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.2604854702949524\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.3094928562641144\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.16756927967071533\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.3347161114215851\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.1323840320110321\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.13139601051807404\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.27441418170928955\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06032625958323479\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.11288417130708694\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.14740285277366638\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.31240391731262207\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.13611337542533875\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.22475294768810272\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.03789328411221504\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.11626510322093964\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.11665105819702148\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.14882056415081024\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.07645729929208755\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.14635561406612396\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.3280766010284424\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.024248719215393\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.23513191938400269\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.6685501337051392\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.39797088503837585\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.20931977033615112\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.32306969165802\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.19548970460891724\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.15306410193443298\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.08458364009857178\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.08842429518699646\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.019240088760852814\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.17365436255931854\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.27229735255241394\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.2948657274246216\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.10462142527103424\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.02987002581357956\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.05848649889230728\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.1701711118221283\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.08891449123620987\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.07348664104938507\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.06448949128389359\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.09448166936635971\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1586967706680298\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.3053230941295624\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.48934710025787354\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.2874743640422821\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.3656585216522217\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.3270592987537384\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.20320801436901093\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.19157344102859497\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.24607926607131958\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.16633757948875427\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.14121605455875397\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0689229965209961\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.08285866677761078\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.0916227176785469\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.05570604279637337\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.1301782876253128\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.14611728489398956\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.1204570010304451\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.09541720896959305\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.08483927696943283\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.09249777346849442\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.028881674632430077\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0277800559997559\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.17162814736366272\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.42235979437828064\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.19704730808734894\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.23841232061386108\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.16502070426940918\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.08259305357933044\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.13879884779453278\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.051742736250162125\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.1381670981645584\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.17369715869426727\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.06423775106668472\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.11331958323717117\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.08013240247964859\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.19330380856990814\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.2147214263677597\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.11079023033380508\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.0971161425113678\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.13354644179344177\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0413149818778038\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.18322181701660156\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.07615301758050919\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = BERT_Dropout_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, bilstm_in_features=256, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.968973</td>\n",
       "      <td>0.968973</td>\n",
       "      <td>0.824030</td>\n",
       "      <td>0.968973</td>\n",
       "      <td>0.826076</td>\n",
       "      <td>0.968973</td>\n",
       "      <td>0.823478</td>\n",
       "      <td>114.479259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.963371</td>\n",
       "      <td>0.963371</td>\n",
       "      <td>0.838970</td>\n",
       "      <td>0.963371</td>\n",
       "      <td>0.801146</td>\n",
       "      <td>0.963371</td>\n",
       "      <td>0.798742</td>\n",
       "      <td>110.475375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.970810</td>\n",
       "      <td>0.970810</td>\n",
       "      <td>0.848723</td>\n",
       "      <td>0.970810</td>\n",
       "      <td>0.826299</td>\n",
       "      <td>0.970810</td>\n",
       "      <td>0.836264</td>\n",
       "      <td>107.795127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.966620</td>\n",
       "      <td>0.966620</td>\n",
       "      <td>0.803690</td>\n",
       "      <td>0.966620</td>\n",
       "      <td>0.843743</td>\n",
       "      <td>0.966620</td>\n",
       "      <td>0.821871</td>\n",
       "      <td>113.568082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.967966</td>\n",
       "      <td>0.967966</td>\n",
       "      <td>0.827648</td>\n",
       "      <td>0.967966</td>\n",
       "      <td>0.832434</td>\n",
       "      <td>0.967966</td>\n",
       "      <td>0.827483</td>\n",
       "      <td>113.804849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.970737</td>\n",
       "      <td>0.970737</td>\n",
       "      <td>0.849157</td>\n",
       "      <td>0.970737</td>\n",
       "      <td>0.842819</td>\n",
       "      <td>0.970737</td>\n",
       "      <td>0.844711</td>\n",
       "      <td>108.592092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.954054</td>\n",
       "      <td>0.954054</td>\n",
       "      <td>0.756217</td>\n",
       "      <td>0.954054</td>\n",
       "      <td>0.668436</td>\n",
       "      <td>0.954054</td>\n",
       "      <td>0.639836</td>\n",
       "      <td>111.354443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.971528</td>\n",
       "      <td>0.971528</td>\n",
       "      <td>0.850040</td>\n",
       "      <td>0.971528</td>\n",
       "      <td>0.850557</td>\n",
       "      <td>0.971528</td>\n",
       "      <td>0.850297</td>\n",
       "      <td>111.676461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.865857</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.816864</td>\n",
       "      <td>110.984551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.969122</td>\n",
       "      <td>0.969122</td>\n",
       "      <td>0.821398</td>\n",
       "      <td>0.969122</td>\n",
       "      <td>0.808392</td>\n",
       "      <td>0.969122</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>109.699027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.968973               0.968973               0.824030            0.968973   \n",
       "1  0.963371               0.963371               0.838970            0.963371   \n",
       "2  0.970810               0.970810               0.848723            0.970810   \n",
       "3  0.966620               0.966620               0.803690            0.966620   \n",
       "4  0.967966               0.967966               0.827648            0.967966   \n",
       "5  0.970737               0.970737               0.849157            0.970737   \n",
       "6  0.954054               0.954054               0.756217            0.954054   \n",
       "7  0.971528               0.971528               0.850040            0.971528   \n",
       "8  0.964305               0.964305               0.800726            0.964305   \n",
       "9  0.969122               0.969122               0.821398            0.969122   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.826076        0.968973        0.823478      114.479259  \n",
       "1            0.801146        0.963371        0.798742      110.475375  \n",
       "2            0.826299        0.970810        0.836264      107.795127  \n",
       "3            0.843743        0.966620        0.821871      113.568082  \n",
       "4            0.832434        0.967966        0.827483      113.804849  \n",
       "5            0.842819        0.970737        0.844711      108.592092  \n",
       "6            0.668436        0.954054        0.639836      111.354443  \n",
       "7            0.850557        0.971528        0.850297      111.676461  \n",
       "8            0.865857        0.964305        0.816864      110.984551  \n",
       "9            0.808392        0.969122        0.806400      109.699027  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
