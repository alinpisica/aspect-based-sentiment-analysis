{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_BiLSTM_Linear import ATE_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned_512.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned_dropout_bilstm_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_fine_tuned_dropout_bilstm_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0567107200622559\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.06472396850585938\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.03597085177898407\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.020604435354471207\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.017084496095776558\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.022459881380200386\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.012631247751414776\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.012077114544808865\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.009210973046720028\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.008592245168983936\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.010511224158108234\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.015417607501149178\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.013595931231975555\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.00958581455051899\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.02069658599793911\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.022258061915636063\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.004459151532500982\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.009741871617734432\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.009794924408197403\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.005302230827510357\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.0032362542115151882\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.004634300246834755\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0502203702926636\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.0754614770412445\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.04170023649930954\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.029969610273838043\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.01734648458659649\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.013856290839612484\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.013587740249931812\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.00872570089995861\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.016499532386660576\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.013687454164028168\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.011154106818139553\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.014689276926219463\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.015519044362008572\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.00603315606713295\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.009148611687123775\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.012575996108353138\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.015232972800731659\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.00556156923994422\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0064561255276203156\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0030544588807970285\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.006482933647930622\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.010889929719269276\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.2200217247009277\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.06014540418982506\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.03086230345070362\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.03153558820486069\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.02193852886557579\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.01741485670208931\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.010971675626933575\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.009530412033200264\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.011753191240131855\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.01458483375608921\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.009742516092956066\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0076903365552425385\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.012593548744916916\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.009137839078903198\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.006690663285553455\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.016265874728560448\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.01518947258591652\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.005443838890641928\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.007017793599516153\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.008163532242178917\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.012300620786845684\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.01935301534831524\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0280488729476929\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.05715419352054596\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.03201804310083389\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.02091507613658905\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.025871602818369865\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.014175818301737309\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.019219521433115005\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.009751571342349052\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.009504144079983234\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.009578465484082699\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.013251326978206635\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.007045607548207045\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.009554460644721985\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.01743411459028721\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.005407009739428759\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.004636103753000498\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.00478353351354599\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.0030505408067256212\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.00419117184355855\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01886892318725586\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.006508336402475834\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.005713693797588348\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.9787171483039856\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.05938069149851799\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.03136168420314789\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.02242259867489338\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.021278036758303642\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.008810864761471748\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.016989007592201233\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.008277205750346184\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.010711582377552986\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.014438350684940815\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.017030810937285423\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.006326728034764528\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.004474921151995659\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.011908553540706635\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.010105033405125141\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.012542137876152992\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.00676681287586689\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.008301996625959873\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.005666894838213921\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.008274581283330917\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.012676158919930458\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.009236287325620651\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0844708681106567\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.06581877171993256\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.05091245099902153\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.022581590339541435\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.02697119303047657\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.01544873509556055\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.015955328941345215\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.013418449088931084\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.01050196960568428\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.014732378534972668\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.019100084900856018\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01659461483359337\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.008666805922985077\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.011840948835015297\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.011390355415642262\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.016855012625455856\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.008389240130782127\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.006841897964477539\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.005236760713160038\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.007385808043181896\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.013222267851233482\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.004756505135446787\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1705869436264038\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.09161681681871414\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.0495937317609787\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.0235397070646286\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.017154384404420853\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.014807463623583317\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.010133178904652596\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.016744211316108704\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.01328558661043644\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.007068430073559284\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.009947543032467365\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.011953211389482021\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.008304468356072903\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.007957253605127335\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.00831469614058733\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.013934724032878876\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.012811734341084957\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.006880022585391998\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.008695589378476143\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.010777551680803299\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.004190970212221146\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.008452541194856167\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.308883786201477\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.05923246592283249\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.03532532975077629\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.021994534879922867\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.028655633330345154\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.013912760652601719\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.00940537080168724\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.009611209854483604\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.012351397424936295\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.010692676529288292\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.019319936633110046\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.007781742140650749\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.012461097911000252\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.007171095814555883\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.005009425804018974\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.006102197337895632\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.01308316271752119\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.008084981702268124\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.01125466637313366\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.011752932332456112\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.005917822476476431\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.012271784245967865\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1657207012176514\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.06438621878623962\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.03696106746792793\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.023010477423667908\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.016852205619215965\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.017126213759183884\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.012924936600029469\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.013717342168092728\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.011507336981594563\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.00898092333227396\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.012689958326518536\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.02034139819443226\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.008514156565070152\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.004303991328924894\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.015472786501049995\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.0070193433202803135\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.004151375498622656\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.006414411589503288\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.008422527462244034\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.010762318037450314\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.005432672332972288\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.004785114899277687\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1127351522445679\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.0927254930138588\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.04074195772409439\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.029124196618795395\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.020797695964574814\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.018592078238725662\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.019224870949983597\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.015577222220599651\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.009797562845051289\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.007379175163805485\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.01458165142685175\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.006242532283067703\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.010842959396541119\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.007681081537157297\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.00891595333814621\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.005500745959579945\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.004852611571550369\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.012088295072317123\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.005330889951437712\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01131505612283945\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.003544681705534458\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.009003388695418835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, bilstm_in_features=256, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "\n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.savefig(f'../../../results/ATE/SemEval16 - Task 5 - Restaurants/plots/bert_ft_do_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.804590</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.762586</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.766777</td>\n",
       "      <td>371.374263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997637</td>\n",
       "      <td>0.997637</td>\n",
       "      <td>0.808683</td>\n",
       "      <td>0.997637</td>\n",
       "      <td>0.694958</td>\n",
       "      <td>0.997637</td>\n",
       "      <td>0.681244</td>\n",
       "      <td>369.681169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997930</td>\n",
       "      <td>0.997930</td>\n",
       "      <td>0.809913</td>\n",
       "      <td>0.997930</td>\n",
       "      <td>0.729670</td>\n",
       "      <td>0.997930</td>\n",
       "      <td>0.724607</td>\n",
       "      <td>367.644200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.997941</td>\n",
       "      <td>0.997941</td>\n",
       "      <td>0.853911</td>\n",
       "      <td>0.997941</td>\n",
       "      <td>0.719148</td>\n",
       "      <td>0.997941</td>\n",
       "      <td>0.730887</td>\n",
       "      <td>342.944377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.996969</td>\n",
       "      <td>0.996969</td>\n",
       "      <td>0.803811</td>\n",
       "      <td>0.996969</td>\n",
       "      <td>0.703065</td>\n",
       "      <td>0.996969</td>\n",
       "      <td>0.663990</td>\n",
       "      <td>338.750309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.998070</td>\n",
       "      <td>0.998070</td>\n",
       "      <td>0.811506</td>\n",
       "      <td>0.998070</td>\n",
       "      <td>0.821247</td>\n",
       "      <td>0.998070</td>\n",
       "      <td>0.807674</td>\n",
       "      <td>333.296661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.998051</td>\n",
       "      <td>0.998051</td>\n",
       "      <td>0.839275</td>\n",
       "      <td>0.998051</td>\n",
       "      <td>0.750791</td>\n",
       "      <td>0.998051</td>\n",
       "      <td>0.773922</td>\n",
       "      <td>333.545840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.997477</td>\n",
       "      <td>0.997477</td>\n",
       "      <td>0.841024</td>\n",
       "      <td>0.997477</td>\n",
       "      <td>0.741295</td>\n",
       "      <td>0.997477</td>\n",
       "      <td>0.731997</td>\n",
       "      <td>334.066866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.998039</td>\n",
       "      <td>0.998039</td>\n",
       "      <td>0.836772</td>\n",
       "      <td>0.998039</td>\n",
       "      <td>0.715828</td>\n",
       "      <td>0.998039</td>\n",
       "      <td>0.715179</td>\n",
       "      <td>333.055540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.997664</td>\n",
       "      <td>0.997664</td>\n",
       "      <td>0.814113</td>\n",
       "      <td>0.997664</td>\n",
       "      <td>0.642507</td>\n",
       "      <td>0.997664</td>\n",
       "      <td>0.607791</td>\n",
       "      <td>334.286637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.997879               0.997879               0.804590            0.997879   \n",
       "1  0.997637               0.997637               0.808683            0.997637   \n",
       "2  0.997930               0.997930               0.809913            0.997930   \n",
       "3  0.997941               0.997941               0.853911            0.997941   \n",
       "4  0.996969               0.996969               0.803811            0.996969   \n",
       "5  0.998070               0.998070               0.811506            0.998070   \n",
       "6  0.998051               0.998051               0.839275            0.998051   \n",
       "7  0.997477               0.997477               0.841024            0.997477   \n",
       "8  0.998039               0.998039               0.836772            0.998039   \n",
       "9  0.997664               0.997664               0.814113            0.997664   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.762586        0.997879        0.766777      371.374263  \n",
       "1            0.694958        0.997637        0.681244      369.681169  \n",
       "2            0.729670        0.997930        0.724607      367.644200  \n",
       "3            0.719148        0.997941        0.730887      342.944377  \n",
       "4            0.703065        0.996969        0.663990      338.750309  \n",
       "5            0.821247        0.998070        0.807674      333.296661  \n",
       "6            0.750791        0.998051        0.773922      333.545840  \n",
       "7            0.741295        0.997477        0.731997      334.066866  \n",
       "8            0.715828        0.998039        0.715179      333.055540  \n",
       "9            0.642507        0.997664        0.607791      334.286637  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
