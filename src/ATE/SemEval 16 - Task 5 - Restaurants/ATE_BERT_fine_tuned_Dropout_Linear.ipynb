{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from models.BERT_Dropout_Linear import BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned_dropout_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_fine_tuned_dropout_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.2289338111877441\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.03917279839515686\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.04291333630681038\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.03665787726640701\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.11102994531393051\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.00852618645876646\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.05070319026708603\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.05251587554812431\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.044955626130104065\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.12797310948371887\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.05957163870334625\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0069754584692418575\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.013585255481302738\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.006631867960095406\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.016744136810302734\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.00780668156221509\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.005581907462328672\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.0026953588239848614\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.006987860891968012\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0011499172542244196\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.05862322822213173\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.01509043201804161\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3174700736999512\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.0964294895529747\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.03857416287064552\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.0452997200191021\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.7401041984558105\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03561596944928169\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.028970105573534966\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.020563745871186256\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.01791481114923954\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.04664142429828644\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.014341221190989017\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01642044261097908\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.01358411181718111\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.0733700692653656\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.009977016597986221\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.020087048411369324\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.09570655971765518\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.036255501210689545\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.04466688632965088\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.03364205360412598\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.03827531263232231\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.0010383103508502245\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.7913289070129395\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.07333652675151825\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.028814267367124557\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.05100351199507713\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.022263500839471817\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.0665082111954689\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03015657514333725\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.013646290637552738\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.011234243400394917\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.13160568475723267\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03350554406642914\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.006884700618684292\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.0029992826748639345\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.06619028747081757\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.045938972383737564\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.004983316175639629\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.02442609705030918\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.024107959121465683\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.03312975540757179\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.10894093662500381\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.0028377536218613386\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.0525013692677021\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3096697330474854\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.061309054493904114\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.054601989686489105\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.015611755661666393\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.0039703622460365295\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.09182652831077576\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.030879028141498566\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0635780319571495\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.05151247978210449\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.05218026787042618\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03363331779837608\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.20496398210525513\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.008420783095061779\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.004133020527660847\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.01751384325325489\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.0036593894474208355\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.009911914356052876\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.007946926169097424\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.017507512122392654\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.008539491333067417\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.002312702825292945\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.01043982245028019\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0679442882537842\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.0569402351975441\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.013830162584781647\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.09933705627918243\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.10478922724723816\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.010308659635484219\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.059712864458560944\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.05446111410856247\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.13922591507434845\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0039000229444354773\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.11031319200992584\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.05293263494968414\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.08488722145557404\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.08080905675888062\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.3558327555656433\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.03133358433842659\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.03610233962535858\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.025921065360307693\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.00931567046791315\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.06980140507221222\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.04456698149442673\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.036563366651535034\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1654247045516968\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.06583885103464127\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.08700238913297653\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.035093702375888824\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.01563057117164135\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.01270182803273201\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.006845313124358654\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.02159208245575428\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.030092164874076843\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.03926393389701843\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.07830318808555603\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.005497222766280174\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.004780979827046394\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.006415591575205326\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.026281844824552536\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.015293337404727936\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.03409723564982414\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.021455839276313782\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.002871283795684576\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.09385073930025101\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.05493761599063873\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.01132019329816103\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0056294202804565\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.13884161412715912\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.038504716008901596\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.004616259131580591\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.07507801800966263\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.042925700545310974\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.07613864541053772\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.04624705761671066\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.028188202530145645\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.014019805006682873\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.026920277625322342\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.06084229052066803\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.008836035616695881\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.0061544873751699924\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.048052072525024414\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.024996336549520493\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.00887820590287447\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.06088471785187721\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.008700905367732048\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.002710479311645031\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.031002286821603775\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.0007248112233355641\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1109163761138916\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.09221817553043365\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.02107049897313118\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.06831881403923035\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03367083519697189\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.14119932055473328\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.01777496375143528\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.04164605587720871\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.03783194348216057\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0442202128469944\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.004626563284546137\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.02669314667582512\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.004260607995092869\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.008157039992511272\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.04108399897813797\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.006117681507021189\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.02775450237095356\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.017086153849959373\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.04738926887512207\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.003454794641584158\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.03517676144838333\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.0048870001919567585\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.295700192451477\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.12236665189266205\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.04538356512784958\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.05879751592874527\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.01388498768210411\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.014891249127686024\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.013254772871732712\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.054576948285102844\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0028579954523593187\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.00286076869815588\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03551045060157776\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.007892899215221405\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.038633283227682114\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.004316659644246101\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.04345741868019104\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.004034471232444048\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.005478970240801573\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.009579687379300594\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.01941581256687641\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.008237726055085659\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.07283344864845276\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.0114187803119421\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.8382770419120789\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.08123617619276047\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.039738692343235016\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.010759216733276844\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.026031585410237312\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.006842132192105055\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.04896770790219307\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.04140559211373329\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.008173075504601002\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06167352944612503\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.006346168462187052\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.14425347745418549\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.05635743588209152\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.0028504692018032074\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.00952156912535429\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.024094583466649055\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.004139499738812447\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.00702217873185873\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.012625551782548428\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.038947176188230515\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.008164850994944572\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.001156255486421287\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = BERT_Dropout_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.991157</td>\n",
       "      <td>0.991157</td>\n",
       "      <td>0.946290</td>\n",
       "      <td>0.991157</td>\n",
       "      <td>0.971335</td>\n",
       "      <td>0.991157</td>\n",
       "      <td>0.958443</td>\n",
       "      <td>105.077601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990346</td>\n",
       "      <td>0.990346</td>\n",
       "      <td>0.935645</td>\n",
       "      <td>0.990346</td>\n",
       "      <td>0.978509</td>\n",
       "      <td>0.990346</td>\n",
       "      <td>0.956198</td>\n",
       "      <td>95.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.994720</td>\n",
       "      <td>0.994720</td>\n",
       "      <td>0.978735</td>\n",
       "      <td>0.994720</td>\n",
       "      <td>0.981495</td>\n",
       "      <td>0.994720</td>\n",
       "      <td>0.980092</td>\n",
       "      <td>95.344027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992277</td>\n",
       "      <td>0.992277</td>\n",
       "      <td>0.957825</td>\n",
       "      <td>0.992277</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.992277</td>\n",
       "      <td>0.968216</td>\n",
       "      <td>95.486997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.991278</td>\n",
       "      <td>0.991278</td>\n",
       "      <td>0.946407</td>\n",
       "      <td>0.991278</td>\n",
       "      <td>0.964484</td>\n",
       "      <td>0.991278</td>\n",
       "      <td>0.955286</td>\n",
       "      <td>96.287032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.989744</td>\n",
       "      <td>0.989744</td>\n",
       "      <td>0.951350</td>\n",
       "      <td>0.989744</td>\n",
       "      <td>0.955265</td>\n",
       "      <td>0.989744</td>\n",
       "      <td>0.953235</td>\n",
       "      <td>95.160029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.984501</td>\n",
       "      <td>0.984501</td>\n",
       "      <td>0.914494</td>\n",
       "      <td>0.984501</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.984501</td>\n",
       "      <td>0.939322</td>\n",
       "      <td>95.744032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.991775</td>\n",
       "      <td>0.991775</td>\n",
       "      <td>0.955562</td>\n",
       "      <td>0.991775</td>\n",
       "      <td>0.962752</td>\n",
       "      <td>0.991775</td>\n",
       "      <td>0.959061</td>\n",
       "      <td>95.143997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.989735</td>\n",
       "      <td>0.989735</td>\n",
       "      <td>0.935984</td>\n",
       "      <td>0.989735</td>\n",
       "      <td>0.964182</td>\n",
       "      <td>0.989735</td>\n",
       "      <td>0.949658</td>\n",
       "      <td>96.333028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.993289</td>\n",
       "      <td>0.993289</td>\n",
       "      <td>0.963006</td>\n",
       "      <td>0.993289</td>\n",
       "      <td>0.979815</td>\n",
       "      <td>0.993289</td>\n",
       "      <td>0.971280</td>\n",
       "      <td>97.102969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.991157               0.991157               0.946290            0.991157   \n",
       "1  0.990346               0.990346               0.935645            0.990346   \n",
       "2  0.994720               0.994720               0.978735            0.994720   \n",
       "3  0.992277               0.992277               0.957825            0.992277   \n",
       "4  0.991278               0.991278               0.946407            0.991278   \n",
       "5  0.989744               0.989744               0.951350            0.989744   \n",
       "6  0.984501               0.984501               0.914494            0.984501   \n",
       "7  0.991775               0.991775               0.955562            0.991775   \n",
       "8  0.989735               0.989735               0.935984            0.989735   \n",
       "9  0.993289               0.993289               0.963006            0.993289   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.971335        0.991157        0.958443      105.077601  \n",
       "1            0.978509        0.990346        0.956198       95.691000  \n",
       "2            0.981495        0.994720        0.980092       95.344027  \n",
       "3            0.979021        0.992277        0.968216       95.486997  \n",
       "4            0.964484        0.991278        0.955286       96.287032  \n",
       "5            0.955265        0.989744        0.953235       95.160029  \n",
       "6            0.966842        0.984501        0.939322       95.744032  \n",
       "7            0.962752        0.991775        0.959061       95.143997  \n",
       "8            0.964182        0.989735        0.949658       96.333028  \n",
       "9            0.979815        0.993289        0.971280       97.102969  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
