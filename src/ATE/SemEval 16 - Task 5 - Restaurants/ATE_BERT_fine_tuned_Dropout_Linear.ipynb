{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_Linear import ATE_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned_dropout_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_fine_tuned_dropout_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1791822910308838\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.06299033761024475\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.027463695034384727\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.07536836713552475\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.05586967617273331\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.11766643822193146\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.05079363286495209\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.019494501873850822\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.03275502845644951\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.04453429579734802\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.05034754052758217\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.011222917586565018\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.00599991949275136\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.10742638260126114\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.006061771418899298\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.017336741089820862\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.012046333402395248\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.009807547554373741\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.027128148823976517\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.00755654601380229\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.020454157143831253\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.10264995694160461\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.8248075842857361\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.04841706529259682\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.09710051864385605\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.03162137046456337\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.04906435310840607\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.06776577979326248\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.006620130036026239\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.017506366595625877\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.003376615699380636\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.029197705909609795\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.010291941463947296\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.05603788420557976\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.007940390147268772\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.003612132277339697\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.04894208535552025\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.006168001797050238\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.0209828969091177\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.018579009920358658\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.02900678664445877\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0291974488645792\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.004693822003901005\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.0039256117306649685\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3072748184204102\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.08968957513570786\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10433537513017654\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.050367359071969986\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.09116953611373901\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.2045753449201584\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.04654252529144287\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.02848546952009201\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.030298985540866852\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.09030426293611526\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.010227993130683899\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.10261088609695435\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.01826580800116062\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03357012942433357\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.029647359624505043\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.001750455005094409\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.004582071676850319\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.040522005409002304\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.003949414007365704\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.010735840536653996\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.010134570300579071\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.14776675403118134\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0718176364898682\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.11728812009096146\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.05237904563546181\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.04887361451983452\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.06075534597039223\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.016142712906003\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.006517719477415085\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0052420226857066154\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.13306419551372528\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06393495947122574\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.04196500778198242\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.033089909702539444\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.04377901926636696\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.031537752598524094\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.06694367527961731\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.029102828353643417\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.02645907923579216\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.06419264525175095\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0018796168733388186\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.05775252729654312\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.001769208232872188\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.017872516065835953\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0478670597076416\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.06714684516191483\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.02310870960354805\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.05356832966208458\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03221864625811577\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.030939152464270592\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.006150754168629646\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.08159825205802917\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0933971181511879\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.030521132051944733\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.048923417925834656\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0838635042309761\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.041004762053489685\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.1572464108467102\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.08184591680765152\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.002722872653976083\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.009850533679127693\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.029202358797192574\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.00780200120061636\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.009479617699980736\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.05395224317908287\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.014729413203895092\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.8138583302497864\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.05299196392297745\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.0689762532711029\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.025109659880399704\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.012727532535791397\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.010008295066654682\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.006305092945694923\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.048491593450307846\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.003831289242953062\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0627344623208046\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.08236481994390488\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.008358148857951164\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.06798117607831955\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.00981768500059843\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.0024250417482107878\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.004699383396655321\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.004423127975314856\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.02028486505150795\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.006353839300572872\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.03532951697707176\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.001512832473963499\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.016958240419626236\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1110652685165405\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.07815738767385483\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.04070711135864258\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.0371154323220253\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.05111829563975334\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05839770287275314\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.06026560440659523\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.005284671206027269\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0698813796043396\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0037951970007270575\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.02601213939487934\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.18689605593681335\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.015566998161375523\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.005965245421975851\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.06492183357477188\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.011280622333288193\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.008719068951904774\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.004666823893785477\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.021647177636623383\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.039051301777362823\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.004527676850557327\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.02092866599559784\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3853416442871094\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.16049128770828247\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.023323675617575645\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.0634162649512291\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.011503858491778374\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.013540796935558319\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03672970086336136\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.04061881825327873\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.009481683373451233\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.005162335932254791\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.003497711382806301\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0038230984937399626\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.012016549706459045\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.0074562751688063145\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.056170616298913956\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.026076024398207664\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.005272296257317066\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.010507775470614433\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.009646774269640446\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.2322162240743637\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.02495851181447506\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.00505228852853179\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.8955656886100769\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.03581200912594795\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.05626819282770157\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.0060285842046141624\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03463614359498024\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03286345303058624\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03475131839513779\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.07875503599643707\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.008432709611952305\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.007362011820077896\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.10008552670478821\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.033156655728816986\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.0035433731973171234\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.15932630002498627\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.0023664082400500774\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.0034547047689557076\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.02124738320708275\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.002355153439566493\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.02008354663848877\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.043142128735780716\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.001174840610474348\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.013395581394433975\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.276344895362854\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.08613162487745285\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.05550975725054741\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.03815334290266037\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.09199821203947067\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.04521124064922333\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.008714469149708748\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.039697933942079544\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.03444354236125946\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.05715188384056091\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.01729598268866539\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.006201033014804125\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.10147898644208908\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.09814850986003876\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.06187257543206215\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.006090576760470867\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.008813504129648209\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.006231712177395821\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0593423992395401\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0037753162905573845\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.00434253690764308\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.03206256031990051\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992788</td>\n",
       "      <td>0.992788</td>\n",
       "      <td>0.970552</td>\n",
       "      <td>0.992788</td>\n",
       "      <td>0.965060</td>\n",
       "      <td>0.992788</td>\n",
       "      <td>0.967790</td>\n",
       "      <td>96.355031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992224</td>\n",
       "      <td>0.992224</td>\n",
       "      <td>0.950183</td>\n",
       "      <td>0.992224</td>\n",
       "      <td>0.985151</td>\n",
       "      <td>0.992224</td>\n",
       "      <td>0.967111</td>\n",
       "      <td>93.304001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.994041</td>\n",
       "      <td>0.994041</td>\n",
       "      <td>0.970904</td>\n",
       "      <td>0.994041</td>\n",
       "      <td>0.975170</td>\n",
       "      <td>0.994041</td>\n",
       "      <td>0.973020</td>\n",
       "      <td>94.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.991765</td>\n",
       "      <td>0.991765</td>\n",
       "      <td>0.966662</td>\n",
       "      <td>0.991765</td>\n",
       "      <td>0.953821</td>\n",
       "      <td>0.991765</td>\n",
       "      <td>0.960126</td>\n",
       "      <td>93.671970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.988254</td>\n",
       "      <td>0.988254</td>\n",
       "      <td>0.935816</td>\n",
       "      <td>0.988254</td>\n",
       "      <td>0.965274</td>\n",
       "      <td>0.988254</td>\n",
       "      <td>0.950032</td>\n",
       "      <td>93.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.990992</td>\n",
       "      <td>0.990992</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.990992</td>\n",
       "      <td>0.961429</td>\n",
       "      <td>0.990992</td>\n",
       "      <td>0.956573</td>\n",
       "      <td>95.040032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.991417</td>\n",
       "      <td>0.991417</td>\n",
       "      <td>0.957754</td>\n",
       "      <td>0.991417</td>\n",
       "      <td>0.972668</td>\n",
       "      <td>0.991417</td>\n",
       "      <td>0.964983</td>\n",
       "      <td>93.354970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.991897</td>\n",
       "      <td>0.991897</td>\n",
       "      <td>0.972993</td>\n",
       "      <td>0.991897</td>\n",
       "      <td>0.947653</td>\n",
       "      <td>0.991897</td>\n",
       "      <td>0.959672</td>\n",
       "      <td>93.714030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.990165</td>\n",
       "      <td>0.990165</td>\n",
       "      <td>0.931812</td>\n",
       "      <td>0.990165</td>\n",
       "      <td>0.976710</td>\n",
       "      <td>0.990165</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>94.031001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.991163</td>\n",
       "      <td>0.991163</td>\n",
       "      <td>0.955625</td>\n",
       "      <td>0.991163</td>\n",
       "      <td>0.969765</td>\n",
       "      <td>0.991163</td>\n",
       "      <td>0.962280</td>\n",
       "      <td>94.227999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.992788               0.992788               0.970552            0.992788   \n",
       "1  0.992224               0.992224               0.950183            0.992224   \n",
       "2  0.994041               0.994041               0.970904            0.994041   \n",
       "3  0.991765               0.991765               0.966662            0.991765   \n",
       "4  0.988254               0.988254               0.935816            0.988254   \n",
       "5  0.990992               0.990992               0.951807            0.990992   \n",
       "6  0.991417               0.991417               0.957754            0.991417   \n",
       "7  0.991897               0.991897               0.972993            0.991897   \n",
       "8  0.990165               0.990165               0.931812            0.990165   \n",
       "9  0.991163               0.991163               0.955625            0.991163   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.965060        0.992788        0.967790       96.355031  \n",
       "1            0.985151        0.992224        0.967111       93.304001  \n",
       "2            0.975170        0.994041        0.973020       94.293000  \n",
       "3            0.953821        0.991765        0.960126       93.671970  \n",
       "4            0.965274        0.988254        0.950032       93.385000  \n",
       "5            0.961429        0.990992        0.956573       95.040032  \n",
       "6            0.972668        0.991417        0.964983       93.354970  \n",
       "7            0.947653        0.991897        0.959672       93.714030  \n",
       "8            0.976710        0.990165        0.953333       94.031001  \n",
       "9            0.969765        0.991163        0.962280       94.227999  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
