{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_CNN_BiLSTM_Linear import ATE_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_pre_trained_dropout_cnn_bilstm_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_pre_trained_dropout_cnn_bilstm_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1140022277832031\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5221658945083618\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.15322467684745789\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.08344922214746475\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.051482122391462326\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.030551783740520477\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03515926003456116\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.030131492763757706\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.044823456555604935\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.02224038727581501\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03260359168052673\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.03461591899394989\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.05116621032357216\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.061228424310684204\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.014008823782205582\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.03966030105948448\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.03220851719379425\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.030350349843502045\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.014072654768824577\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.015951847657561302\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.010524754412472248\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.02103482186794281\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1035497188568115\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.37245213985443115\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1180925965309143\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.05462363362312317\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.039833273738622665\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03668103367090225\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.029225245118141174\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0528959296643734\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.06303057074546814\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.03181071951985359\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.016007764264941216\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.032158318907022476\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.02484755590558052\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.022654982283711433\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.020304925739765167\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.024076350033283234\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.01492229662835598\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.010821830481290817\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.013985303230583668\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.014892248436808586\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.030284730717539787\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.019249480217695236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.10434889793396\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.3326457738876343\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10724114626646042\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.05139080807566643\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.0411040224134922\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.04565957188606262\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.0314297191798687\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.023328833281993866\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.012339103035628796\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.030100708827376366\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.02282460778951645\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0181474220007658\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.012101374566555023\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.018924085423350334\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.025568518787622452\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.015688953921198845\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.013581948354840279\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.012071270495653152\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.01409031543880701\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01067903358489275\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.03898009657859802\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.0135886799544096\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.135961890220642\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5848504304885864\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.16357415914535522\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.07665006071329117\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.056243278086185455\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05621536448597908\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03695843741297722\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.027975743636488914\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.027645748108625412\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.02693677321076393\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.019505860283970833\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.029139444231987\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.028672033920884132\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.026418747380375862\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.013403886929154396\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.010203002020716667\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.018846986815333366\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.022441014647483826\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.01303604431450367\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01362951286137104\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.021741794422268867\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.018888482823967934\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.115967035293579\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.4505239725112915\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.14112971723079681\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.06706078350543976\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.055567264556884766\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.04295870661735535\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.023411918431520462\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.026023203507065773\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.04532782360911369\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.026581192389130592\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03232169151306152\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.027134234085679054\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.012797772884368896\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.011920517310500145\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.019983423873782158\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.052250221371650696\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.025148620828986168\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.01605362258851528\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.021854819729924202\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.011686594225466251\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.018738174811005592\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.010538754053413868\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1343328952789307\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.507053792476654\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.11588049679994583\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.053770098835229874\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.04670039936900139\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.02591012604534626\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.027759574353694916\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.03451690450310707\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.02871510572731495\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.034847624599933624\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03149498254060745\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.012964719906449318\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.025053616613149643\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.02293449081480503\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.025553612038493156\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.017819056287407875\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.01952490210533142\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.018267357721924782\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0239777322858572\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.040394995361566544\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.01651298813521862\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.010052370838820934\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1052144765853882\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.55247962474823\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.115333192050457\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.0888730064034462\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.038772955536842346\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.026799628511071205\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03200896829366684\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.025918837636709213\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.013892719522118568\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.029247086495161057\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.01818987913429737\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.029208723455667496\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.014432577416300774\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.021453551948070526\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.012173835188150406\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.022503377869725227\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.014146825298666954\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.020527174696326256\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.01904415898025036\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.027487508952617645\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.016618501394987106\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.012265625409781933\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1292791366577148\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.7220713496208191\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.13025330007076263\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.059628624469041824\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.044561389833688736\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.028412800282239914\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.07250618189573288\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.017387330532073975\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0201625507324934\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.028826549649238586\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.02113889344036579\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.03377760201692581\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.04755828529596329\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.02130776084959507\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.015804408118128777\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.02409396506845951\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.018956972286105156\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.009328330866992474\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0225027184933424\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01637657918035984\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.020153818652033806\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.01326144952327013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.108407974243164\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5189666152000427\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.14142046868801117\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.08267635852098465\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.040450308471918106\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03132370859384537\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.022990979254245758\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.031113281846046448\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.020561091601848602\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.013209493830800056\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.02954140119254589\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.029649389907717705\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.02405465580523014\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.023307012394070625\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.01490751001983881\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.023895015940070152\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.027554700151085854\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.015118242241442204\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.024900151416659355\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.025519782677292824\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.014800851233303547\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.014170315116643906\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0885101556777954\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5672980546951294\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1492146998643875\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.08197136968374252\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.05338316038250923\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.039321575313806534\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03144116327166557\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.038055092096328735\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.018846195191144943\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0174875371158123\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03217441961169243\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.018428301438689232\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.02290821447968483\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.019087890163064003\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.024047469720244408\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.014837986789643764\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.021571675315499306\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.03686092421412468\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.02752585895359516\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01046804990619421\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.017269395291805267\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.010435088537633419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_CNN_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "\n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.savefig(f'../../../results/ATE/SemEval16 - Task 5 - Restaurants/plots/bert_pt_do_cnn_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995078</td>\n",
       "      <td>0.995078</td>\n",
       "      <td>0.526929</td>\n",
       "      <td>0.995078</td>\n",
       "      <td>0.363465</td>\n",
       "      <td>0.995078</td>\n",
       "      <td>0.383117</td>\n",
       "      <td>415.784072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995219</td>\n",
       "      <td>0.995219</td>\n",
       "      <td>0.454790</td>\n",
       "      <td>0.995219</td>\n",
       "      <td>0.370154</td>\n",
       "      <td>0.995219</td>\n",
       "      <td>0.389278</td>\n",
       "      <td>383.933037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.994910</td>\n",
       "      <td>0.994910</td>\n",
       "      <td>0.549152</td>\n",
       "      <td>0.994910</td>\n",
       "      <td>0.352124</td>\n",
       "      <td>0.994910</td>\n",
       "      <td>0.365240</td>\n",
       "      <td>384.568043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994887</td>\n",
       "      <td>0.994887</td>\n",
       "      <td>0.496924</td>\n",
       "      <td>0.994887</td>\n",
       "      <td>0.350833</td>\n",
       "      <td>0.994887</td>\n",
       "      <td>0.364275</td>\n",
       "      <td>385.595846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.995324</td>\n",
       "      <td>0.995324</td>\n",
       "      <td>0.700308</td>\n",
       "      <td>0.995324</td>\n",
       "      <td>0.356688</td>\n",
       "      <td>0.995324</td>\n",
       "      <td>0.376474</td>\n",
       "      <td>381.474509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.994906</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>0.446301</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>0.358998</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>0.374490</td>\n",
       "      <td>373.427891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.994996</td>\n",
       "      <td>0.994996</td>\n",
       "      <td>0.582448</td>\n",
       "      <td>0.994996</td>\n",
       "      <td>0.362355</td>\n",
       "      <td>0.994996</td>\n",
       "      <td>0.382391</td>\n",
       "      <td>349.605191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.994906</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>0.434815</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>0.357065</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>0.371125</td>\n",
       "      <td>345.543205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.994934</td>\n",
       "      <td>0.994934</td>\n",
       "      <td>0.610850</td>\n",
       "      <td>0.994934</td>\n",
       "      <td>0.351595</td>\n",
       "      <td>0.994934</td>\n",
       "      <td>0.365276</td>\n",
       "      <td>348.022727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.995004</td>\n",
       "      <td>0.995004</td>\n",
       "      <td>0.423372</td>\n",
       "      <td>0.995004</td>\n",
       "      <td>0.342627</td>\n",
       "      <td>0.995004</td>\n",
       "      <td>0.349408</td>\n",
       "      <td>351.684240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.995078               0.995078               0.526929            0.995078   \n",
       "1  0.995219               0.995219               0.454790            0.995219   \n",
       "2  0.994910               0.994910               0.549152            0.994910   \n",
       "3  0.994887               0.994887               0.496924            0.994887   \n",
       "4  0.995324               0.995324               0.700308            0.995324   \n",
       "5  0.994906               0.994906               0.446301            0.994906   \n",
       "6  0.994996               0.994996               0.582448            0.994996   \n",
       "7  0.994906               0.994906               0.434815            0.994906   \n",
       "8  0.994934               0.994934               0.610850            0.994934   \n",
       "9  0.995004               0.995004               0.423372            0.995004   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.363465        0.995078        0.383117      415.784072  \n",
       "1            0.370154        0.995219        0.389278      383.933037  \n",
       "2            0.352124        0.994910        0.365240      384.568043  \n",
       "3            0.350833        0.994887        0.364275      385.595846  \n",
       "4            0.356688        0.995324        0.376474      381.474509  \n",
       "5            0.358998        0.994906        0.374490      373.427891  \n",
       "6            0.362355        0.994996        0.382391      349.605191  \n",
       "7            0.357065        0.994906        0.371125      345.543205  \n",
       "8            0.351595        0.994934        0.365276      348.022727  \n",
       "9            0.342627        0.995004        0.349408      351.684240  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
