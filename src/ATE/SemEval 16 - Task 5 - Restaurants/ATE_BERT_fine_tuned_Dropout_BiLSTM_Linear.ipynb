{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from models.BERT_Dropout_BiLSTM_Linear import BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned_dropout_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_fine_tuned_dropout_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.2310829162597656\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.29282650351524353\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.13377776741981506\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.2031979262828827\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.09002398699522018\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.09110508114099503\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.12059461325407028\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0393509604036808\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.027842549607157707\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.09087565541267395\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.06600473821163177\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.11092810332775116\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.025190506130456924\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.06172860786318779\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.05311788618564606\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.03558126837015152\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.06435580551624298\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.01338998693972826\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.026889953762292862\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.027289574965834618\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.05708164721727371\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.010695507749915123\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0707676410675049\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.15610404312610626\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10001830756664276\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.19656555354595184\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.08787349611520767\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03480858728289604\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.11116582900285721\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.17762558162212372\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.06494798511266708\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.11564456671476364\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.04156969487667084\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.02918326109647751\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.08598602563142776\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.02345171570777893\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.0894574373960495\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.022395942360162735\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.024584606289863586\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.05423736944794655\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.059829797595739365\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.031162992119789124\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.045389190316200256\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.022605033591389656\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.9536964297294617\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.28998151421546936\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.17464829981327057\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.14573946595191956\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.1014895811676979\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.036029163748025894\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.09015101939439774\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.16024604439735413\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.12181100249290466\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.07863111793994904\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.04149175435304642\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.06984452158212662\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.1536680907011032\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.05187692493200302\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.03113860823214054\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.05182952061295509\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.019546134397387505\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.026933761313557625\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.03275390341877937\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.05782114341855049\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.09728366881608963\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.05257300287485123\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0203171968460083\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.1688448190689087\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.07899174839258194\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.2277955412864685\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.09342633187770844\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.1027551144361496\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.04321259260177612\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.13374820351600647\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.10840243101119995\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0509103387594223\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.20278380811214447\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.05991670489311218\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.08082091808319092\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.06655731052160263\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.03778691589832306\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.039940204471349716\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.030774855986237526\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.06895411759614944\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.00908008124679327\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.037552349269390106\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.034615520387887955\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.027214404195547104\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0749269723892212\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.24847464263439178\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.06062573939561844\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.15414859354496002\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.11565813422203064\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.06252210587263107\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03250613436102867\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.044183399528265\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0549936518073082\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.017626138404011726\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.12208618223667145\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.06158614158630371\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.06526163965463638\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.01792193576693535\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.028562771156430244\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.05777554214000702\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.017491113394498825\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.050762493163347244\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.025213317945599556\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.0260817501693964\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.05461004376411438\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.026530850678682327\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0556961297988892\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.1872086375951767\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.35293158888816833\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.18121466040611267\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.11914990842342377\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05839258059859276\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.06242305785417557\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0787137821316719\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.07853123545646667\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.09800048917531967\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.08391211926937103\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.07380247861146927\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.017218079417943954\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03254827857017517\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.07547545433044434\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.03143392875790596\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.017438914626836777\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.055758196860551834\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.013219169341027737\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.10044147819280624\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.05999341979622841\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.03417152911424637\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1118087768554688\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.12489951401948929\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10775493830442429\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.11889655143022537\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.18165431916713715\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05710852891206741\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.042856987565755844\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.036125198006629944\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.04755093902349472\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.023430589586496353\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.09372644871473312\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.026218371465802193\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.07471590489149094\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.0690760612487793\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.018465105444192886\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.09318044781684875\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.013981536962091923\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.02593592368066311\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.07319372892379761\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.07005498558282852\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.015008218586444855\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.04129480570554733\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0222649574279785\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.2558659613132477\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.16576382517814636\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.08855093270540237\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.1047714576125145\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.17472799122333527\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.054091278463602066\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.04656122997403145\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.06799590587615967\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06449192017316818\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03643161803483963\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.08106684684753418\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.04610268399119377\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.02052846923470497\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.023846104741096497\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.06470344215631485\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.07702077180147171\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.027957536280155182\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.027346491813659668\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.026571504771709442\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.010868550278246403\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.02161380462348461\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1026841402053833\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.23578180372714996\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.11789197474718094\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.1755128800868988\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.09249390661716461\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.07728676497936249\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.08277805894613266\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.05515790358185768\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.07044615596532822\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.05088772624731064\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.019587045535445213\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.053623635321855545\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.0543142594397068\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.022605296224355698\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.017505250871181488\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.06437159329652786\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.09014272689819336\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.07572313398122787\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.09831516444683075\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.08604330569505692\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.05834788829088211\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.030097031965851784\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.055314302444458\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.21459852159023285\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.20464229583740234\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.13801035284996033\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.10929637402296066\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.025343168526887894\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.0804818794131279\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.05249353125691414\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.1775679588317871\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.03214483708143234\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.165388286113739\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.10669264197349548\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.069982148706913\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.049209728837013245\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.03736557066440582\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.01662035658955574\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.05573495477437973\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.025899387896060944\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.05471932515501976\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.02176184393465519\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.033976223319768906\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.019034110009670258\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = BERT_Dropout_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, bilstm_in_features=256, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.991843</td>\n",
       "      <td>0.991843</td>\n",
       "      <td>0.964069</td>\n",
       "      <td>0.991843</td>\n",
       "      <td>0.971585</td>\n",
       "      <td>0.991843</td>\n",
       "      <td>0.967604</td>\n",
       "      <td>113.780996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992267</td>\n",
       "      <td>0.992267</td>\n",
       "      <td>0.956891</td>\n",
       "      <td>0.992267</td>\n",
       "      <td>0.977282</td>\n",
       "      <td>0.992267</td>\n",
       "      <td>0.966774</td>\n",
       "      <td>100.590002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.988162</td>\n",
       "      <td>0.988162</td>\n",
       "      <td>0.958969</td>\n",
       "      <td>0.988162</td>\n",
       "      <td>0.913056</td>\n",
       "      <td>0.988162</td>\n",
       "      <td>0.934476</td>\n",
       "      <td>100.867003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.983683</td>\n",
       "      <td>0.983683</td>\n",
       "      <td>0.895807</td>\n",
       "      <td>0.983683</td>\n",
       "      <td>0.963374</td>\n",
       "      <td>0.983683</td>\n",
       "      <td>0.927423</td>\n",
       "      <td>100.978001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990569</td>\n",
       "      <td>0.990569</td>\n",
       "      <td>0.943243</td>\n",
       "      <td>0.990569</td>\n",
       "      <td>0.966362</td>\n",
       "      <td>0.990569</td>\n",
       "      <td>0.954562</td>\n",
       "      <td>100.475029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.987472</td>\n",
       "      <td>0.987472</td>\n",
       "      <td>0.930815</td>\n",
       "      <td>0.987472</td>\n",
       "      <td>0.953164</td>\n",
       "      <td>0.987472</td>\n",
       "      <td>0.940467</td>\n",
       "      <td>101.199029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.990097</td>\n",
       "      <td>0.990097</td>\n",
       "      <td>0.939725</td>\n",
       "      <td>0.990097</td>\n",
       "      <td>0.968227</td>\n",
       "      <td>0.990097</td>\n",
       "      <td>0.953537</td>\n",
       "      <td>100.505996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.986130</td>\n",
       "      <td>0.986130</td>\n",
       "      <td>0.919279</td>\n",
       "      <td>0.986130</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.986130</td>\n",
       "      <td>0.943294</td>\n",
       "      <td>100.929030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.990088</td>\n",
       "      <td>0.990088</td>\n",
       "      <td>0.944022</td>\n",
       "      <td>0.990088</td>\n",
       "      <td>0.962344</td>\n",
       "      <td>0.990088</td>\n",
       "      <td>0.953021</td>\n",
       "      <td>100.482998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.988246</td>\n",
       "      <td>0.988246</td>\n",
       "      <td>0.920482</td>\n",
       "      <td>0.988246</td>\n",
       "      <td>0.964751</td>\n",
       "      <td>0.988246</td>\n",
       "      <td>0.941520</td>\n",
       "      <td>100.830030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.991843               0.991843               0.964069            0.991843   \n",
       "1  0.992267               0.992267               0.956891            0.992267   \n",
       "2  0.988162               0.988162               0.958969            0.988162   \n",
       "3  0.983683               0.983683               0.895807            0.983683   \n",
       "4  0.990569               0.990569               0.943243            0.990569   \n",
       "5  0.987472               0.987472               0.930815            0.987472   \n",
       "6  0.990097               0.990097               0.939725            0.990097   \n",
       "7  0.986130               0.986130               0.919279            0.986130   \n",
       "8  0.990088               0.990088               0.944022            0.990088   \n",
       "9  0.988246               0.988246               0.920482            0.988246   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.971585        0.991843        0.967604      113.780996  \n",
       "1            0.977282        0.992267        0.966774      100.590002  \n",
       "2            0.913056        0.988162        0.934476      100.867003  \n",
       "3            0.963374        0.983683        0.927423      100.978001  \n",
       "4            0.966362        0.990569        0.954562      100.475029  \n",
       "5            0.953164        0.987472        0.940467      101.199029  \n",
       "6            0.968227        0.990097        0.953537      100.505996  \n",
       "7            0.971171        0.986130        0.943294      100.929030  \n",
       "8            0.962344        0.990088        0.953021      100.482998  \n",
       "9            0.964751        0.988246        0.941520      100.830030  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
