{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_BiLSTM_Linear import ATE_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned_dropout_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_fine_tuned_dropout_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.142454981803894\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.19809329509735107\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.09953846037387848\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.041870955377817154\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.1527135670185089\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.08697402477264404\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.07678920775651932\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.06680800765752792\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.08175411075353622\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0808749869465828\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.043375544250011444\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.04537026956677437\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.022255761548876762\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.07243037968873978\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.0755750983953476\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.0420139916241169\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.02586478926241398\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.016366608440876007\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.05257890745997429\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.015172445215284824\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.010948657058179379\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.023968545719981194\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1817644834518433\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.14966465532779694\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.12644003331661224\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.07673560827970505\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.17741335928440094\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.06559519469738007\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.09403042495250702\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.10531902313232422\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.08536908775568008\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.14310306310653687\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.054546985775232315\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.03198302909731865\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.021250346675515175\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.017747176811099052\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.04391269013285637\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.045068301260471344\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.055661626160144806\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.021658826619386673\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0243374090641737\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01640021800994873\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.013188876211643219\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.03540388494729996\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.111304759979248\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.19193005561828613\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.20935125648975372\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.12931029498577118\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.12042543292045593\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.10471725463867188\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.09966270625591278\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.043603867292404175\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.04131850227713585\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06548959761857986\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.06163403019309044\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.08145863562822342\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.027273446321487427\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03882681578397751\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.025270843878388405\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.027097806334495544\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.02631225995719433\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.14503121376037598\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.06110093742609024\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.016773663461208344\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.06785843521356583\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.06811350584030151\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0107237100601196\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.16561920940876007\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.08818311989307404\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.12738756835460663\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.04920157045125961\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.032586343586444855\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.04755554348230362\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.06609310954809189\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0920458510518074\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.04222426563501358\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.038390904664993286\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.04199852794408798\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.03433248773217201\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.023385142907500267\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.044475581496953964\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.06979268044233322\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.070372574031353\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.07357204705476761\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.060605522245168686\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.027832278981804848\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.02725977636873722\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.039542779326438904\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0439430475234985\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.3379031717777252\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.09618902951478958\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.09934648871421814\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.05781467631459236\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.04437922313809395\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.05550588294863701\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.044355329126119614\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0430833138525486\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.10490690916776657\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.0644959956407547\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.03781275451183319\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.06269016861915588\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.052646100521087646\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.02114139497280121\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.027960119768977165\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.04449809715151787\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.042949385941028595\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.015156173147261143\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.024954475462436676\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.01375890988856554\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.03326938673853874\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1622047424316406\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.2943360507488251\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.18790870904922485\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.14468169212341309\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.09549500793218613\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.09690593183040619\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.07806238532066345\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0984654501080513\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.23991799354553223\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.03683585673570633\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.04202275723218918\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.042661506682634354\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.03807007893919945\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.11134551465511322\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.06394238770008087\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.04609968513250351\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.01430058665573597\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.04276463761925697\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0628134161233902\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.05406226962804794\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.018000248819589615\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.005651609972119331\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.2179386615753174\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.23764248192310333\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.24338406324386597\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.11973892152309418\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.06901217997074127\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.02578384056687355\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.024081697687506676\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.060656994581222534\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.1413162499666214\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.07015823572874069\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.02617993764579296\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.08758433163166046\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.06973036378622055\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.045739706605672836\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.02832084707915783\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.14465859532356262\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.024570375680923462\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.10144662111997604\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.008335774764418602\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.03479047119617462\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.0255400612950325\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.015681255608797073\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1126177310943604\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.17019347846508026\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.18127262592315674\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.06684225052595139\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.1034025251865387\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.1055939644575119\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.07136140763759613\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.08794888108968735\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0966482162475586\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.024356704205274582\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.018796134740114212\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.03449363261461258\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.019556773826479912\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.06933438032865524\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.034953560680150986\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.04491975158452988\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.10326860845088959\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.013803885318338871\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.05540432035923004\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.07515799254179001\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.022052370011806488\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.10731828957796097\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 0.9761792421340942\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.2974989414215088\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.15063278377056122\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.10987795144319534\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.07835207134485245\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.0759015679359436\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.10590320825576782\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.06753366440534592\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.08457650244235992\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.052803102880716324\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.04894402623176575\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.04977085441350937\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.031425319612026215\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.029286418110132217\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.016152117401361465\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.038754016160964966\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.021380051970481873\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.10815179347991943\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.11383434385061264\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.04572061449289322\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.08694019168615341\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.02753165364265442\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.004201054573059\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.16958370804786682\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1073206290602684\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.2657417356967926\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.16629579663276672\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.06897233426570892\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.06234896183013916\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.06227339804172516\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.07596554607152939\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.05287456512451172\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.027500901371240616\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.04650748521089554\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.028948340564966202\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03824661672115326\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.0449577160179615\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.04542626067996025\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.011270455084741116\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.03721419721841812\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.020268810912966728\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.016602227464318275\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.021404387429356575\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.014640145935118198\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, bilstm_in_features=256, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.991340</td>\n",
       "      <td>0.991340</td>\n",
       "      <td>0.961497</td>\n",
       "      <td>0.991340</td>\n",
       "      <td>0.956842</td>\n",
       "      <td>0.991340</td>\n",
       "      <td>0.958573</td>\n",
       "      <td>103.776998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.989473</td>\n",
       "      <td>0.989473</td>\n",
       "      <td>0.950023</td>\n",
       "      <td>0.989473</td>\n",
       "      <td>0.955502</td>\n",
       "      <td>0.989473</td>\n",
       "      <td>0.952669</td>\n",
       "      <td>100.019030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.962204</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.948792</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.954991</td>\n",
       "      <td>100.245970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.991224</td>\n",
       "      <td>0.991224</td>\n",
       "      <td>0.948168</td>\n",
       "      <td>0.991224</td>\n",
       "      <td>0.972086</td>\n",
       "      <td>0.991224</td>\n",
       "      <td>0.959813</td>\n",
       "      <td>100.064030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.988355</td>\n",
       "      <td>0.988355</td>\n",
       "      <td>0.926044</td>\n",
       "      <td>0.988355</td>\n",
       "      <td>0.966472</td>\n",
       "      <td>0.988355</td>\n",
       "      <td>0.945459</td>\n",
       "      <td>100.479028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.992385</td>\n",
       "      <td>0.992385</td>\n",
       "      <td>0.953423</td>\n",
       "      <td>0.992385</td>\n",
       "      <td>0.974566</td>\n",
       "      <td>0.992385</td>\n",
       "      <td>0.963783</td>\n",
       "      <td>100.418999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.988353</td>\n",
       "      <td>0.988353</td>\n",
       "      <td>0.934524</td>\n",
       "      <td>0.988353</td>\n",
       "      <td>0.954053</td>\n",
       "      <td>0.988353</td>\n",
       "      <td>0.942885</td>\n",
       "      <td>100.404004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.989161</td>\n",
       "      <td>0.989161</td>\n",
       "      <td>0.929225</td>\n",
       "      <td>0.989161</td>\n",
       "      <td>0.968667</td>\n",
       "      <td>0.989161</td>\n",
       "      <td>0.948190</td>\n",
       "      <td>101.904998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.990296</td>\n",
       "      <td>0.990296</td>\n",
       "      <td>0.938190</td>\n",
       "      <td>0.990296</td>\n",
       "      <td>0.970718</td>\n",
       "      <td>0.990296</td>\n",
       "      <td>0.953925</td>\n",
       "      <td>101.086999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.989329</td>\n",
       "      <td>0.989329</td>\n",
       "      <td>0.943298</td>\n",
       "      <td>0.989329</td>\n",
       "      <td>0.959153</td>\n",
       "      <td>0.989329</td>\n",
       "      <td>0.950843</td>\n",
       "      <td>100.458001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.991340               0.991340               0.961497            0.991340   \n",
       "1  0.989473               0.989473               0.950023            0.989473   \n",
       "2  0.991228               0.991228               0.962204            0.991228   \n",
       "3  0.991224               0.991224               0.948168            0.991224   \n",
       "4  0.988355               0.988355               0.926044            0.988355   \n",
       "5  0.992385               0.992385               0.953423            0.992385   \n",
       "6  0.988353               0.988353               0.934524            0.988353   \n",
       "7  0.989161               0.989161               0.929225            0.989161   \n",
       "8  0.990296               0.990296               0.938190            0.990296   \n",
       "9  0.989329               0.989329               0.943298            0.989329   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.956842        0.991340        0.958573      103.776998  \n",
       "1            0.955502        0.989473        0.952669      100.019030  \n",
       "2            0.948792        0.991228        0.954991      100.245970  \n",
       "3            0.972086        0.991224        0.959813      100.064030  \n",
       "4            0.966472        0.988355        0.945459      100.479028  \n",
       "5            0.974566        0.992385        0.963783      100.418999  \n",
       "6            0.954053        0.988353        0.942885      100.404004  \n",
       "7            0.968667        0.989161        0.948190      101.904998  \n",
       "8            0.970718        0.990296        0.953925      101.086999  \n",
       "9            0.959153        0.989329        0.950843      100.458001  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
