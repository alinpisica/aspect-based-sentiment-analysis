{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from models.BERT_Dropout_CNN_BiLSTM_Linear import BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_pre_trained_dropout_cnn_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_pre_trained_dropout_cnn_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.066458821296692\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5157679319381714\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.12106795608997345\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.057508982717990875\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.05051800608634949\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.040825698524713516\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.05167870223522186\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.03849620372056961\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.026828965172171593\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.021661341190338135\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.017453383654356003\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.018749820068478584\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.02531721442937851\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.021940916776657104\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.021481355652213097\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.013598578982055187\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.0321226641535759\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.03448912873864174\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.014376621693372726\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.027991296723484993\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.016503943130373955\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.04114541411399841\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1475491523742676\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.3874679207801819\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10278134047985077\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.04531971365213394\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.04025539383292198\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05400661751627922\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.026382584124803543\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.02678036317229271\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.017983833327889442\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.02833191677927971\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.053659193217754364\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.07253346592187881\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.017465483397245407\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.011876665987074375\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.037440910935401917\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.016460593789815903\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.013173142448067665\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.010857736691832542\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.017544535920023918\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.021325474604964256\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.03831600025296211\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.008066950365900993\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1235995292663574\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.6310632228851318\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1582549810409546\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.07273559272289276\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.049502015113830566\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.06983830779790878\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03896072506904602\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.030109629034996033\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.030515141785144806\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.03717712312936783\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.02436777763068676\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.0320274643599987\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.018996533006429672\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.023386234417557716\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.01848835125565529\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.022607624530792236\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.03231409564614296\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.028244495391845703\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.02596442401409149\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.02920803427696228\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.01810276508331299\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.012272045016288757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1185169219970703\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.45900189876556396\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.13250985741615295\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.07833289355039597\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.04811972752213478\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.042225755751132965\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.02841750718653202\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.03172757104039192\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.02371467649936676\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.02392943762242794\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.022861652076244354\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.03690163791179657\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.01071824599057436\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.021401330828666687\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.012666112743318081\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.02982257306575775\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.033920422196388245\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.03127976506948471\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.03885691985487938\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01975964568555355\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.00902322307229042\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.017628349363803864\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1182199716567993\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5885642766952515\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.15928061306476593\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.10501328855752945\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.07047528773546219\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03172282874584198\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.04882686212658882\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0214224960654974\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.028598427772521973\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.02503509260714054\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.027124855667352676\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01678626425564289\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.027322877198457718\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03340255096554756\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.036512341350317\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.023861030116677284\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.020080367103219032\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.03132212907075882\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.04165254160761833\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.007841159589588642\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.03094499185681343\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.01574256457388401\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0803357362747192\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.46024027466773987\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.09791560471057892\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.054378971457481384\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.05387434735894203\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.024151267483830452\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.017651470378041267\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.013653794303536415\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.023768479004502296\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.014762439765036106\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.02780715562403202\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.012194850482046604\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.020859766751527786\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.013832860626280308\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.015857912600040436\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.033690862357616425\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.01245390996336937\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.009420705027878284\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.027865754440426826\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.034921951591968536\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.005658249370753765\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.005032956600189209\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1382884979248047\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.4436281621456146\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1216452419757843\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.06096535176038742\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03758900985121727\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03528113663196564\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03214786946773529\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.035470038652420044\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.03547472506761551\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.018043382093310356\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.016810793429613113\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01931271329522133\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.02800644002854824\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.027660205960273743\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.021685173735022545\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.02889201231300831\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.029499925673007965\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.016388380900025368\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.015500659123063087\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.02209535427391529\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.01718984916806221\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.021765122190117836\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0733156204223633\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.3204776346683502\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.09001357108354568\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.066684789955616\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.05241181701421738\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03362327441573143\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.0269668847322464\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.039367374032735825\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.015676550567150116\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.022970518097281456\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.020438693463802338\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01992047019302845\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.024616239592432976\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.023879822343587875\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.02069377340376377\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.02131212316453457\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.019015375524759293\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.019514162093400955\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.018192017450928688\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.03195187821984291\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.015261911787092686\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.017273174598813057\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0249965190887451\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.45917460322380066\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.15915976464748383\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.06740161031484604\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.04143524169921875\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.02888825722038746\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03715988248586655\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0451466329395771\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0293298177421093\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.037878140807151794\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.027039818465709686\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01591353863477707\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.031696949154138565\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.015970205888152122\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.02512206695973873\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.022813046351075172\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.009949235245585442\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.027947062626481056\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.013041562400758266\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.029421182349324226\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.028590533882379532\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.017053859308362007\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0351223945617676\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.43276524543762207\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1014581099152565\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.07753448188304901\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03390258923172951\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.026460915803909302\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03419269993901253\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.046073995530605316\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.02800852432847023\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.028979957103729248\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.024378813803195953\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01412383932620287\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.013786833733320236\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.013776671141386032\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.0250416062772274\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.014495297335088253\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.020493919029831886\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.015628134831786156\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.01604359596967697\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.011979755945503712\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.04781300574541092\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.010730993002653122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = BERT_Dropout_CNN_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995398</td>\n",
       "      <td>0.995398</td>\n",
       "      <td>0.558875</td>\n",
       "      <td>0.995398</td>\n",
       "      <td>0.341201</td>\n",
       "      <td>0.995398</td>\n",
       "      <td>0.347412</td>\n",
       "      <td>379.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995062</td>\n",
       "      <td>0.995062</td>\n",
       "      <td>0.626600</td>\n",
       "      <td>0.995062</td>\n",
       "      <td>0.365132</td>\n",
       "      <td>0.995062</td>\n",
       "      <td>0.383893</td>\n",
       "      <td>364.071501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995027</td>\n",
       "      <td>0.995027</td>\n",
       "      <td>0.331676</td>\n",
       "      <td>0.995027</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.995027</td>\n",
       "      <td>0.332502</td>\n",
       "      <td>364.300125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.995109</td>\n",
       "      <td>0.995109</td>\n",
       "      <td>0.683628</td>\n",
       "      <td>0.995109</td>\n",
       "      <td>0.361455</td>\n",
       "      <td>0.995109</td>\n",
       "      <td>0.384275</td>\n",
       "      <td>365.182656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.994637</td>\n",
       "      <td>0.994637</td>\n",
       "      <td>0.577167</td>\n",
       "      <td>0.994637</td>\n",
       "      <td>0.352177</td>\n",
       "      <td>0.994637</td>\n",
       "      <td>0.364741</td>\n",
       "      <td>363.327795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.995027</td>\n",
       "      <td>0.995027</td>\n",
       "      <td>0.598084</td>\n",
       "      <td>0.995027</td>\n",
       "      <td>0.366324</td>\n",
       "      <td>0.995027</td>\n",
       "      <td>0.391179</td>\n",
       "      <td>384.738053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.995043</td>\n",
       "      <td>0.995043</td>\n",
       "      <td>0.565520</td>\n",
       "      <td>0.995043</td>\n",
       "      <td>0.346574</td>\n",
       "      <td>0.995043</td>\n",
       "      <td>0.357144</td>\n",
       "      <td>414.651173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.994988</td>\n",
       "      <td>0.994988</td>\n",
       "      <td>0.551710</td>\n",
       "      <td>0.994988</td>\n",
       "      <td>0.359725</td>\n",
       "      <td>0.994988</td>\n",
       "      <td>0.375842</td>\n",
       "      <td>417.224002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.995230</td>\n",
       "      <td>0.995230</td>\n",
       "      <td>0.588105</td>\n",
       "      <td>0.995230</td>\n",
       "      <td>0.363148</td>\n",
       "      <td>0.995230</td>\n",
       "      <td>0.385116</td>\n",
       "      <td>394.022647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.994738</td>\n",
       "      <td>0.994738</td>\n",
       "      <td>0.443821</td>\n",
       "      <td>0.994738</td>\n",
       "      <td>0.346366</td>\n",
       "      <td>0.994738</td>\n",
       "      <td>0.355895</td>\n",
       "      <td>378.653908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.995398               0.995398               0.558875            0.995398   \n",
       "1  0.995062               0.995062               0.626600            0.995062   \n",
       "2  0.995027               0.995027               0.331676            0.995027   \n",
       "3  0.995109               0.995109               0.683628            0.995109   \n",
       "4  0.994637               0.994637               0.577167            0.994637   \n",
       "5  0.995027               0.995027               0.598084            0.995027   \n",
       "6  0.995043               0.995043               0.565520            0.995043   \n",
       "7  0.994988               0.994988               0.551710            0.994988   \n",
       "8  0.995230               0.995230               0.588105            0.995230   \n",
       "9  0.994738               0.994738               0.443821            0.994738   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.341201        0.995398        0.347412      379.121500  \n",
       "1            0.365132        0.995062        0.383893      364.071501  \n",
       "2            0.333333        0.995027        0.332502      364.300125  \n",
       "3            0.361455        0.995109        0.384275      365.182656  \n",
       "4            0.352177        0.994637        0.364741      363.327795  \n",
       "5            0.366324        0.995027        0.391179      384.738053  \n",
       "6            0.346574        0.995043        0.357144      414.651173  \n",
       "7            0.359725        0.994988        0.375842      417.224002  \n",
       "8            0.363148        0.995230        0.385116      394.022647  \n",
       "9            0.346366        0.994738        0.355895      378.653908  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
