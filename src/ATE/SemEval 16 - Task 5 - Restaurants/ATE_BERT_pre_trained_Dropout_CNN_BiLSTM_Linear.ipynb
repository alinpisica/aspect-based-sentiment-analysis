{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_CNN_BiLSTM_Linear import ATE_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/models/bert_pre_trained_dropout_cnn_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/SemEval16 - Task 5 - Restaurants/stats/bert_pre_trained_dropout_cnn_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0883926153182983\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5317003726959229\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10796362161636353\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.05566960573196411\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03925463929772377\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.028470046818256378\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.022127095609903336\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.02559364028275013\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.015582856722176075\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.01946859247982502\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.020484158769249916\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.023793162778019905\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.016448715701699257\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.05165943130850792\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.022481486201286316\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.025866171345114708\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.010106914676725864\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.016842104494571686\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.027324916794896126\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01136272493749857\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.021258650347590446\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.018405582755804062\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1328232288360596\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.40517157316207886\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10945509374141693\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.05769738554954529\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.038755349814891815\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.040604233741760254\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.020122980698943138\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0268927663564682\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.02524571120738983\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.014447370544075966\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.024299880489706993\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01702883280813694\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.023011548444628716\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.01995132304728031\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.02081172540783882\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.010557943023741245\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.018493657931685448\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.015915656462311745\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.014391498640179634\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.04043542966246605\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.024411795660853386\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.020924540236592293\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1247438192367554\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.516514241695404\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1351255476474762\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.057711947709321976\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03380083665251732\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05379876494407654\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.028969187289476395\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.029599538072943687\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.08664677292108536\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.032157160341739655\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.020713789388537407\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.01942821778357029\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.02846248634159565\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.02372518740594387\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.059085648506879807\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.0346263162791729\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.02507941424846649\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.015466319397091866\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.03064100444316864\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.019735323265194893\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.039802733808755875\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.014140651561319828\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0913108587265015\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.4628494381904602\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.11566394567489624\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.05617588758468628\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03460080176591873\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.02657174877822399\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03359585627913475\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.04837673157453537\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.02382766455411911\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.03709016367793083\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.016609778627753258\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.023315412923693657\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.02014881558716297\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.021395953372120857\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.018431467935442924\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.014665232971310616\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.020486198365688324\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.006660184357315302\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.02264304831624031\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01224489975720644\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.026987532153725624\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.014426907524466515\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1227943897247314\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5483309626579285\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.141338050365448\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.06716646999120712\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.05094776675105095\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.042533136904239655\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.036390919238328934\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.02292344905436039\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.04339483007788658\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.019080590456724167\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.015760695561766624\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.022058594971895218\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.01733318530023098\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.04763117805123329\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.0262556504458189\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.013998485170304775\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.014421544969081879\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.012003698386251926\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.020889632403850555\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.013899651356041431\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.014853367581963539\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.009505101479589939\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0900168418884277\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5769866108894348\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.15234559774398804\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.08727613091468811\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03996425122022629\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03246930241584778\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.02685864455997944\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.07679865509271622\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.03445444628596306\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.020444653928279877\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03331427648663521\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.014257805421948433\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.021640723571181297\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.01891378127038479\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.06881831586360931\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.025926927104592323\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.025735026225447655\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.018414262682199478\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.020969033241271973\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.009689523838460445\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.010420133359730244\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.015118628740310669\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.089339017868042\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.5446906089782715\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10970749706029892\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.07591556757688522\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.048353973776102066\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.034753940999507904\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.025707222521305084\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.030072687193751335\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.014338603243231773\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.02528868243098259\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.017932677641510963\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.015672592446208\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.0258649755269289\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.01893521100282669\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.012227354571223259\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.009952288120985031\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.0169488787651062\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.019346565008163452\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.018644195050001144\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.01448693498969078\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.014585742726922035\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.02001301757991314\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.0402615070343018\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.4213261008262634\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.11237084865570068\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.05209006741642952\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.04092619940638542\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03026794083416462\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.020667504519224167\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.03985828533768654\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.03177974373102188\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.0181308314204216\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.026049388572573662\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.019748160615563393\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.03448587283492088\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.020290793851017952\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.020945614203810692\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.018210845068097115\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.011619714088737965\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.013977745547890663\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.021160084754228592\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.02405449189245701\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.029680313542485237\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.021777663379907608\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.095453143119812\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.6246659159660339\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.18939875066280365\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.07703127712011337\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.04891408607363701\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.06832954287528992\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.04075796529650688\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.05475492775440216\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.022136840969324112\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.03080900013446808\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.018762364983558655\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.02331032231450081\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.014806574210524559\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.030102871358394623\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.017440473660826683\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.029324112460017204\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.016071928665041924\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.02548048458993435\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.017629163339734077\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.006160421762615442\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.01960171014070511\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.016161194071173668\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.1023476123809814\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.38322100043296814\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.10255151987075806\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.06028631702065468\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.03248703479766846\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.038088664412498474\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.0636378601193428\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.034099750220775604\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.020521698519587517\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.02372562140226364\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.019855068996548653\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.023672493174672127\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.030824478715658188\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.060400187969207764\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.015014034695923328\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.024731166660785675\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.023766402155160904\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.02586904913187027\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.02030380256474018\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.012657489627599716\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.042660776525735855\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.016603445634245872\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_CNN_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994781</td>\n",
       "      <td>0.994781</td>\n",
       "      <td>0.609435</td>\n",
       "      <td>0.994781</td>\n",
       "      <td>0.351361</td>\n",
       "      <td>0.994781</td>\n",
       "      <td>0.364412</td>\n",
       "      <td>471.878178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.994973</td>\n",
       "      <td>0.994973</td>\n",
       "      <td>0.803490</td>\n",
       "      <td>0.994973</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.994973</td>\n",
       "      <td>0.407451</td>\n",
       "      <td>474.323679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995020</td>\n",
       "      <td>0.995020</td>\n",
       "      <td>0.655878</td>\n",
       "      <td>0.995020</td>\n",
       "      <td>0.346545</td>\n",
       "      <td>0.995020</td>\n",
       "      <td>0.357006</td>\n",
       "      <td>491.119469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994883</td>\n",
       "      <td>0.994883</td>\n",
       "      <td>0.522184</td>\n",
       "      <td>0.994883</td>\n",
       "      <td>0.352377</td>\n",
       "      <td>0.994883</td>\n",
       "      <td>0.366741</td>\n",
       "      <td>488.005002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.995230</td>\n",
       "      <td>0.995230</td>\n",
       "      <td>0.577566</td>\n",
       "      <td>0.995230</td>\n",
       "      <td>0.344999</td>\n",
       "      <td>0.995230</td>\n",
       "      <td>0.353113</td>\n",
       "      <td>489.836057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.995094</td>\n",
       "      <td>0.995094</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.995094</td>\n",
       "      <td>0.336512</td>\n",
       "      <td>0.995094</td>\n",
       "      <td>0.338794</td>\n",
       "      <td>487.363261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.995594</td>\n",
       "      <td>0.995594</td>\n",
       "      <td>0.445251</td>\n",
       "      <td>0.995594</td>\n",
       "      <td>0.352006</td>\n",
       "      <td>0.995594</td>\n",
       "      <td>0.364715</td>\n",
       "      <td>490.212037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.995039</td>\n",
       "      <td>0.995039</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>0.995039</td>\n",
       "      <td>0.346534</td>\n",
       "      <td>0.995039</td>\n",
       "      <td>0.356645</td>\n",
       "      <td>487.644616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.995301</td>\n",
       "      <td>0.995301</td>\n",
       "      <td>0.610311</td>\n",
       "      <td>0.995301</td>\n",
       "      <td>0.348959</td>\n",
       "      <td>0.995301</td>\n",
       "      <td>0.362153</td>\n",
       "      <td>488.875778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.995391</td>\n",
       "      <td>0.995391</td>\n",
       "      <td>0.425159</td>\n",
       "      <td>0.995391</td>\n",
       "      <td>0.336063</td>\n",
       "      <td>0.995391</td>\n",
       "      <td>0.337894</td>\n",
       "      <td>487.350621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.994781               0.994781               0.609435            0.994781   \n",
       "1  0.994973               0.994973               0.803490            0.994973   \n",
       "2  0.995020               0.995020               0.655878            0.995020   \n",
       "3  0.994883               0.994883               0.522184            0.994883   \n",
       "4  0.995230               0.995230               0.577566            0.995230   \n",
       "5  0.995094               0.995094               0.565044            0.995094   \n",
       "6  0.995594               0.995594               0.445251            0.995594   \n",
       "7  0.995039               0.995039               0.773381            0.995039   \n",
       "8  0.995301               0.995301               0.610311            0.995301   \n",
       "9  0.995391               0.995391               0.425159            0.995391   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.351361        0.994781        0.364412      471.878178  \n",
       "1            0.383178        0.994973        0.407451      474.323679  \n",
       "2            0.346545        0.995020        0.357006      491.119469  \n",
       "3            0.352377        0.994883        0.366741      488.005002  \n",
       "4            0.344999        0.995230        0.353113      489.836057  \n",
       "5            0.336512        0.995094        0.338794      487.363261  \n",
       "6            0.352006        0.995594        0.364715      490.212037  \n",
       "7            0.346534        0.995039        0.356645      487.644616  \n",
       "8            0.348959        0.995301        0.362153      488.875778  \n",
       "9            0.336063        0.995391        0.337894      487.350621  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
