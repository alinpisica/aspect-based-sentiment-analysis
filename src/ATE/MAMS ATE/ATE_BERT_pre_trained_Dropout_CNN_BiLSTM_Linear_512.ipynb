{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_CNN_BiLSTM_Linear import ATE_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_pre_trained_dropout_cnn_bilstm_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_pre_trained_dropout_cnn_bilstm_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0722105503082275\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.07327410578727722\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.047774072736501694\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.034519366919994354\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.040949661284685135\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.033908817917108536\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.024280918762087822\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.029632771387696266\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.028078394010663033\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.022507723420858383\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03429226204752922\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.030462585389614105\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04630207270383835\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.026648161932826042\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.028076767921447754\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.027968836948275566\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.023867644369602203\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02558557130396366\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03242260217666626\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.040095917880535126\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.018305575475096703\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.018831025809049606\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0737848281860352\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06387398391962051\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03097093664109707\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.058335840702056885\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03345552459359169\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03248818591237068\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.027393223717808723\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.04507104307413101\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02493748627603054\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.031232260167598724\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02017257735133171\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.025053836405277252\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0185561403632164\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04380832612514496\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.017532486468553543\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.019145680591464043\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.013193057850003242\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.017495470121502876\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02522967755794525\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02077573537826538\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.034920647740364075\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.013251048512756824\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1298468112945557\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04802500829100609\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03033764846622944\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.030040765181183815\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.023740651085972786\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.027329228818416595\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03967466205358505\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.028994785621762276\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.04263892024755478\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03407888486981392\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.027866343036293983\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03386683762073517\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04309724643826485\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.024094676598906517\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.032703474164009094\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03137130290269852\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.019337471574544907\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02595972828567028\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.035935476422309875\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03493688255548477\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.025697417557239532\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.028498640283942223\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1004761457443237\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04726315289735794\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03328808397054672\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.027546020224690437\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.08959202468395233\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03068206086754799\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.045136693865060806\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03205167502164841\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02946833334863186\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.029521524906158447\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.023681897670030594\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.026413213461637497\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02305968850851059\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.023606501519680023\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.030845219269394875\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.02015741914510727\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01779097318649292\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.027611155062913895\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.016828997060656548\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.020010195672512054\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.018014565110206604\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02422136440873146\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1010161638259888\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.057074110954999924\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.044458210468292236\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.031082704663276672\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03224840387701988\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.026546472683548927\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.039644546806812286\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.05322542041540146\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.025713466107845306\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.029789229854941368\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.026086093857884407\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02606247551739216\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.019953357055783272\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03506564348936081\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.025687556713819504\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.02123839221894741\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.025450704619288445\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02283530682325363\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.028535909950733185\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.027656586840748787\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.01788993738591671\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.014342265203595161\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.10966956615448\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.045122113078832626\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0805087685585022\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.021824780851602554\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03127044811844826\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03593875467777252\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.022875191643834114\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0228995643556118\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.024012025445699692\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.028425613418221474\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.022502150386571884\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03728010877966881\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04245905578136444\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01922130025923252\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.026413504034280777\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.024360617622733116\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.05339482054114342\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.015447905287146568\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.015862617641687393\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0361875519156456\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0206911638379097\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.026019364595413208\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1223090887069702\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05683133006095886\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.044266294687986374\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.06603918969631195\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.04402400553226471\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02214009314775467\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03423939645290375\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.029491545632481575\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.022577019408345222\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03107777237892151\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02865820750594139\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.028783060610294342\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.021039750427007675\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.025889959186315536\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.025989821180701256\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.032517265528440475\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.014412818476557732\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.018465038388967514\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.023988759145140648\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.023069623857736588\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02136245369911194\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.04337257519364357\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0585724115371704\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0634501650929451\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03244468942284584\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.026291202753782272\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.029924942180514336\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0247984416782856\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.023882895708084106\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.024337030947208405\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.035740360617637634\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.023375386372208595\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.022359421476721764\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.020365862175822258\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.028396835550665855\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.019672486931085587\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03566073998808861\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.046001970767974854\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.025798605754971504\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02099829539656639\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03390241041779518\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.018342874944210052\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.021846452727913857\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.022830668836832047\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0724594593048096\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05205552279949188\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.052152011543512344\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04690888896584511\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.04445413500070572\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.06102200970053673\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.05164103955030441\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02354876510798931\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03713667020201683\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.017428576946258545\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03270089998841286\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.022854488343000412\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02353593520820141\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.022737424820661545\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.037634823471307755\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.020679935812950134\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.021363411098718643\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.019589673727750778\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.019157350063323975\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.023713387548923492\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.015860605984926224\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.030602704733610153\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1215571165084839\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.07639642804861069\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04459767788648605\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05857452377676964\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03877803683280945\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04086586460471153\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.04061436653137207\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02350779063999653\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03391306847333908\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0321902334690094\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.026730623096227646\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.039695776998996735\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02389610931277275\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02552765980362892\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.06193671375513077\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.024838615208864212\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.018448077142238617\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01970028132200241\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.021452024579048157\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.022515365853905678\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03174562752246857\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.020001625642180443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_CNN_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "        \n",
    "        train_losses += losses\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses) \n",
    "    plt.savefig(f'../../../results/ATE/MAMS/plots/bert_pt_do_cnn_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990639</td>\n",
       "      <td>0.990639</td>\n",
       "      <td>0.705853</td>\n",
       "      <td>0.990639</td>\n",
       "      <td>0.352494</td>\n",
       "      <td>0.990639</td>\n",
       "      <td>0.367024</td>\n",
       "      <td>1616.191297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992349</td>\n",
       "      <td>0.992349</td>\n",
       "      <td>0.707277</td>\n",
       "      <td>0.992349</td>\n",
       "      <td>0.551279</td>\n",
       "      <td>0.992349</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>1615.171319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.992280</td>\n",
       "      <td>0.992280</td>\n",
       "      <td>0.727045</td>\n",
       "      <td>0.992280</td>\n",
       "      <td>0.520513</td>\n",
       "      <td>0.992280</td>\n",
       "      <td>0.586322</td>\n",
       "      <td>1613.149523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992128</td>\n",
       "      <td>0.992128</td>\n",
       "      <td>0.701378</td>\n",
       "      <td>0.992128</td>\n",
       "      <td>0.580567</td>\n",
       "      <td>0.992128</td>\n",
       "      <td>0.627427</td>\n",
       "      <td>1614.307716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992334</td>\n",
       "      <td>0.992334</td>\n",
       "      <td>0.703994</td>\n",
       "      <td>0.992334</td>\n",
       "      <td>0.582319</td>\n",
       "      <td>0.992334</td>\n",
       "      <td>0.630882</td>\n",
       "      <td>1617.130129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.991962</td>\n",
       "      <td>0.991962</td>\n",
       "      <td>0.702444</td>\n",
       "      <td>0.991962</td>\n",
       "      <td>0.500992</td>\n",
       "      <td>0.991962</td>\n",
       "      <td>0.557916</td>\n",
       "      <td>1620.009012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.716169</td>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.581547</td>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.631535</td>\n",
       "      <td>1617.106561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.992224</td>\n",
       "      <td>0.992224</td>\n",
       "      <td>0.686282</td>\n",
       "      <td>0.992224</td>\n",
       "      <td>0.555975</td>\n",
       "      <td>0.992224</td>\n",
       "      <td>0.606128</td>\n",
       "      <td>1617.528468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.991886</td>\n",
       "      <td>0.991886</td>\n",
       "      <td>0.685826</td>\n",
       "      <td>0.991886</td>\n",
       "      <td>0.519126</td>\n",
       "      <td>0.991886</td>\n",
       "      <td>0.571762</td>\n",
       "      <td>1617.472460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.991347</td>\n",
       "      <td>0.991347</td>\n",
       "      <td>0.734032</td>\n",
       "      <td>0.991347</td>\n",
       "      <td>0.442649</td>\n",
       "      <td>0.991347</td>\n",
       "      <td>0.488628</td>\n",
       "      <td>1618.234841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.990639               0.990639               0.705853            0.990639   \n",
       "1  0.992349               0.992349               0.707277            0.992349   \n",
       "2  0.992280               0.992280               0.727045            0.992280   \n",
       "3  0.992128               0.992128               0.701378            0.992128   \n",
       "4  0.992334               0.992334               0.703994            0.992334   \n",
       "5  0.991962               0.991962               0.702444            0.991962   \n",
       "6  0.992621               0.992621               0.716169            0.992621   \n",
       "7  0.992224               0.992224               0.686282            0.992224   \n",
       "8  0.991886               0.991886               0.685826            0.991886   \n",
       "9  0.991347               0.991347               0.734032            0.991347   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.352494        0.990639        0.367024     1616.191297  \n",
       "1            0.551279        0.992349        0.608252     1615.171319  \n",
       "2            0.520513        0.992280        0.586322     1613.149523  \n",
       "3            0.580567        0.992128        0.627427     1614.307716  \n",
       "4            0.582319        0.992334        0.630882     1617.130129  \n",
       "5            0.500992        0.991962        0.557916     1620.009012  \n",
       "6            0.581547        0.992621        0.631535     1617.106561  \n",
       "7            0.555975        0.992224        0.606128     1617.528468  \n",
       "8            0.519126        0.991886        0.571762     1617.472460  \n",
       "9            0.442649        0.991347        0.488628     1618.234841  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
