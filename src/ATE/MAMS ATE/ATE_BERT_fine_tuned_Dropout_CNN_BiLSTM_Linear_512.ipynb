{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_CNN_BiLSTM_Linear import ATE_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/MAMS/models/bert_fine_tuned_512.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_fine_tuned_dropout_cnn_bilstm_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_fine_tuned_dropout_cnn_bilstm_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0817738771438599\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05245033651590347\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.048219457268714905\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03292359039187431\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0362018421292305\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.026968931779265404\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.019939076155424118\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.031239192932844162\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.032638635486364365\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01984493061900139\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03697456791996956\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.025826171040534973\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.014516953378915787\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01959838718175888\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.022304031997919083\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.011032888665795326\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.017039502039551735\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02391478791832924\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02354644611477852\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.00971208419650793\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02221178077161312\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.011675633490085602\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1055426597595215\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.11218653619289398\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03141457587480545\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03610438480973244\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.029389256611466408\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03801650553941727\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02089819312095642\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.021628689020872116\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.017555924132466316\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.030505066737532616\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.019392425194382668\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.030258987098932266\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01840665005147457\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.027984991669654846\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01687038503587246\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.017811136320233345\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01821507140994072\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01707831397652626\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.013808722607791424\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01530909352004528\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.012630513869225979\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01350175216794014\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1055556535720825\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.046992696821689606\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05607130751013756\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.040707848966121674\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.044303048402071\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03153584152460098\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.037640318274497986\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03262295201420784\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.017033329233527184\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02075091190636158\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.023919714614748955\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.018711986020207405\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.017345331609249115\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.017389483749866486\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01281698141247034\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.015868280082941055\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.016543611884117126\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.00991617888212204\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.027229513972997665\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.010218790732324123\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.008501283824443817\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.018522290512919426\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1065788269042969\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05212390795350075\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0614202618598938\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.029071182012557983\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.028157778084278107\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.036809924989938736\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02687687613070011\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02301495149731636\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03140217065811157\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.021931277588009834\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.01955137960612774\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.016890408471226692\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.011453751474618912\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.016537431627511978\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.013157297857105732\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.013896051794290543\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.021861126646399498\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.014653806574642658\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01542662549763918\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.04905477911233902\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.007021242752671242\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.011178282089531422\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1092873811721802\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05865968391299248\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.040880750864744186\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0287055354565382\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03300394490361214\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.05129442736506462\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03721131384372711\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.020580818876624107\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.025348467752337456\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.030692782253026962\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02223176881670952\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.014711868949234486\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.020716676488518715\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.017471106722950935\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.012030140496790409\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.02142452634871006\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.017166784033179283\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01509707048535347\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.015635626390576363\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01963229849934578\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.018777962774038315\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.017695028334856033\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0921021699905396\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05609435588121414\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.06324911117553711\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03769751265645027\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.035714976489543915\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.06806435436010361\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.056954991072416306\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0200053583830595\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.031858984380960464\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02072257176041603\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02284414693713188\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.018295614048838615\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01360603328794241\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.014332912862300873\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.012751933187246323\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.01967979408800602\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.028412727639079094\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.020639164373278618\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.012714230455458164\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02689984440803528\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.019482310861349106\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.017320051789283752\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0352718830108643\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05459490045905113\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.034994158893823624\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.028414415195584297\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.031326331198215485\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.019237590953707695\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03530197963118553\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.04391621798276901\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.04411963000893593\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.027639225125312805\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03408530354499817\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03246813267469406\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0234641395509243\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01620626263320446\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.036344774067401886\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.01577540673315525\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.011575686745345592\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.029167771339416504\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01741102524101734\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.018181975930929184\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.013804558664560318\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.012314660474658012\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1249240636825562\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.054258234798908234\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.045962680131196976\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03433224558830261\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.04125601053237915\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02866545133292675\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02685537561774254\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02893136814236641\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02188110537827015\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.023779256269335747\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.015415026806294918\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01764250546693802\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.025352541357278824\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01779506914317608\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.018995337188243866\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.020332301035523415\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01892879232764244\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.025390127673745155\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.013183039613068104\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.007749040611088276\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.011390564031898975\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.012541376054286957\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1628848314285278\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06849126517772675\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03998212888836861\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.06290428340435028\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0519978329539299\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.025989282876253128\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.022268686443567276\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.025224747136235237\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03508234769105911\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.025683874264359474\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02991431951522827\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.030520565807819366\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.023390991613268852\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.023462673649191856\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.020209651440382004\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.02245229110121727\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01848745159804821\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01939738169312477\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01537617202848196\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.019733719527721405\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.016813727095723152\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.015032025054097176\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0769325494766235\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.045187808573246\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05848295986652374\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05121244117617607\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.02353888377547264\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.033525463193655014\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.026041632518172264\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.033546630293130875\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.018970170989632607\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.027881909161806107\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.015200822614133358\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.014926483854651451\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01921825483441353\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04480750858783722\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.021692512556910515\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.01633327454328537\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.024607021361589432\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01353341806679964\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02920754998922348\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.014541187323629856\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.01662966050207615\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.016031630337238312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_CNN_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "\n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses) \n",
    "    plt.savefig(f'../../../results/ATE/MAMS/plots/bert_ft_do_cnn_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993817</td>\n",
       "      <td>0.993817</td>\n",
       "      <td>0.752448</td>\n",
       "      <td>0.993817</td>\n",
       "      <td>0.684912</td>\n",
       "      <td>0.993817</td>\n",
       "      <td>0.706520</td>\n",
       "      <td>1659.912503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993898</td>\n",
       "      <td>0.993898</td>\n",
       "      <td>0.753364</td>\n",
       "      <td>0.993898</td>\n",
       "      <td>0.718110</td>\n",
       "      <td>0.993898</td>\n",
       "      <td>0.733681</td>\n",
       "      <td>1630.261852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.735741</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.742791</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.738844</td>\n",
       "      <td>1627.692947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994087</td>\n",
       "      <td>0.994087</td>\n",
       "      <td>0.750959</td>\n",
       "      <td>0.994087</td>\n",
       "      <td>0.735246</td>\n",
       "      <td>0.994087</td>\n",
       "      <td>0.742676</td>\n",
       "      <td>1629.407459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.994255</td>\n",
       "      <td>0.994255</td>\n",
       "      <td>0.788902</td>\n",
       "      <td>0.994255</td>\n",
       "      <td>0.673206</td>\n",
       "      <td>0.994255</td>\n",
       "      <td>0.718792</td>\n",
       "      <td>1630.436893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.993772</td>\n",
       "      <td>0.993772</td>\n",
       "      <td>0.739853</td>\n",
       "      <td>0.993772</td>\n",
       "      <td>0.755009</td>\n",
       "      <td>0.993772</td>\n",
       "      <td>0.745245</td>\n",
       "      <td>1629.688537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.994003</td>\n",
       "      <td>0.994003</td>\n",
       "      <td>0.749265</td>\n",
       "      <td>0.994003</td>\n",
       "      <td>0.712162</td>\n",
       "      <td>0.994003</td>\n",
       "      <td>0.727160</td>\n",
       "      <td>1629.666215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.994328</td>\n",
       "      <td>0.994328</td>\n",
       "      <td>0.769974</td>\n",
       "      <td>0.994328</td>\n",
       "      <td>0.731971</td>\n",
       "      <td>0.994328</td>\n",
       "      <td>0.747820</td>\n",
       "      <td>1627.797495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.993852</td>\n",
       "      <td>0.993852</td>\n",
       "      <td>0.768365</td>\n",
       "      <td>0.993852</td>\n",
       "      <td>0.689388</td>\n",
       "      <td>0.993852</td>\n",
       "      <td>0.705667</td>\n",
       "      <td>1629.460791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.994390</td>\n",
       "      <td>0.994390</td>\n",
       "      <td>0.763168</td>\n",
       "      <td>0.994390</td>\n",
       "      <td>0.739222</td>\n",
       "      <td>0.994390</td>\n",
       "      <td>0.750065</td>\n",
       "      <td>1629.666848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.993817               0.993817               0.752448            0.993817   \n",
       "1  0.993898               0.993898               0.753364            0.993898   \n",
       "2  0.993967               0.993967               0.735741            0.993967   \n",
       "3  0.994087               0.994087               0.750959            0.994087   \n",
       "4  0.994255               0.994255               0.788902            0.994255   \n",
       "5  0.993772               0.993772               0.739853            0.993772   \n",
       "6  0.994003               0.994003               0.749265            0.994003   \n",
       "7  0.994328               0.994328               0.769974            0.994328   \n",
       "8  0.993852               0.993852               0.768365            0.993852   \n",
       "9  0.994390               0.994390               0.763168            0.994390   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.684912        0.993817        0.706520     1659.912503  \n",
       "1            0.718110        0.993898        0.733681     1630.261852  \n",
       "2            0.742791        0.993967        0.738844     1627.692947  \n",
       "3            0.735246        0.994087        0.742676     1629.407459  \n",
       "4            0.673206        0.994255        0.718792     1630.436893  \n",
       "5            0.755009        0.993772        0.745245     1629.688537  \n",
       "6            0.712162        0.994003        0.727160     1629.666215  \n",
       "7            0.731971        0.994328        0.747820     1627.797495  \n",
       "8            0.689388        0.993852        0.705667     1629.460791  \n",
       "9            0.739222        0.994390        0.750065     1629.666848  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
