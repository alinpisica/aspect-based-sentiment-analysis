{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_Linear import ATE_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_MODEL_PATH = '../../../results/ATE/MAMS/models/bert_fine_tuned_512.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_pre_trained_dropout_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_pre_trained_dropout_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1924453973770142\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.024176152423024178\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.01796279475092888\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.017223253846168518\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.008598576299846172\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.01594330556690693\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.006236921530216932\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.004286002833396196\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.011311477050185204\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0320364348590374\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.00383065571077168\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013008076697587967\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.004415555391460657\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.009358244016766548\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.012043026275932789\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.007764523383229971\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004216928035020828\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.007966681383550167\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.014413350261747837\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.010056991130113602\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.007960570976138115\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.006932782009243965\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0125080347061157\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.020128700882196426\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.011937817558646202\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.007028648629784584\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.010422198101878166\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.017876064404845238\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.008293294347822666\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.012244375422596931\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.013506107032299042\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.029494300484657288\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.00897677056491375\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013549135997891426\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.006485210731625557\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.010733379051089287\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.011716723442077637\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.001213112729601562\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0028019181918352842\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.00529248034581542\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.007840731181204319\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.006350764539092779\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.010588201694190502\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.016345391049981117\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.9878734350204468\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.024241222068667412\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.014137506484985352\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.022702433168888092\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.011485728435218334\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.008894492872059345\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.007836658507585526\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.014546742662787437\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.012791294604539871\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.004580087959766388\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.007746411021798849\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.007918178103864193\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01483047567307949\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.009062704630196095\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.00723673403263092\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.003304511308670044\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.010200200602412224\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.00442953547462821\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.002715876791626215\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.009081128984689713\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.019334105774760246\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.005440130364149809\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.9986047148704529\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.02172737754881382\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.018403131514787674\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.015527918003499508\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.019320154562592506\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.011319897137582302\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.010273019783198833\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0031503092031925917\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.007661033421754837\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.018856197595596313\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.009833207353949547\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.006617583800107241\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0031937898602336645\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0017781549831852317\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.010083135217428207\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.006224574986845255\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.00936242938041687\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.00513113709166646\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.015620488673448563\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.002106259111315012\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.002410308690741658\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01019531860947609\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1548113822937012\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.02567427046597004\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.01857382245361805\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.026140455156564713\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0054001230746507645\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.005812857765704393\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.013300611637532711\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.011696266941726208\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.015692593529820442\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01279025711119175\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.007549256552010775\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.002982571255415678\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.010333821177482605\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.011463817209005356\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0075408415868878365\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0026938687078654766\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0159580260515213\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006803295109421015\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.016714729368686676\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.005134328734129667\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.004692165646702051\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.005710985045880079\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4083480834960938\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03499842435121536\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.018354538828134537\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.028116967529058456\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.008764048106968403\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.015903344377875328\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.012894106097519398\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.029017116874456406\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.005313941277563572\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0186796672642231\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.009173869155347347\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.011134753935039043\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0017972453497350216\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.005594467278569937\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01209789328277111\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0035237802658230066\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004080282058566809\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.008594248443841934\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.00559084489941597\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.004703043960034847\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.008462212048470974\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.005983871873468161\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.087485671043396\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.02953917905688286\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.02166815847158432\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.02070716954767704\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.02059430070221424\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.021155575290322304\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.010466156527400017\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.012892154976725578\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.008056040853261948\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.007998361252248287\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.005776331759989262\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.00672511849552393\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0055222720839083195\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.013703960925340652\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0074324882589280605\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.010351592674851418\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.005310573615133762\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.009153475984930992\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0051751332357525826\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.004884473979473114\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.00704790698364377\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.008782758377492428\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1002578735351562\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03336598351597786\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.010933461599051952\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.029579905793070793\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.021173132583498955\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.020015211775898933\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.007753194309771061\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.009687705896794796\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.016927601769566536\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.010938002727925777\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.006525915116071701\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0058903321623802185\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.004275386221706867\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0072469934821128845\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.008291331119835377\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.014482226222753525\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.006824432872235775\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.005605590995401144\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.004735719878226519\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0059196241199970245\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.013285736553370953\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.004291690420359373\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2121310234069824\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03761761635541916\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.011927072890102863\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.017295755445957184\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.013831921853125095\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02072271704673767\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.011444672010838985\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.010107786394655704\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.012503480538725853\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01919478364288807\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.01547943614423275\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013325800187885761\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0036249367985874414\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.014528179541230202\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.012582000344991684\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.020987020805478096\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.007517472840845585\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0070978631265461445\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.015486222691833973\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.004786169156432152\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0056465622037649155\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.007058004382997751\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.790860652923584\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.023070676252245903\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.021677963435649872\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0069967410527169704\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.017838114872574806\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0178251750767231\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.007212388329207897\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02356744185090065\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.006412006914615631\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.008824719116091728\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.015576932579278946\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.012491482309997082\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.013545624911785126\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.011980757117271423\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.009872387163341045\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.00862923078238964\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.007662393618375063\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.007429803721606731\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0048936521634459496\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0054051876068115234\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.010556438937783241\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.00626655388623476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "\n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses) \n",
    "    plt.savefig(f'../../../results/ATE/MAMS/plots/bert_pt_do_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, BERT_PRETRAINED_MODEL_PATH)\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997646</td>\n",
       "      <td>0.997646</td>\n",
       "      <td>0.860884</td>\n",
       "      <td>0.997646</td>\n",
       "      <td>0.953007</td>\n",
       "      <td>0.997646</td>\n",
       "      <td>0.901611</td>\n",
       "      <td>1439.388437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997861</td>\n",
       "      <td>0.997861</td>\n",
       "      <td>0.895496</td>\n",
       "      <td>0.997861</td>\n",
       "      <td>0.936613</td>\n",
       "      <td>0.997861</td>\n",
       "      <td>0.915183</td>\n",
       "      <td>1438.388121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.907002</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.941772</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.923730</td>\n",
       "      <td>1434.535740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.997744</td>\n",
       "      <td>0.997744</td>\n",
       "      <td>0.876554</td>\n",
       "      <td>0.997744</td>\n",
       "      <td>0.949764</td>\n",
       "      <td>0.997744</td>\n",
       "      <td>0.909944</td>\n",
       "      <td>1435.887912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.884698</td>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.953805</td>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.917156</td>\n",
       "      <td>1437.132590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.997338</td>\n",
       "      <td>0.997338</td>\n",
       "      <td>0.907207</td>\n",
       "      <td>0.997338</td>\n",
       "      <td>0.884491</td>\n",
       "      <td>0.997338</td>\n",
       "      <td>0.895515</td>\n",
       "      <td>1437.318824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.998040</td>\n",
       "      <td>0.998040</td>\n",
       "      <td>0.898722</td>\n",
       "      <td>0.998040</td>\n",
       "      <td>0.955386</td>\n",
       "      <td>0.998040</td>\n",
       "      <td>0.925654</td>\n",
       "      <td>1436.254085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.997885</td>\n",
       "      <td>0.997885</td>\n",
       "      <td>0.897226</td>\n",
       "      <td>0.997885</td>\n",
       "      <td>0.942250</td>\n",
       "      <td>0.997885</td>\n",
       "      <td>0.918713</td>\n",
       "      <td>1420.754454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.997763</td>\n",
       "      <td>0.997763</td>\n",
       "      <td>0.893185</td>\n",
       "      <td>0.997763</td>\n",
       "      <td>0.946991</td>\n",
       "      <td>0.997763</td>\n",
       "      <td>0.918362</td>\n",
       "      <td>1421.780626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.997946</td>\n",
       "      <td>0.997946</td>\n",
       "      <td>0.896056</td>\n",
       "      <td>0.997946</td>\n",
       "      <td>0.948585</td>\n",
       "      <td>0.997946</td>\n",
       "      <td>0.920990</td>\n",
       "      <td>1419.940643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.997646               0.997646               0.860884            0.997646   \n",
       "1  0.997861               0.997861               0.895496            0.997861   \n",
       "2  0.998004               0.998004               0.907002            0.998004   \n",
       "3  0.997744               0.997744               0.876554            0.997744   \n",
       "4  0.997765               0.997765               0.884698            0.997765   \n",
       "5  0.997338               0.997338               0.907207            0.997338   \n",
       "6  0.998040               0.998040               0.898722            0.998040   \n",
       "7  0.997885               0.997885               0.897226            0.997885   \n",
       "8  0.997763               0.997763               0.893185            0.997763   \n",
       "9  0.997946               0.997946               0.896056            0.997946   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.953007        0.997646        0.901611     1439.388437  \n",
       "1            0.936613        0.997861        0.915183     1438.388121  \n",
       "2            0.941772        0.998004        0.923730     1434.535740  \n",
       "3            0.949764        0.997744        0.909944     1435.887912  \n",
       "4            0.953805        0.997765        0.917156     1437.132590  \n",
       "5            0.884491        0.997338        0.895515     1437.318824  \n",
       "6            0.955386        0.998040        0.925654     1436.254085  \n",
       "7            0.942250        0.997885        0.918713     1420.754454  \n",
       "8            0.946991        0.997763        0.918362     1421.780626  \n",
       "9            0.948585        0.997946        0.920990     1419.940643  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
