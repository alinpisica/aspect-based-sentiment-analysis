{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_BiLSTM_Linear import ATE_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/MAMS/models/bert_fine_tuned_512.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_fine_tuned_dropout_bilstm_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_fine_tuned_dropout_bilstm_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2165844440460205\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.02155662700533867\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.011315091513097286\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.011806274764239788\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0063907308503985405\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.006575372070074081\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.004701375961303711\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0045302025973796844\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.004071451257914305\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.00749045517295599\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0028955568559467793\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.012212586589157581\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0020249783992767334\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.006487362086772919\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0042348457500338554\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0038898009806871414\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0013606420252472162\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.003549288958311081\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.002672238741070032\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.005649803671985865\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.003622849239036441\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.00152248062659055\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1219574213027954\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.016258610412478447\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.01285570953041315\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.012134500779211521\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.00986169558018446\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.00872652605175972\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.005763006396591663\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0051498799584805965\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.007256134878844023\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0035901707597076893\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.004432050045579672\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.004655987955629826\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0020390988793224096\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0035284783225506544\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0009552345145493746\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.00671065878123045\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.006783642340451479\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0023023271933197975\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.006134079769253731\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0035921521484851837\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0024364145938307047\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.010825802572071552\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1024092435836792\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.020677141845226288\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.014678585343062878\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.011122867465019226\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.006372710224241018\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.007230306509882212\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.011512117460370064\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.01040169969201088\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.004146298859268427\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.004417250398546457\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0066412705928087234\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.004462616518139839\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0026666182093322277\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0032422197982668877\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0016213702037930489\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0015297713689506054\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0017821639776229858\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0037525659427046776\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.00418085278943181\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.011306226253509521\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0024939256254583597\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0014642798341810703\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.958931028842926\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.019638102501630783\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.016047250479459763\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.005050027742981911\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.00496092950925231\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.008224737830460072\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0034882347099483013\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.00941743329167366\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.005807788576930761\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.008955300785601139\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.004008215386420488\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.004880945198237896\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.004554683808237314\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.005991359706968069\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.005457188934087753\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.005690151825547218\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.003199923550710082\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.005312050227075815\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.00541103258728981\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0021049289498478174\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0017692813416942954\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0017019567312672734\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.168684482574463\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.01726606860756874\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.009084268473088741\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.008772141300141811\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.006322181783616543\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.008923923596739769\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.007632116787135601\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0033675532322376966\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.009334364905953407\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.002447383711114526\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0029032218735665083\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.003347288118675351\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0028569521382451057\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.002702167956158519\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0027001688722521067\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.004030196461826563\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0036896453239023685\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0063750543631613255\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0044584982097148895\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0016991631127893925\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0032855281606316566\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.002343634609133005\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0792279243469238\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.034579355269670486\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.012052883394062519\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0071812840178608894\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.014675600454211235\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0055225444957613945\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0059524718672037125\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.008479995653033257\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.009116382338106632\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.008254659362137318\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.005619560368359089\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0027624673675745726\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0044966088607907295\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.009407277218997478\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.008926698938012123\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.003130198922008276\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004265427589416504\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0024113920517265797\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0027793399058282375\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.006412080023437738\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0032127206213772297\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0028629249427467585\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0938260555267334\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.02942511811852455\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.009488703683018684\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.015037141740322113\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.009779362007975578\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.006871916353702545\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.009415291249752045\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.005492922384291887\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.010155096650123596\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0025937799364328384\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.004882173612713814\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.005657623987644911\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.007167264353483915\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.003327643498778343\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.006222209893167019\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.006107193883508444\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004149130079895258\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.001957102445885539\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.006799662485718727\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.00258718803524971\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.009025434032082558\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0023016936611384153\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1421126127243042\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.023928994312882423\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.01234526839107275\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.00723551120609045\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.008427337743341923\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.003601644653826952\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0060875494964420795\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.010314320214092731\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.013627324253320694\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.003363702679052949\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.003097750712186098\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.003676215186715126\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0019268222386017442\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.00292965373955667\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0032608932815492153\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.004588679410517216\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0014828700805082917\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.002732593333348632\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.005885889288038015\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.006637897342443466\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.004124348051846027\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0024618427269160748\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1490663290023804\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.019503716379404068\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.009902562014758587\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.009025687351822853\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0065376125276088715\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.009380136616528034\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.00802620593458414\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.00870621856302023\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.010281136259436607\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0037071695551276207\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.00278289383277297\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.005769258365035057\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.001982869813218713\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.006954406388103962\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0013162664836272597\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.006084950640797615\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004714782349765301\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0021761248353868723\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0036099296994507313\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0024597961455583572\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0032391289714723825\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0033442042768001556\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.042443871498108\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0186191126704216\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.009622413665056229\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.014354364946484566\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.01290209125727415\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.004736942704766989\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.007496774196624756\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.008221819065511227\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0051863547414541245\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.002364536514505744\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.009106321260333061\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0038816630840301514\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0024936299305409193\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0016975613543763757\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.004098900128155947\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0019058799371123314\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004465548787266016\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0031983861699700356\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0014410641742870212\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0009681959636509418\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.001664796145632863\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.009079987183213234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, bilstm_in_features=256, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "\n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses) \n",
    "    plt.savefig(f'../../../results/ATE/MAMS/plots/bert_ft_do_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999131</td>\n",
       "      <td>0.999131</td>\n",
       "      <td>0.951485</td>\n",
       "      <td>0.999131</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.999131</td>\n",
       "      <td>0.968226</td>\n",
       "      <td>1618.310789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998998</td>\n",
       "      <td>0.998998</td>\n",
       "      <td>0.938221</td>\n",
       "      <td>0.998998</td>\n",
       "      <td>0.986781</td>\n",
       "      <td>0.998998</td>\n",
       "      <td>0.961551</td>\n",
       "      <td>1666.408872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999085</td>\n",
       "      <td>0.999085</td>\n",
       "      <td>0.947506</td>\n",
       "      <td>0.999085</td>\n",
       "      <td>0.988524</td>\n",
       "      <td>0.999085</td>\n",
       "      <td>0.967323</td>\n",
       "      <td>1627.174331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999230</td>\n",
       "      <td>0.999230</td>\n",
       "      <td>0.958212</td>\n",
       "      <td>0.999230</td>\n",
       "      <td>0.986020</td>\n",
       "      <td>0.999230</td>\n",
       "      <td>0.971796</td>\n",
       "      <td>1583.238395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.945824</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.987281</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.965865</td>\n",
       "      <td>1573.022270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.998913</td>\n",
       "      <td>0.998913</td>\n",
       "      <td>0.937490</td>\n",
       "      <td>0.998913</td>\n",
       "      <td>0.990781</td>\n",
       "      <td>0.998913</td>\n",
       "      <td>0.962981</td>\n",
       "      <td>1567.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.999035</td>\n",
       "      <td>0.999035</td>\n",
       "      <td>0.945077</td>\n",
       "      <td>0.999035</td>\n",
       "      <td>0.989697</td>\n",
       "      <td>0.999035</td>\n",
       "      <td>0.966549</td>\n",
       "      <td>1526.996192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999016</td>\n",
       "      <td>0.999016</td>\n",
       "      <td>0.956077</td>\n",
       "      <td>0.999016</td>\n",
       "      <td>0.971442</td>\n",
       "      <td>0.999016</td>\n",
       "      <td>0.963630</td>\n",
       "      <td>1503.032133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.950140</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.985888</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.967471</td>\n",
       "      <td>1559.971233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.998912</td>\n",
       "      <td>0.998912</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.998912</td>\n",
       "      <td>0.985864</td>\n",
       "      <td>0.998912</td>\n",
       "      <td>0.958461</td>\n",
       "      <td>1605.397306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.999131               0.999131               0.951485            0.999131   \n",
       "1  0.998998               0.998998               0.938221            0.998998   \n",
       "2  0.999085               0.999085               0.947506            0.999085   \n",
       "3  0.999230               0.999230               0.958212            0.999230   \n",
       "4  0.999055               0.999055               0.945824            0.999055   \n",
       "5  0.998913               0.998913               0.937490            0.998913   \n",
       "6  0.999035               0.999035               0.945077            0.999035   \n",
       "7  0.999016               0.999016               0.956077            0.999016   \n",
       "8  0.999103               0.999103               0.950140            0.999103   \n",
       "9  0.998912               0.998912               0.933333            0.998912   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.985915        0.999131        0.968226     1618.310789  \n",
       "1            0.986781        0.998998        0.961551     1666.408872  \n",
       "2            0.988524        0.999085        0.967323     1627.174331  \n",
       "3            0.986020        0.999230        0.971796     1583.238395  \n",
       "4            0.987281        0.999055        0.965865     1573.022270  \n",
       "5            0.990781        0.998913        0.962981     1567.759400  \n",
       "6            0.989697        0.999035        0.966549     1526.996192  \n",
       "7            0.971442        0.999016        0.963630     1503.032133  \n",
       "8            0.985888        0.999103        0.967471     1559.971233  \n",
       "9            0.985864        0.998912        0.958461     1605.397306  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
