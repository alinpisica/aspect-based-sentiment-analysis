{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_BiLSTM_Linear import ATE_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/MAMS/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_fine_tuned_dropout_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_fine_tuned_dropout_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1951433420181274\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.07952836900949478\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.10330259799957275\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.07506105303764343\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.06397013366222382\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.054204873740673065\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0828690305352211\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.045660171657800674\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.037886153906583786\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01620856113731861\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.04230530932545662\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.024709636345505714\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0064264643006026745\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.041651055216789246\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.041600558906793594\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.052952591329813004\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.009115450084209442\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.03007272630929947\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.030419912189245224\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.018021874129772186\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.008745215833187103\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.014184019528329372\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.129387617111206\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.09908320754766464\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0796404778957367\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.12138734757900238\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.038559868931770325\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.05108030140399933\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.033417172729969025\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.16075578331947327\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.010896271094679832\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03441360592842102\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.024832498282194138\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.037313736975193024\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.08951009064912796\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.06377158313989639\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.025032132863998413\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.022777769714593887\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.010609214194118977\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.04287461191415787\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.012695308774709702\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.039183858782052994\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.007972600869834423\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02189757488667965\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1713448762893677\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.12825259566307068\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.2551252543926239\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04246290400624275\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0385529063642025\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03430711850523949\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03676210716366768\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.011078936979174614\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.05679008364677429\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.011119617149233818\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.09106018394231796\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.022748198360204697\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.027071157470345497\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.014675276353955269\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.021615689620375633\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04047789052128792\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02767067402601242\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0307144932448864\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02276570163667202\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03158942610025406\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.012144696898758411\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.009312921203672886\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0517972707748413\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.1228826567530632\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04400913417339325\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.08551246672868729\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.08111982047557831\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02492532692849636\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.021263297647237778\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.07275693118572235\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.053509198129177094\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.10205071419477463\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.05323083698749542\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06798427551984787\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.019755415618419647\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04527664929628372\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.006499660201370716\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.060663629323244095\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.008741738274693489\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.029806390404701233\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01723765768110752\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.016177361831068993\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.006206739228218794\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.04734978452324867\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.090611457824707\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.09961865097284317\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.061337824910879135\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.09547613561153412\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.052617449313402176\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.05337483808398247\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.04272180423140526\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.05434737727046013\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.08511494100093842\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.07599174976348877\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.006855139974504709\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.023408053442835808\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.07760109752416611\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.006904395762830973\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03212180361151695\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.016662806272506714\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02651950903236866\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.05154125764966011\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.05148383602499962\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.06043761223554611\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.012941424734890461\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.021945560351014137\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1230614185333252\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06756704300642014\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03569667786359787\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05558047071099281\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.020688496530056\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.028990978375077248\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.07995877414941788\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02287563681602478\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.06428323686122894\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.029521653428673744\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.014547866769134998\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.07875055074691772\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03631591796875\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.009657561779022217\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.055911581963300705\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.009191259741783142\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.012628380209207535\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02326134778559208\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.004834598861634731\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01546418946236372\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03586428239941597\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.011409317143261433\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.035710334777832\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.11040976643562317\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0667223408818245\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.048829205334186554\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07281678915023804\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.028651472181081772\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.09053962677717209\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02085495926439762\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.014124994166195393\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.017064765095710754\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02148357219994068\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.028905222192406654\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.021645639091730118\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.031744081526994705\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.006600145250558853\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.006928762886673212\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.007534600328654051\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.024942966178059578\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.04996403679251671\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.027792036533355713\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.011467845179140568\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.07978498190641403\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.102984070777893\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.14450311660766602\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.09612374007701874\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.026237728074193\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.1254877895116806\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.06699351221323013\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.16102167963981628\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.1181565672159195\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.055037952959537506\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.05992279574275017\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.016413792967796326\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03591964393854141\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.008636616170406342\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02124939300119877\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02517440915107727\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.041927237063646317\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01655888371169567\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.03286566212773323\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0650719478726387\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.026303580030798912\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0029779623728245497\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0059592705219984055\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1234674453735352\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.09109373390674591\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04529539495706558\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.13861139118671417\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.04101138189435005\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.07178997248411179\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.012893644161522388\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.023697661235928535\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.07556021213531494\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06344884634017944\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0158168263733387\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.060734059661626816\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.020166903734207153\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.005183672532439232\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04964861273765564\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.09833645820617676\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02277984283864498\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.003966080490499735\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.015376473776996136\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.021604351699352264\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.060630958527326584\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.020554611459374428\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2551283836364746\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.11106639355421066\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03502683714032173\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04066413268446922\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.026426320895552635\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.10094178467988968\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.020561635494232178\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.012621167115867138\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.1059238463640213\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.023471003398299217\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.016466783359646797\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01890513114631176\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.05036304518580437\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.015989389270544052\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03392653912305832\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.05233372375369072\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03876104950904846\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02568853087723255\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.038958799093961716\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.012217873707413673\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.012279079295694828\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.010213838890194893\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, bilstm_in_features=256, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.967493</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.989120</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.978096</td>\n",
       "      <td>453.174581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.994098</td>\n",
       "      <td>0.994098</td>\n",
       "      <td>0.972322</td>\n",
       "      <td>0.994098</td>\n",
       "      <td>0.990406</td>\n",
       "      <td>0.994098</td>\n",
       "      <td>0.981217</td>\n",
       "      <td>437.798032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.992646</td>\n",
       "      <td>0.992646</td>\n",
       "      <td>0.964468</td>\n",
       "      <td>0.992646</td>\n",
       "      <td>0.989560</td>\n",
       "      <td>0.992646</td>\n",
       "      <td>0.976722</td>\n",
       "      <td>437.227031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992693</td>\n",
       "      <td>0.992693</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.992693</td>\n",
       "      <td>0.987819</td>\n",
       "      <td>0.992693</td>\n",
       "      <td>0.976976</td>\n",
       "      <td>438.053217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992962</td>\n",
       "      <td>0.992962</td>\n",
       "      <td>0.968637</td>\n",
       "      <td>0.992962</td>\n",
       "      <td>0.984984</td>\n",
       "      <td>0.992962</td>\n",
       "      <td>0.976671</td>\n",
       "      <td>436.140970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.992916</td>\n",
       "      <td>0.992916</td>\n",
       "      <td>0.970344</td>\n",
       "      <td>0.992916</td>\n",
       "      <td>0.984887</td>\n",
       "      <td>0.992916</td>\n",
       "      <td>0.977475</td>\n",
       "      <td>435.893028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.993717</td>\n",
       "      <td>0.993717</td>\n",
       "      <td>0.976571</td>\n",
       "      <td>0.993717</td>\n",
       "      <td>0.984014</td>\n",
       "      <td>0.993717</td>\n",
       "      <td>0.980268</td>\n",
       "      <td>437.232998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.992397</td>\n",
       "      <td>0.992397</td>\n",
       "      <td>0.964090</td>\n",
       "      <td>0.992397</td>\n",
       "      <td>0.990211</td>\n",
       "      <td>0.992397</td>\n",
       "      <td>0.976828</td>\n",
       "      <td>436.384657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.993425</td>\n",
       "      <td>0.993425</td>\n",
       "      <td>0.972847</td>\n",
       "      <td>0.993425</td>\n",
       "      <td>0.984210</td>\n",
       "      <td>0.993425</td>\n",
       "      <td>0.978471</td>\n",
       "      <td>435.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.992985</td>\n",
       "      <td>0.992985</td>\n",
       "      <td>0.967598</td>\n",
       "      <td>0.992985</td>\n",
       "      <td>0.987767</td>\n",
       "      <td>0.992985</td>\n",
       "      <td>0.977500</td>\n",
       "      <td>437.152997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.993164               0.993164               0.967493            0.993164   \n",
       "1  0.994098               0.994098               0.972322            0.994098   \n",
       "2  0.992646               0.992646               0.964468            0.992646   \n",
       "3  0.992693               0.992693               0.966554            0.992693   \n",
       "4  0.992962               0.992962               0.968637            0.992962   \n",
       "5  0.992916               0.992916               0.970344            0.992916   \n",
       "6  0.993717               0.993717               0.976571            0.993717   \n",
       "7  0.992397               0.992397               0.964090            0.992397   \n",
       "8  0.993425               0.993425               0.972847            0.993425   \n",
       "9  0.992985               0.992985               0.967598            0.992985   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.989120        0.993164        0.978096      453.174581  \n",
       "1            0.990406        0.994098        0.981217      437.798032  \n",
       "2            0.989560        0.992646        0.976722      437.227031  \n",
       "3            0.987819        0.992693        0.976976      438.053217  \n",
       "4            0.984984        0.992962        0.976671      436.140970  \n",
       "5            0.984887        0.992916        0.977475      435.893028  \n",
       "6            0.984014        0.993717        0.980268      437.232998  \n",
       "7            0.990211        0.992397        0.976828      436.384657  \n",
       "8            0.984210        0.993425        0.978471      435.891000  \n",
       "9            0.987767        0.992985        0.977500      437.152997  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
