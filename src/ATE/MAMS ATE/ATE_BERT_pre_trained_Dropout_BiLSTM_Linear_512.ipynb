{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_BiLSTM_Linear import ATE_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_pre_trained_dropout_bilstm_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_pre_trained_dropout_bilstm_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0524739027023315\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05075586214661598\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.023732533678412437\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.012878531590104103\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.01804565079510212\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.022823181003332138\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.015177041292190552\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.012283275835216045\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.011845088563859463\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.011085424572229385\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.010218852199614048\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.006267505697906017\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.004450907930731773\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.012675280682742596\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01570931263267994\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.004875104874372482\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.009359868243336678\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.015602030791342258\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.00276951864361763\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0073662991635501385\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0076770964078605175\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.003451436758041382\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0258015394210815\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.029458753764629364\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.019010081887245178\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.026621997356414795\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.01477204728871584\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.009174561128020287\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.027622945606708527\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.01637989655137062\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.028611138463020325\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.018122095614671707\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.018898040056228638\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01845734938979149\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.00758818956092\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.007923259399831295\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.014147389680147171\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.006650861818343401\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.00896221213042736\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.004937910940498114\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.016780750826001167\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.007604691199958324\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.016315925866365433\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.00792739912867546\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.9823005795478821\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04772821068763733\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04670845717191696\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.016011053696274757\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.020122675225138664\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02332482486963272\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.009617510251700878\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.015293771401047707\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.012134602293372154\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.005921065341681242\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.008090980350971222\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.008618702180683613\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.013626727275550365\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.010975205339491367\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.007930477149784565\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.005770382471382618\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.006743626669049263\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.014072148129343987\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.004638813901692629\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.004776866640895605\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.005583310034126043\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0040573906153440475\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0907145738601685\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0448945015668869\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.02185433730483055\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.011137743480503559\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.025496497750282288\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.013839175924658775\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.014338795095682144\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.005986051633954048\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.013011856935918331\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01748509518802166\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.015554741024971008\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02789953723549843\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.00964678917080164\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.006610089913010597\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.014578120782971382\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.011193716898560524\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.005217353813350201\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.004154901951551437\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.005572829861193895\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.007591418921947479\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.005392801016569138\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01147172786295414\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0348601341247559\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.048848707228899\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0302173662930727\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.014064946211874485\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.015600969083607197\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.018360218033194542\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.006287736818194389\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.009848641231656075\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.022860055789351463\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01362231932580471\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.00858396291732788\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013103066943585873\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.011735532432794571\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.007976546883583069\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.008773759938776493\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.01057452242821455\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.015035624615848064\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.009580876678228378\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0038201622664928436\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01618119701743126\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0028231956530362368\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.005703860893845558\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.157322645187378\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04371889680624008\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04892541095614433\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.01291572954505682\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.015701469033956528\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.012245888821780682\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.010679072700440884\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.025172511115670204\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0066506206057965755\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.006587284617125988\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.008723948150873184\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013557512313127518\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.012559457682073116\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.005004168953746557\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.013160486705601215\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.003756757127121091\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.003861275501549244\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01638858951628208\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.014691540971398354\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.009388340637087822\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.007741433568298817\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.006010475568473339\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2208443880081177\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.047178324311971664\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.025959927588701248\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03799204155802727\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.033752214163541794\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.013632292859256268\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.00976462010294199\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.014465028420090675\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.008703937754034996\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01632850058376789\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.008650506846606731\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.005320657044649124\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.00355168036185205\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.006870412267744541\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.011584886349737644\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0025933929719030857\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.021350763738155365\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.014913113787770271\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.011758356355130672\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.015438673086464405\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.004521080292761326\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.008817804977297783\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1666287183761597\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04496804252266884\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.037069231271743774\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03183051943778992\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.019225869327783585\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.014020930975675583\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.01158401183784008\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.017863769084215164\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.015523544512689114\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.010672209784388542\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.010550412349402905\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013340531848371029\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.012257668189704418\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.007746531628072262\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.013251116499304771\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.00768179539591074\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.014226582832634449\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.005041953641921282\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.010211153887212276\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.00943494588136673\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.013541329652071\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.004888914059847593\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1149154901504517\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0626545250415802\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.018232399597764015\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.021718282252550125\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.013201824389398098\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.021076997742056847\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.014685430563986301\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.010705587454140186\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.012005147524178028\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01143679954111576\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.00960561167448759\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01561211608350277\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02669733390212059\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.011718476191163063\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.010449898429214954\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.008478730916976929\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.002673044800758362\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.009517457336187363\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.007043927442282438\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0071108401753008366\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.016479283571243286\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01452533807605505\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1798455715179443\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05226011574268341\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.031570032238960266\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.02716733142733574\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0083234254270792\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02493463084101677\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.010781840421259403\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.013251597061753273\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0239246878772974\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.014107819646596909\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.011384434998035431\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01430082693696022\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.005396383814513683\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.011254220269620419\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0027539862785488367\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0105183320119977\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.003972810227423906\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.009072748012840748\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.007554551120847464\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.011076786555349827\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.009667452424764633\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0030354768969118595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, bilstm_in_features=256, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "\n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses) \n",
    "    plt.savefig(f'../../../results/ATE/MAMS/plots/bert_pt_do_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997848</td>\n",
       "      <td>0.997848</td>\n",
       "      <td>0.887405</td>\n",
       "      <td>0.997848</td>\n",
       "      <td>0.940465</td>\n",
       "      <td>0.997848</td>\n",
       "      <td>0.912232</td>\n",
       "      <td>1601.180877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997539</td>\n",
       "      <td>0.997539</td>\n",
       "      <td>0.860959</td>\n",
       "      <td>0.997539</td>\n",
       "      <td>0.943438</td>\n",
       "      <td>0.997539</td>\n",
       "      <td>0.899020</td>\n",
       "      <td>1592.038178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997065</td>\n",
       "      <td>0.997065</td>\n",
       "      <td>0.835078</td>\n",
       "      <td>0.997065</td>\n",
       "      <td>0.965402</td>\n",
       "      <td>0.997065</td>\n",
       "      <td>0.892418</td>\n",
       "      <td>1595.153557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.997009</td>\n",
       "      <td>0.997009</td>\n",
       "      <td>0.834805</td>\n",
       "      <td>0.997009</td>\n",
       "      <td>0.965219</td>\n",
       "      <td>0.997009</td>\n",
       "      <td>0.892176</td>\n",
       "      <td>1591.363011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.997593</td>\n",
       "      <td>0.997593</td>\n",
       "      <td>0.868987</td>\n",
       "      <td>0.997593</td>\n",
       "      <td>0.950577</td>\n",
       "      <td>0.997593</td>\n",
       "      <td>0.906680</td>\n",
       "      <td>1592.828417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.997545</td>\n",
       "      <td>0.997545</td>\n",
       "      <td>0.861201</td>\n",
       "      <td>0.997545</td>\n",
       "      <td>0.959760</td>\n",
       "      <td>0.997545</td>\n",
       "      <td>0.906134</td>\n",
       "      <td>1594.022329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.997051</td>\n",
       "      <td>0.997051</td>\n",
       "      <td>0.833029</td>\n",
       "      <td>0.997051</td>\n",
       "      <td>0.940074</td>\n",
       "      <td>0.997051</td>\n",
       "      <td>0.880897</td>\n",
       "      <td>1593.928940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.997360</td>\n",
       "      <td>0.997360</td>\n",
       "      <td>0.857951</td>\n",
       "      <td>0.997360</td>\n",
       "      <td>0.941569</td>\n",
       "      <td>0.997360</td>\n",
       "      <td>0.896601</td>\n",
       "      <td>1595.600686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.997324</td>\n",
       "      <td>0.997324</td>\n",
       "      <td>0.842108</td>\n",
       "      <td>0.997324</td>\n",
       "      <td>0.950373</td>\n",
       "      <td>0.997324</td>\n",
       "      <td>0.890170</td>\n",
       "      <td>1594.791980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.997305</td>\n",
       "      <td>0.997305</td>\n",
       "      <td>0.862776</td>\n",
       "      <td>0.997305</td>\n",
       "      <td>0.932647</td>\n",
       "      <td>0.997305</td>\n",
       "      <td>0.895509</td>\n",
       "      <td>1596.341058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.997848               0.997848               0.887405            0.997848   \n",
       "1  0.997539               0.997539               0.860959            0.997539   \n",
       "2  0.997065               0.997065               0.835078            0.997065   \n",
       "3  0.997009               0.997009               0.834805            0.997009   \n",
       "4  0.997593               0.997593               0.868987            0.997593   \n",
       "5  0.997545               0.997545               0.861201            0.997545   \n",
       "6  0.997051               0.997051               0.833029            0.997051   \n",
       "7  0.997360               0.997360               0.857951            0.997360   \n",
       "8  0.997324               0.997324               0.842108            0.997324   \n",
       "9  0.997305               0.997305               0.862776            0.997305   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.940465        0.997848        0.912232     1601.180877  \n",
       "1            0.943438        0.997539        0.899020     1592.038178  \n",
       "2            0.965402        0.997065        0.892418     1595.153557  \n",
       "3            0.965219        0.997009        0.892176     1591.363011  \n",
       "4            0.950577        0.997593        0.906680     1592.828417  \n",
       "5            0.959760        0.997545        0.906134     1594.022329  \n",
       "6            0.940074        0.997051        0.880897     1593.928940  \n",
       "7            0.941569        0.997360        0.896601     1595.600686  \n",
       "8            0.950373        0.997324        0.890170     1594.791980  \n",
       "9            0.932647        0.997305        0.895509     1596.341058  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
