{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_Linear import ATE_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/MAMS/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_fine_tuned_dropout_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_fine_tuned_dropout_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.8253220319747925\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.09373361617326736\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03874218836426735\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.06171668320894241\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.022044910117983818\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03532682731747627\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02525978349149227\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.040643732994794846\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.1437479853630066\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.046463221311569214\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0947369709610939\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01749487780034542\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.008459491655230522\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.013092623092234135\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.021999165415763855\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.014408806338906288\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.010691668838262558\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0033929275814443827\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.020120564848184586\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.00875220075249672\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.001420886255800724\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.004765243269503117\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3018518686294556\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06625039875507355\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.039360906928777695\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05417761206626892\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.018448499962687492\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.013856247998774052\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0602416954934597\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.11845317482948303\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.036353468894958496\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02877792902290821\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.025062190368771553\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02065221220254898\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.025951871648430824\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.022570831701159477\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.023754047229886055\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0013141062809154391\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.028849363327026367\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.011390019208192825\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0074479891918599606\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.05802009254693985\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0468110553920269\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.019582705572247505\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1493006944656372\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.041860081255435944\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.018506085500121117\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0323871411383152\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.029489362612366676\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0209696963429451\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.024536238983273506\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.047829002141952515\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.06914641708135605\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.04978269711136818\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.019629433751106262\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03267183154821396\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04134126380085945\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.015377243049442768\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0043899598531425\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.011353024281561375\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.022020133212208748\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.012651106342673302\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.006861672271043062\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.008012874983251095\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.05382414907217026\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.024415316060185432\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.9257681369781494\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.052076153457164764\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.00597963435575366\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.02761293761432171\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03935936465859413\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.06103223189711571\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.008647860027849674\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.06459292024374008\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.024769706651568413\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.016049236059188843\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.015972904860973358\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.022894814610481262\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.007028558757156134\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.011983782984316349\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.026127396151423454\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.08038841187953949\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.062213145196437836\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.015316587872803211\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.014672146178781986\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0008483736892230809\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.04415009543299675\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.028678862378001213\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.7893312573432922\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.1391998827457428\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03703594580292702\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03145043924450874\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.08911529928445816\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.07639939337968826\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.024358849972486496\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02574489638209343\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03969712182879448\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.023348795250058174\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.010112925432622433\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.017885414883494377\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.013241749256849289\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.018481384962797165\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.08598905056715012\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.06375180929899216\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.008701969869434834\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.013015997596085072\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.024128291755914688\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.04073275998234749\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.010763331316411495\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.017632372677326202\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.334572434425354\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04139217734336853\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03432620316743851\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03029673360288143\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.061029255390167236\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.013713940046727657\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03809050843119621\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.01454094797372818\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0445605032145977\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06226672604680061\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.05722521245479584\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.008241517469286919\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.05894049257040024\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.026440294459462166\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.00334421475417912\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.008560649119317532\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0021668821573257446\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.016555285081267357\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.04669390991330147\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.005486479960381985\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.006731611210852861\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02472522109746933\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.6141420602798462\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.07683505117893219\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.06620344519615173\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.021614160388708115\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.021966909989714622\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03949287161231041\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.06175200268626213\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.047778110951185226\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.047315020114183426\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.05750330165028572\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.046562984585762024\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06318919360637665\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.055151063948869705\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.06432695686817169\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.011650479398667812\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.024007629603147507\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.04869001358747482\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.014325796626508236\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0316762700676918\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.04975227266550064\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.005870991852134466\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.011999230831861496\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.9241389036178589\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.1399969905614853\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.050845298916101456\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.023865239694714546\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.013672945089638233\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.021358279511332512\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.05191965401172638\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.1843200922012329\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.025556473061442375\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.04853806272149086\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.023468749597668648\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.035013165324926376\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.014346837997436523\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04358772933483124\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.019811514765024185\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.015546531416475773\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0009009880595840514\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.041306041181087494\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.005929348058998585\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02781115658581257\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.08298608660697937\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.049463286995887756\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2717945575714111\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05520481616258621\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05628585070371628\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.08453387767076492\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07676713168621063\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0318840816617012\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.1570250242948532\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.014692113734781742\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.016534704715013504\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.07352108508348465\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.029600104317069054\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02942214347422123\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0070200213231146336\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03763028606772423\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.11362176388502121\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.016824647784233093\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.027284596115350723\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.003919567447155714\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.022729378193616867\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.017277933657169342\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.012109944596886635\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.04012014716863632\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.113491177558899\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05841140076518059\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.09142502397298813\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.009914737194776535\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03989610821008682\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.009752732701599598\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.011637412942945957\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.11869440227746964\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02132258750498295\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02242400124669075\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.008060003630816936\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013381448574364185\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01019351091235876\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.05236893147230148\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.00731982197612524\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.134765163064003\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02613324671983719\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0111531438305974\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03542977571487427\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.10115344077348709\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.008881189860403538\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.005029032938182354\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990167</td>\n",
       "      <td>0.990167</td>\n",
       "      <td>0.947379</td>\n",
       "      <td>0.990167</td>\n",
       "      <td>0.990025</td>\n",
       "      <td>0.990167</td>\n",
       "      <td>0.967871</td>\n",
       "      <td>413.557688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.994718</td>\n",
       "      <td>0.994718</td>\n",
       "      <td>0.975335</td>\n",
       "      <td>0.994718</td>\n",
       "      <td>0.991522</td>\n",
       "      <td>0.994718</td>\n",
       "      <td>0.983312</td>\n",
       "      <td>390.914779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.970064</td>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.987888</td>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>472.306999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.993822</td>\n",
       "      <td>0.993822</td>\n",
       "      <td>0.974714</td>\n",
       "      <td>0.993822</td>\n",
       "      <td>0.983585</td>\n",
       "      <td>0.993822</td>\n",
       "      <td>0.979100</td>\n",
       "      <td>436.486308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.994372</td>\n",
       "      <td>0.994372</td>\n",
       "      <td>0.977938</td>\n",
       "      <td>0.994372</td>\n",
       "      <td>0.987801</td>\n",
       "      <td>0.994372</td>\n",
       "      <td>0.982823</td>\n",
       "      <td>422.597274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.994319</td>\n",
       "      <td>0.994319</td>\n",
       "      <td>0.976768</td>\n",
       "      <td>0.994319</td>\n",
       "      <td>0.988026</td>\n",
       "      <td>0.994319</td>\n",
       "      <td>0.982335</td>\n",
       "      <td>420.170002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.992434</td>\n",
       "      <td>0.992434</td>\n",
       "      <td>0.957546</td>\n",
       "      <td>0.992434</td>\n",
       "      <td>0.987773</td>\n",
       "      <td>0.992434</td>\n",
       "      <td>0.972180</td>\n",
       "      <td>420.519029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.994098</td>\n",
       "      <td>0.994098</td>\n",
       "      <td>0.976370</td>\n",
       "      <td>0.994098</td>\n",
       "      <td>0.985933</td>\n",
       "      <td>0.994098</td>\n",
       "      <td>0.981079</td>\n",
       "      <td>420.135999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.994141</td>\n",
       "      <td>0.994141</td>\n",
       "      <td>0.972448</td>\n",
       "      <td>0.994141</td>\n",
       "      <td>0.989486</td>\n",
       "      <td>0.994141</td>\n",
       "      <td>0.980838</td>\n",
       "      <td>420.305002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.994776</td>\n",
       "      <td>0.994776</td>\n",
       "      <td>0.976898</td>\n",
       "      <td>0.994776</td>\n",
       "      <td>0.990802</td>\n",
       "      <td>0.994776</td>\n",
       "      <td>0.983762</td>\n",
       "      <td>423.089000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.990167               0.990167               0.947379            0.990167   \n",
       "1  0.994718               0.994718               0.975335            0.994718   \n",
       "2  0.993523               0.993523               0.970064            0.993523   \n",
       "3  0.993822               0.993822               0.974714            0.993822   \n",
       "4  0.994372               0.994372               0.977938            0.994372   \n",
       "5  0.994319               0.994319               0.976768            0.994319   \n",
       "6  0.992434               0.992434               0.957546            0.992434   \n",
       "7  0.994098               0.994098               0.976370            0.994098   \n",
       "8  0.994141               0.994141               0.972448            0.994141   \n",
       "9  0.994776               0.994776               0.976898            0.994776   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.990025        0.990167        0.967871      413.557688  \n",
       "1            0.991522        0.994718        0.983312      390.914779  \n",
       "2            0.987888        0.993523        0.978791      472.306999  \n",
       "3            0.983585        0.993822        0.979100      436.486308  \n",
       "4            0.987801        0.994372        0.982823      422.597274  \n",
       "5            0.988026        0.994319        0.982335      420.170002  \n",
       "6            0.987773        0.992434        0.972180      420.519029  \n",
       "7            0.985933        0.994098        0.981079      420.135999  \n",
       "8            0.989486        0.994141        0.980838      420.305002  \n",
       "9            0.990802        0.994776        0.983762      423.089000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
