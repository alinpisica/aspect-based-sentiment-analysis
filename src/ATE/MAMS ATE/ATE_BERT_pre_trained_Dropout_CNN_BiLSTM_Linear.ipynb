{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_CNN_BiLSTM_Linear import ATE_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_pre_trained_dropout_cnn_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_pre_trained_dropout_cnn_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1187759637832642\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04927690699696541\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.047461193054914474\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.028581561520695686\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.049163468182086945\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02915411815047264\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02281624637544155\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.037637416273355484\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.026918020099401474\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01651853695511818\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.024136990308761597\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03069862350821495\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.038711726665496826\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.024979515001177788\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.016242971643805504\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03551582992076874\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.019024142995476723\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.035541385412216187\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.020005611702799797\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03754341974854469\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.014347595162689686\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.015167620033025742\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1274285316467285\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04624859243631363\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03419871628284454\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.034160930663347244\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.02203398011624813\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03639815002679825\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.027950596064329147\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.030989278107881546\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.04174838587641716\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.023222414776682854\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.025081612169742584\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.024739665910601616\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03194790706038475\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.018041934818029404\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03462987765669823\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.02186179906129837\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.027711721137166023\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01964230090379715\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.013350975699722767\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0380324088037014\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02009309083223343\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.014693248085677624\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1580837965011597\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04719264805316925\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03148455172777176\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03341330587863922\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03440370410680771\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.022701703011989594\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.036133721470832825\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.01978996768593788\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.021938398480415344\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.04189087450504303\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.037069469690322876\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.026616599410772324\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04247425124049187\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01861460693180561\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03697711601853371\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.035327497869729996\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03201849386096001\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.015896014869213104\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.023458676412701607\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.020093943923711777\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.012069622054696083\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02872963808476925\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1403683423995972\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.11246760189533234\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04842368885874748\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.055803559720516205\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.031067920848727226\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03026951476931572\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03220672160387039\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.032430507242679596\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03163095563650131\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.039984993636608124\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03194186091423035\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03377261012792587\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.046649932861328125\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.023780684918165207\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.027617178857326508\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.046277180314064026\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02727774903178215\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.028848698362708092\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03378510847687721\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.026397591456770897\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.04419640079140663\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.03924061730504036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0628331899642944\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05161100625991821\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.058933358639478683\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03573499247431755\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.023753123357892036\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.042205315083265305\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.022515788674354553\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.01568017341196537\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.025573570281267166\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.020349258556962013\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03986644744873047\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.015076952055096626\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.051048409193754196\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.028122583404183388\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03489614650607109\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.023066947236657143\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02165108732879162\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.016471901908516884\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02172880247235298\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.029433073475956917\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.023484323173761368\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0252093356102705\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0329887866973877\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0577920638024807\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.026032572612166405\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03247825801372528\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.031446509063243866\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03290824964642525\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.023671982809901237\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03241937980055809\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.055961549282073975\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.017470434308052063\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.019143622368574142\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.031577564775943756\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0267031267285347\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.038743846118450165\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.018555615097284317\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.036112505942583084\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.018325023353099823\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.021928006783127785\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.034418340772390366\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.017531953752040863\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02258707582950592\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.03568766266107559\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0796483755111694\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05245489254593849\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.029893595725297928\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.031667113304138184\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.05188647657632828\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03820591792464256\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.04280373454093933\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.015059917233884335\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02914181724190712\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03510713204741478\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02519015409052372\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.029387060552835464\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.022028082981705666\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.025675738230347633\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.029129143804311752\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.019757457077503204\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02904059737920761\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.029354004189372063\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03007485903799534\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.025446755811572075\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02182522788643837\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.015037345699965954\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1281377077102661\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06878268718719482\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03705130144953728\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.042086441069841385\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0301479771733284\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03143605589866638\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0333477221429348\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.020739082247018814\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02164304442703724\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.04097990691661835\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.018085885792970657\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02339123748242855\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03167567402124405\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04275394603610039\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.020066918805241585\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0525350496172905\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.024989105761051178\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.041292984038591385\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.030813254415988922\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.037029363214969635\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.012526151724159718\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02320123091340065\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0665652751922607\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05068066343665123\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05426132678985596\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04353340342640877\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.034617096185684204\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.042814917862415314\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.026998888701200485\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.019836533814668655\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.06049622222781181\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.013977834954857826\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.016942333430051804\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.04592154547572136\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.016715548932552338\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.029321936890482903\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.023626696318387985\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.01897246390581131\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01772979460656643\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02507040649652481\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0503336526453495\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.028503388166427612\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.028833039104938507\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01449187844991684\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0893787145614624\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0670241191983223\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03986285999417305\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.025129025802016258\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.024909909814596176\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03336478769779205\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.020287269726395607\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0252523273229599\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.027101939544081688\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.029256438836455345\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.022436952218413353\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02443351037800312\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03928171098232269\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.020458418875932693\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.028353648260235786\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.017928587272763252\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03198680281639099\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.022298088297247887\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01627778820693493\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.015218419954180717\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.01736758090555668\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.021306117996573448\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_CNN_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992223</td>\n",
       "      <td>0.992223</td>\n",
       "      <td>0.709229</td>\n",
       "      <td>0.992223</td>\n",
       "      <td>0.540496</td>\n",
       "      <td>0.992223</td>\n",
       "      <td>0.599163</td>\n",
       "      <td>1627.496119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.991759</td>\n",
       "      <td>0.991759</td>\n",
       "      <td>0.686655</td>\n",
       "      <td>0.991759</td>\n",
       "      <td>0.477761</td>\n",
       "      <td>0.991759</td>\n",
       "      <td>0.531759</td>\n",
       "      <td>1620.617624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.992184</td>\n",
       "      <td>0.992184</td>\n",
       "      <td>0.699318</td>\n",
       "      <td>0.992184</td>\n",
       "      <td>0.521614</td>\n",
       "      <td>0.992184</td>\n",
       "      <td>0.567823</td>\n",
       "      <td>1618.982137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.991489</td>\n",
       "      <td>0.991489</td>\n",
       "      <td>0.516394</td>\n",
       "      <td>0.991489</td>\n",
       "      <td>0.398966</td>\n",
       "      <td>0.991489</td>\n",
       "      <td>0.429169</td>\n",
       "      <td>1633.617983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.991837</td>\n",
       "      <td>0.991837</td>\n",
       "      <td>0.689122</td>\n",
       "      <td>0.991837</td>\n",
       "      <td>0.572642</td>\n",
       "      <td>0.991837</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>1631.411775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.992463</td>\n",
       "      <td>0.992463</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.992463</td>\n",
       "      <td>0.569064</td>\n",
       "      <td>0.992463</td>\n",
       "      <td>0.630121</td>\n",
       "      <td>1618.451174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.992406</td>\n",
       "      <td>0.992406</td>\n",
       "      <td>0.723861</td>\n",
       "      <td>0.992406</td>\n",
       "      <td>0.539544</td>\n",
       "      <td>0.992406</td>\n",
       "      <td>0.602857</td>\n",
       "      <td>1620.453216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.992491</td>\n",
       "      <td>0.992491</td>\n",
       "      <td>0.723568</td>\n",
       "      <td>0.992491</td>\n",
       "      <td>0.534491</td>\n",
       "      <td>0.992491</td>\n",
       "      <td>0.593831</td>\n",
       "      <td>1644.169609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.991957</td>\n",
       "      <td>0.991957</td>\n",
       "      <td>0.668212</td>\n",
       "      <td>0.991957</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.991957</td>\n",
       "      <td>0.566107</td>\n",
       "      <td>1660.909867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.992566</td>\n",
       "      <td>0.992566</td>\n",
       "      <td>0.729987</td>\n",
       "      <td>0.992566</td>\n",
       "      <td>0.564230</td>\n",
       "      <td>0.992566</td>\n",
       "      <td>0.619273</td>\n",
       "      <td>1623.852852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.992223               0.992223               0.709229            0.992223   \n",
       "1  0.991759               0.991759               0.686655            0.991759   \n",
       "2  0.992184               0.992184               0.699318            0.992184   \n",
       "3  0.991489               0.991489               0.516394            0.991489   \n",
       "4  0.991837               0.991837               0.689122            0.991837   \n",
       "5  0.992463               0.992463               0.740964            0.992463   \n",
       "6  0.992406               0.992406               0.723861            0.992406   \n",
       "7  0.992491               0.992491               0.723568            0.992491   \n",
       "8  0.991957               0.991957               0.668212            0.991957   \n",
       "9  0.992566               0.992566               0.729987            0.992566   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.540496        0.992223        0.599163     1627.496119  \n",
       "1            0.477761        0.991759        0.531759     1620.617624  \n",
       "2            0.521614        0.992184        0.567823     1618.982137  \n",
       "3            0.398966        0.991489        0.429169     1633.617983  \n",
       "4            0.572642        0.991837        0.618884     1631.411775  \n",
       "5            0.569064        0.992463        0.630121     1618.451174  \n",
       "6            0.539544        0.992406        0.602857     1620.453216  \n",
       "7            0.534491        0.992491        0.593831     1644.169609  \n",
       "8            0.515038        0.991957        0.566107     1660.909867  \n",
       "9            0.564230        0.992566        0.619273     1623.852852  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
