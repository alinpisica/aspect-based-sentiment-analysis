{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_CNN_BiLSTM_Linear import ATE_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/MAMS/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_fine_tuned_dropout_cnn_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_fine_tuned_dropout_cnn_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0499008893966675\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0550273135304451\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03775915503501892\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03741847723722458\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.025592327117919922\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0372077152132988\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.04022283852100372\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02150978147983551\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03554372861981392\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02356659062206745\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.01638416387140751\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.021919500082731247\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.026266729459166527\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.013398619368672371\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.026283428072929382\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.013314185664057732\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.013125890865921974\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.018419701606035233\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.010770386084914207\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02692919410765171\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.028690524399280548\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.009575740434229374\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0552995204925537\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03781562298536301\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03699084743857384\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.020865077152848244\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.027390362694859505\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02532236836850643\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.022519012913107872\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.025343790650367737\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.024325458332896233\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.030012337490916252\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.027951929718255997\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03116106055676937\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.023910801857709885\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.017984692007303238\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.015393665060400963\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03352827951312065\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03091815672814846\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01771855168044567\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03197925165295601\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.011603601276874542\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.010532962158322334\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.013846667483448982\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1580936908721924\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05395352840423584\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04847755655646324\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.030416514724493027\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03232385963201523\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03456244617700577\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.043147165328264236\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.029774988070130348\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.029768694192171097\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.04030065983533859\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.021983366459608078\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.022512545809149742\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01823100447654724\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0180341899394989\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.026436341926455498\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.034248434007167816\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.012824476696550846\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02591259777545929\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01952151395380497\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01365877129137516\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.021911274641752243\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.014804184436798096\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0964469909667969\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.10952895134687424\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04172636196017265\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03686511889100075\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0321115106344223\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04115700349211693\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.050813980400562286\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02378498949110508\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.031055498868227005\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03150751814246178\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.053040359169244766\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.030686164274811745\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02747291699051857\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02615407295525074\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.024218210950493813\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.016060054302215576\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02306470461189747\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01356258150190115\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.021463792771100998\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.023246923461556435\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.011963468044996262\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.015849772840738297\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.137587070465088\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.049657125025987625\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.035347480326890945\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03952901065349579\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.05526164546608925\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02442152239382267\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.024358443915843964\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03131988272070885\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.017767444252967834\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.012992804870009422\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.030449794605374336\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02118087187409401\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02228626236319542\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03196600824594498\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04573537036776543\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.02212020568549633\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01757277175784111\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.013406090438365936\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.016091329976916313\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01824882999062538\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03074638359248638\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.006400418467819691\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0900849103927612\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05272547900676727\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04000786691904068\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05025000125169754\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.029623640701174736\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03115292452275753\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.019043616950511932\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.020603371784090996\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03226974233984947\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.019926052540540695\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.019652655348181725\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.027571041136980057\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01850796304643154\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02388608828186989\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.011746428906917572\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.01874541863799095\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.011682874523103237\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.016631295904517174\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02817249670624733\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.012279108166694641\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.018132859840989113\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.017870619893074036\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1349868774414062\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.07217497378587723\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.06136472523212433\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03799295425415039\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.022710153833031654\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02441364713013172\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.022577103227376938\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.032411783933639526\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.015107429586350918\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.023535260930657387\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.04074724763631821\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.019631126895546913\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.015938589349389076\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.019326671957969666\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.014023398980498314\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03579246997833252\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.009874528273940086\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01705015078186989\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.011135723441839218\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.018253060057759285\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.022486455738544464\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01650332659482956\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.119272232055664\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04808728024363518\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.053747426718473434\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.023601168766617775\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.02537703514099121\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02209659107029438\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03349052742123604\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02872237004339695\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02568928338587284\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02674485556781292\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02544606290757656\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.018671374768018723\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.021060144528746605\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.010733372531831264\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.016884496435523033\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.017631005495786667\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.015312054194509983\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.018380243331193924\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.008897512219846249\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.021347304806113243\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.025616182014346123\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.013948065228760242\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0479521751403809\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04192035645246506\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.033761125057935715\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0248095765709877\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03462691977620125\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02852725237607956\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03016182780265808\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.020831068977713585\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.024990234524011612\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06775149703025818\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02036597952246666\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.020971914753317833\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.030967218801379204\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.017097966745495796\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0065240319818258286\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.022188492119312286\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.013898356817662716\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.010583191178739071\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0174707043915987\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.012593913823366165\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.019665434956550598\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01275684218853712\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1351113319396973\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06669825315475464\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.026497438549995422\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.027315257117152214\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03288079425692558\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03090555965900421\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02741142362356186\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.020939098671078682\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.022673245519399643\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.028331544250249863\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.022045070305466652\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01638607494533062\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.022304659709334373\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.020085813477635384\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03196234628558159\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.014722219668328762\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.028808051720261574\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.013778076507151127\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.021096954122185707\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.022124459967017174\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.021787699311971664\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.018538666889071465\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_CNN_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993868</td>\n",
       "      <td>0.993868</td>\n",
       "      <td>0.770040</td>\n",
       "      <td>0.993868</td>\n",
       "      <td>0.683595</td>\n",
       "      <td>0.993868</td>\n",
       "      <td>0.714426</td>\n",
       "      <td>1596.004598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993867</td>\n",
       "      <td>0.993867</td>\n",
       "      <td>0.757508</td>\n",
       "      <td>0.993867</td>\n",
       "      <td>0.725182</td>\n",
       "      <td>0.993867</td>\n",
       "      <td>0.738948</td>\n",
       "      <td>1598.047123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.994104</td>\n",
       "      <td>0.994104</td>\n",
       "      <td>0.758125</td>\n",
       "      <td>0.994104</td>\n",
       "      <td>0.700888</td>\n",
       "      <td>0.994104</td>\n",
       "      <td>0.722959</td>\n",
       "      <td>1722.910858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992953</td>\n",
       "      <td>0.992953</td>\n",
       "      <td>0.742513</td>\n",
       "      <td>0.992953</td>\n",
       "      <td>0.548345</td>\n",
       "      <td>0.992953</td>\n",
       "      <td>0.610654</td>\n",
       "      <td>1704.150993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.994153</td>\n",
       "      <td>0.994153</td>\n",
       "      <td>0.767556</td>\n",
       "      <td>0.994153</td>\n",
       "      <td>0.661301</td>\n",
       "      <td>0.994153</td>\n",
       "      <td>0.699044</td>\n",
       "      <td>1594.642217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.993916</td>\n",
       "      <td>0.993916</td>\n",
       "      <td>0.754042</td>\n",
       "      <td>0.993916</td>\n",
       "      <td>0.678131</td>\n",
       "      <td>0.993916</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>1596.215966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.993899</td>\n",
       "      <td>0.993899</td>\n",
       "      <td>0.748422</td>\n",
       "      <td>0.993899</td>\n",
       "      <td>0.676678</td>\n",
       "      <td>0.993899</td>\n",
       "      <td>0.702480</td>\n",
       "      <td>1596.097431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.994521</td>\n",
       "      <td>0.994521</td>\n",
       "      <td>0.762338</td>\n",
       "      <td>0.994521</td>\n",
       "      <td>0.745146</td>\n",
       "      <td>0.994521</td>\n",
       "      <td>0.752070</td>\n",
       "      <td>1595.511680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.994687</td>\n",
       "      <td>0.994687</td>\n",
       "      <td>0.775697</td>\n",
       "      <td>0.994687</td>\n",
       "      <td>0.718987</td>\n",
       "      <td>0.994687</td>\n",
       "      <td>0.742684</td>\n",
       "      <td>1595.650192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.994630</td>\n",
       "      <td>0.994630</td>\n",
       "      <td>0.777230</td>\n",
       "      <td>0.994630</td>\n",
       "      <td>0.714082</td>\n",
       "      <td>0.994630</td>\n",
       "      <td>0.737727</td>\n",
       "      <td>1597.130659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.993868               0.993868               0.770040            0.993868   \n",
       "1  0.993867               0.993867               0.757508            0.993867   \n",
       "2  0.994104               0.994104               0.758125            0.994104   \n",
       "3  0.992953               0.992953               0.742513            0.992953   \n",
       "4  0.994153               0.994153               0.767556            0.994153   \n",
       "5  0.993916               0.993916               0.754042            0.993916   \n",
       "6  0.993899               0.993899               0.748422            0.993899   \n",
       "7  0.994521               0.994521               0.762338            0.994521   \n",
       "8  0.994687               0.994687               0.775697            0.994687   \n",
       "9  0.994630               0.994630               0.777230            0.994630   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.683595        0.993868        0.714426     1596.004598  \n",
       "1            0.725182        0.993867        0.738948     1598.047123  \n",
       "2            0.700888        0.994104        0.722959     1722.910858  \n",
       "3            0.548345        0.992953        0.610654     1704.150993  \n",
       "4            0.661301        0.994153        0.699044     1594.642217  \n",
       "5            0.678131        0.993916        0.697000     1596.215966  \n",
       "6            0.676678        0.993899        0.702480     1596.097431  \n",
       "7            0.745146        0.994521        0.752070     1595.511680  \n",
       "8            0.718987        0.994687        0.742684     1595.650192  \n",
       "9            0.714082        0.994630        0.737727     1597.130659  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
