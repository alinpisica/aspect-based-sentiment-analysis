{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_BiLSTM_Linear import ATE_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_pre_trained_dropout_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_pre_trained_dropout_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1357122659683228\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.18687956035137177\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.10554485023021698\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.18656128644943237\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.12772810459136963\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.1410929411649704\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.13740988075733185\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.05944595858454704\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.08070128411054611\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06693942844867706\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.07521174103021622\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.041045863181352615\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.06760697066783905\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.05451666936278343\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.10553969442844391\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.10842005163431168\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.1533718854188919\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.08868544548749924\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.05491261184215546\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.11114295572042465\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.09249994903802872\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.10309505462646484\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.008958339691162\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.2415691316127777\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.25858137011528015\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.2199377417564392\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.28152090311050415\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.07650063186883926\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.20824500918388367\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.04829518869519234\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.3862416446208954\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.08096678555011749\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.15148867666721344\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06839853525161743\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.06524302065372467\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0454988107085228\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.3728533685207367\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.06720224022865295\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0918378010392189\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.07407919317483902\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.022715561091899872\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.1763990819454193\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.058673687279224396\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.039188891649246216\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1546995639801025\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.3104153275489807\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.15267227590084076\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.195752814412117\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.1967812031507492\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.061986617743968964\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.21252772212028503\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.22409850358963013\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.09977170079946518\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06696642190217972\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.04187707602977753\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.05465642362833023\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.08127149194478989\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0776074230670929\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.16745227575302124\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.039376236498355865\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0582999587059021\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0623638853430748\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03337698429822922\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.046510860323905945\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.14732377231121063\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02922607585787773\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0406800508499146\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.4296680688858032\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.2108990103006363\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.2827717065811157\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.12927891314029694\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.1180882453918457\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0823097825050354\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.22427679598331451\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.08955549448728561\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.15861494839191437\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.2008751928806305\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06854671239852905\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04959110915660858\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.12504781782627106\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.07171949744224548\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.042639654129743576\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.07094761729240417\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.09051202237606049\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.04159218445420265\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.07880671322345734\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.12036581337451935\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.07337608188390732\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0321217775344849\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.26788270473480225\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.2932133376598358\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.1704796999692917\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.10783814638853073\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.18794700503349304\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.1974681317806244\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.10839103162288666\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.2780778706073761\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.10331985354423523\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.09643066674470901\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.16281893849372864\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.08954951167106628\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0905657708644867\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.1406133621931076\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.13265271484851837\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03483925759792328\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.09425894916057587\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.040890973061323166\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.08813934773206711\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.04752438887953758\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.033612947911024094\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2465280294418335\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.22373928129673004\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.21285036206245422\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.190577432513237\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.11987380683422089\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.05018644779920578\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.16337472200393677\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.07948042452335358\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.08331036567687988\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.09596916288137436\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.08025351911783218\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.056654032319784164\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.07563389837741852\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.11983679980039597\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.08117224276065826\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.06571915000677109\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.061977460980415344\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.06584744900465012\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.06074642390012741\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03745780140161514\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.04201546683907509\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.009769352152943611\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1265990734100342\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.19069884717464447\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.11062875390052795\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.15863776206970215\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.18564744293689728\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.08693619072437286\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.07923782616853714\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.08821376413106918\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.2909884750843048\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.13187463581562042\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.04856177046895027\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.053604237735271454\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.13275361061096191\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04846563562750816\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0890006572008133\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.1250353753566742\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0663745328783989\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.07428581267595291\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.081815704703331\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.05331398546695709\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.1115010529756546\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0474267341196537\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1069415807724\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.19350086152553558\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.1624090075492859\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.17628206312656403\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.22164161503314972\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.24514658749103546\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.28345179557800293\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.18028831481933594\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.1805357187986374\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.1837945282459259\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.07685951143503189\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.05608604475855827\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.031473007053136826\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.07763554900884628\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.09414298832416534\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04272058233618736\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.07729280740022659\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.06488260626792908\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.10400937497615814\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.09457021206617355\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0928574651479721\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.07696680724620819\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0852336883544922\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.3226812481880188\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.24619853496551514\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.25900939106941223\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.09604416787624359\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.10036459565162659\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0757933109998703\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.09357118606567383\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.06678995490074158\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.09116611629724503\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.07354797422885895\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.07382865250110626\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.11283032596111298\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.12104324251413345\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.11113641411066055\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.10361117869615555\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.054058775305747986\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.07767431437969208\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.09764310717582703\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02262895740568638\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.06452547013759613\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.03982015699148178\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0716605186462402\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.17380516231060028\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.10215987265110016\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.08107047528028488\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.15066471695899963\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.08546615391969681\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.1519201546907425\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.14315445721149445\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.09758368134498596\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06082158163189888\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.14131097495555878\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.11223345994949341\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.17615586519241333\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.14900356531143188\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.05345061793923378\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.05767551064491272\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03941400349140167\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.09994682669639587\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03393641859292984\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.048854853957891464\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03952435776591301\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.062106579542160034\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, bilstm_in_features=256, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.976680</td>\n",
       "      <td>0.976680</td>\n",
       "      <td>0.893152</td>\n",
       "      <td>0.976680</td>\n",
       "      <td>0.947689</td>\n",
       "      <td>0.976680</td>\n",
       "      <td>0.918603</td>\n",
       "      <td>456.284379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.975805</td>\n",
       "      <td>0.975805</td>\n",
       "      <td>0.893461</td>\n",
       "      <td>0.975805</td>\n",
       "      <td>0.943732</td>\n",
       "      <td>0.975805</td>\n",
       "      <td>0.917128</td>\n",
       "      <td>451.303217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.880407</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.950594</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.912929</td>\n",
       "      <td>449.068943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.971390</td>\n",
       "      <td>0.971390</td>\n",
       "      <td>0.852040</td>\n",
       "      <td>0.971390</td>\n",
       "      <td>0.963783</td>\n",
       "      <td>0.971390</td>\n",
       "      <td>0.900864</td>\n",
       "      <td>504.683350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.975361</td>\n",
       "      <td>0.975361</td>\n",
       "      <td>0.882827</td>\n",
       "      <td>0.975361</td>\n",
       "      <td>0.956685</td>\n",
       "      <td>0.975361</td>\n",
       "      <td>0.917018</td>\n",
       "      <td>474.923028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.976063</td>\n",
       "      <td>0.976063</td>\n",
       "      <td>0.888360</td>\n",
       "      <td>0.976063</td>\n",
       "      <td>0.959523</td>\n",
       "      <td>0.976063</td>\n",
       "      <td>0.921390</td>\n",
       "      <td>485.474458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.977830</td>\n",
       "      <td>0.977830</td>\n",
       "      <td>0.897367</td>\n",
       "      <td>0.977830</td>\n",
       "      <td>0.954488</td>\n",
       "      <td>0.977830</td>\n",
       "      <td>0.924319</td>\n",
       "      <td>456.192230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.975614</td>\n",
       "      <td>0.975614</td>\n",
       "      <td>0.886540</td>\n",
       "      <td>0.975614</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.975614</td>\n",
       "      <td>0.920570</td>\n",
       "      <td>451.466935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.978342</td>\n",
       "      <td>0.978342</td>\n",
       "      <td>0.913791</td>\n",
       "      <td>0.978342</td>\n",
       "      <td>0.943148</td>\n",
       "      <td>0.978342</td>\n",
       "      <td>0.927930</td>\n",
       "      <td>454.542513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.979342</td>\n",
       "      <td>0.979342</td>\n",
       "      <td>0.909614</td>\n",
       "      <td>0.979342</td>\n",
       "      <td>0.947326</td>\n",
       "      <td>0.979342</td>\n",
       "      <td>0.927710</td>\n",
       "      <td>433.690682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.976680               0.976680               0.893152            0.976680   \n",
       "1  0.975805               0.975805               0.893461            0.975805   \n",
       "2  0.974769               0.974769               0.880407            0.974769   \n",
       "3  0.971390               0.971390               0.852040            0.971390   \n",
       "4  0.975361               0.975361               0.882827            0.975361   \n",
       "5  0.976063               0.976063               0.888360            0.976063   \n",
       "6  0.977830               0.977830               0.897367            0.977830   \n",
       "7  0.975614               0.975614               0.886540            0.975614   \n",
       "8  0.978342               0.978342               0.913791            0.978342   \n",
       "9  0.979342               0.979342               0.909614            0.979342   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.947689        0.976680        0.918603      456.284379  \n",
       "1            0.943732        0.975805        0.917128      451.303217  \n",
       "2            0.950594        0.974769        0.912929      449.068943  \n",
       "3            0.963783        0.971390        0.900864      504.683350  \n",
       "4            0.956685        0.975361        0.917018      474.923028  \n",
       "5            0.959523        0.976063        0.921390      485.474458  \n",
       "6            0.954488        0.977830        0.924319      456.192230  \n",
       "7            0.960100        0.975614        0.920570      451.466935  \n",
       "8            0.943148        0.978342        0.927930      454.542513  \n",
       "9            0.947326        0.979342        0.927710      433.690682  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
