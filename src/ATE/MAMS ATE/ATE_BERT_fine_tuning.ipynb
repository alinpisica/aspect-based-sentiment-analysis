{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_Linear import ATE_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_MODEL_PATH = '../../../results/ATE/MAMS/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_pre_trained_dropout_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_pre_trained_dropout_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1373381614685059\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.1962832510471344\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.11865421384572983\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.11552409827709198\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.15178672969341278\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.08002081513404846\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.10678910464048386\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.04966593533754349\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.033130988478660583\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.14423935115337372\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.060405611991882324\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.04950731620192528\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.13368351757526398\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.07373274117708206\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0949251651763916\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.06942903250455856\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.041336704045534134\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.06517510116100311\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.13488639891147614\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.038649220019578934\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.025470424443483353\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.12016570568084717\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.9861470460891724\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.145374596118927\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.24376532435417175\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.11324480175971985\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.1449729949235916\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.10464540123939514\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.1051965206861496\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.14755775034427643\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.06046386808156967\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03727802634239197\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.06880731880664825\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.08147300034761429\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.190094992518425\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.24619023501873016\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.08868943154811859\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.11536215245723724\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.08055492490530014\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.05125429108738899\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02226637303829193\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.05013120174407959\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.051039788872003555\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.04535235837101936\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.053313970565796\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.11147148162126541\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.18902887403964996\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.13040579855442047\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.20283262431621552\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.22009392082691193\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.24378377199172974\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0724760890007019\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.17810900509357452\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.1577511876821518\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.11764781922101974\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.1864679753780365\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.14117592573165894\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04490648955106735\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.10306631773710251\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.07593471556901932\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.04459518939256668\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02045680582523346\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03936704620718956\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.04707006365060806\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03461409732699394\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.03677026554942131\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.097551941871643\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.13205423951148987\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.141730397939682\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.14873459935188293\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.09967935085296631\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.11871971189975739\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.08988768607378006\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.2099861353635788\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.1387353241443634\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.041201818734407425\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.027949711307883263\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.025079036131501198\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.05205467343330383\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.07026199251413345\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0766783356666565\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.05423854663968086\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.043298617005348206\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.034566767513751984\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.06571637094020844\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.061727654188871384\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.04270246997475624\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.12917979061603546\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2868958711624146\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.22270984947681427\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.16893887519836426\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.1574217975139618\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.08974970132112503\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.07557601481676102\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.17451687157154083\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.07081982493400574\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.2138475775718689\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03706006333231926\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.13935165107250214\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.07973399013280869\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.09709243476390839\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.045911673456430435\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.11819925159215927\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03956737369298935\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03244057670235634\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02951645851135254\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.10805193334817886\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02261810563504696\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03420189023017883\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.048615481704473495\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2355812788009644\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.22148571908473969\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.12738466262817383\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.07714395970106125\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.08178060501813889\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.09029635787010193\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.08015456795692444\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.042065948247909546\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.19200997054576874\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06743170320987701\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.10931731015443802\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06986686587333679\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0393679179251194\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.1989845186471939\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.059071265161037445\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.2720172703266144\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.053616590797901154\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.10222682356834412\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03222274407744408\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.030484655871987343\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.08472234755754471\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.044252071529626846\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0957540273666382\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.15037880837917328\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.23211555182933807\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.13992317020893097\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.17471085488796234\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0703071802854538\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.17732782661914825\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0973670706152916\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.06281960755586624\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0747842937707901\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.09590646624565125\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06602919846773148\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04315224662423134\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.06947290897369385\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.022451944649219513\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.041810717433691025\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.06889607757329941\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.03723480924963951\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03279389068484306\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.023197894915938377\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.06922738254070282\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.08126456290483475\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2769993543624878\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.20161977410316467\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.10122455656528473\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.24895533919334412\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.08880243450403214\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0481221042573452\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.08079945296049118\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.1924334168434143\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.15077002346515656\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.12578679621219635\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.10126523673534393\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.09018348902463913\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04215497896075249\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.05261671543121338\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.036663033068180084\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.19244790077209473\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.10320160537958145\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.09656449407339096\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.053135212510824203\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02570922300219536\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03362058103084564\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.031703244894742966\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2813127040863037\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.2819730043411255\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.12413974106311798\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.15487129986286163\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.14364677667617798\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.09049388021230698\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.09642654657363892\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.15176448225975037\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.04790983721613884\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.10755563527345657\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.056624315679073334\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.10152869671583176\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.1356676071882248\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.035027120262384415\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0547918826341629\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.17600159347057343\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02628488466143608\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.028278883546590805\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.025672568008303642\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.06224285811185837\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.12694323062896729\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.11831308901309967\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0875155925750732\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.2996404469013214\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.1655236780643463\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.11024142056703568\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07681483775377274\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.2894136905670166\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.07240835577249527\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.07339347153902054\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.030494460836052895\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.04252785071730614\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0605604313313961\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.09313557296991348\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.10450424253940582\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0871400386095047\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04068363085389137\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.06892711669206619\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.036040306091308594\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.1253620982170105\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.04982632398605347\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.04993395879864693\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.09265083074569702\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.033334676176309586\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, BERT_PRETRAINED_MODEL_PATH)\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.915922</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.954241</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.934343</td>\n",
       "      <td>417.770709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.976904</td>\n",
       "      <td>0.976904</td>\n",
       "      <td>0.900406</td>\n",
       "      <td>0.976904</td>\n",
       "      <td>0.958465</td>\n",
       "      <td>0.976904</td>\n",
       "      <td>0.927681</td>\n",
       "      <td>413.002854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.975107</td>\n",
       "      <td>0.975107</td>\n",
       "      <td>0.874659</td>\n",
       "      <td>0.975107</td>\n",
       "      <td>0.964778</td>\n",
       "      <td>0.975107</td>\n",
       "      <td>0.915313</td>\n",
       "      <td>412.978957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.981497</td>\n",
       "      <td>0.981497</td>\n",
       "      <td>0.926462</td>\n",
       "      <td>0.981497</td>\n",
       "      <td>0.954353</td>\n",
       "      <td>0.981497</td>\n",
       "      <td>0.939901</td>\n",
       "      <td>413.968614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.980567</td>\n",
       "      <td>0.980567</td>\n",
       "      <td>0.924952</td>\n",
       "      <td>0.980567</td>\n",
       "      <td>0.951071</td>\n",
       "      <td>0.980567</td>\n",
       "      <td>0.937631</td>\n",
       "      <td>413.786613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.977515</td>\n",
       "      <td>0.977515</td>\n",
       "      <td>0.895984</td>\n",
       "      <td>0.977515</td>\n",
       "      <td>0.965011</td>\n",
       "      <td>0.977515</td>\n",
       "      <td>0.928111</td>\n",
       "      <td>412.339390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.979080</td>\n",
       "      <td>0.979080</td>\n",
       "      <td>0.916051</td>\n",
       "      <td>0.979080</td>\n",
       "      <td>0.949858</td>\n",
       "      <td>0.979080</td>\n",
       "      <td>0.932284</td>\n",
       "      <td>412.870199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.977157</td>\n",
       "      <td>0.977157</td>\n",
       "      <td>0.897273</td>\n",
       "      <td>0.977157</td>\n",
       "      <td>0.961228</td>\n",
       "      <td>0.977157</td>\n",
       "      <td>0.927172</td>\n",
       "      <td>412.440044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.978634</td>\n",
       "      <td>0.978634</td>\n",
       "      <td>0.911932</td>\n",
       "      <td>0.978634</td>\n",
       "      <td>0.948086</td>\n",
       "      <td>0.978634</td>\n",
       "      <td>0.929318</td>\n",
       "      <td>413.235717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.979797</td>\n",
       "      <td>0.979797</td>\n",
       "      <td>0.918235</td>\n",
       "      <td>0.979797</td>\n",
       "      <td>0.950002</td>\n",
       "      <td>0.979797</td>\n",
       "      <td>0.933231</td>\n",
       "      <td>414.098797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.979750               0.979750               0.915922            0.979750   \n",
       "1  0.976904               0.976904               0.900406            0.976904   \n",
       "2  0.975107               0.975107               0.874659            0.975107   \n",
       "3  0.981497               0.981497               0.926462            0.981497   \n",
       "4  0.980567               0.980567               0.924952            0.980567   \n",
       "5  0.977515               0.977515               0.895984            0.977515   \n",
       "6  0.979080               0.979080               0.916051            0.979080   \n",
       "7  0.977157               0.977157               0.897273            0.977157   \n",
       "8  0.978634               0.978634               0.911932            0.978634   \n",
       "9  0.979797               0.979797               0.918235            0.979797   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.954241        0.979750        0.934343      417.770709  \n",
       "1            0.958465        0.976904        0.927681      413.002854  \n",
       "2            0.964778        0.975107        0.915313      412.978957  \n",
       "3            0.954353        0.981497        0.939901      413.968614  \n",
       "4            0.951071        0.980567        0.937631      413.786613  \n",
       "5            0.965011        0.977515        0.928111      412.339390  \n",
       "6            0.949858        0.979080        0.932284      412.870199  \n",
       "7            0.961228        0.977157        0.927172      412.440044  \n",
       "8            0.948086        0.978634        0.929318      413.235717  \n",
       "9            0.950002        0.979797        0.933231      414.098797  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
