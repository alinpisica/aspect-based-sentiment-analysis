{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from ate_models.ATE_BERT_Dropout_Linear import ATE_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ATE_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_aspect_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                     iob_aspect_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "3            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ATE/MAMS/models/bert_fine_tuned_512.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ATE/MAMS/models/bert_fine_tuned_dropout_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ATE/MAMS/stats/bert_fine_tuned_dropout_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 3), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.6150352954864502\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.005217926111072302\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0033880972769111395\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.01870238594710827\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0019186127465218306\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.004861706402152777\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.003781108185648918\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.011865857057273388\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0018339604139328003\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0072298175655305386\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.007025955244898796\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.00043506044312380254\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.002683178288862109\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.008636474609375\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.006070611532777548\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0036993820685893297\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.008268752135336399\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0007063344237394631\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.002245777752250433\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0015196927124634385\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.005133463069796562\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0030193831771612167\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2107380628585815\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.003755983430892229\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.011027749627828598\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.005236417520791292\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.004612727090716362\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.013529431074857712\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.009010431356728077\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.00514708086848259\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.001528362394310534\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.007992671802639961\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0043618036434054375\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.007998603396117687\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.005973775405436754\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.004158210940659046\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0009417907567694783\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0032974006608128548\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.002394720446318388\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0031013537663966417\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0020314485300332308\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.001914190943352878\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.001655234256759286\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0010467154206708074\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0272917747497559\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.007056630682200193\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.008045368827879429\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0032465069089084864\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.01832488924264908\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0012297737412154675\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.005102561786770821\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0018841743003576994\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.009083579294383526\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.004686738830059767\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0057671875692903996\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0022135586477816105\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.00631760573014617\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.005763975903391838\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.002246091142296791\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0016959205968305469\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004514593631029129\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0024987906217575073\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.005460008047521114\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01147374790161848\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.004368727095425129\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.001862401608377695\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4238107204437256\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.006384960375726223\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.007598354946821928\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.005241782870143652\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.003116810228675604\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.008472640998661518\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.006807222496718168\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0015752194449305534\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0030834460631012917\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.004469662439078093\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.006767247803509235\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.003015009919181466\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.007802722044289112\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0012112671975046396\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0021180924959480762\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0007376932189799845\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.00968959555029869\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0014391392469406128\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0005466329166665673\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.005560154560953379\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.002471152925863862\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.001447938964702189\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2630335092544556\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0026242523454129696\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.005942009389400482\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0064298054203391075\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.003757441882044077\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.008888217620551586\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.003741134889423847\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.009830905124545097\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0036875694058835506\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.004291214048862457\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.010849907994270325\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0020380020141601562\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0019267465686425567\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0031644704286009073\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0010720596183091402\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0006979432073421776\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.00167306256480515\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.003867250867187977\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.004893519449979067\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.003659204812720418\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.00830074306577444\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0009918803116306663\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.902619481086731\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.005691931117326021\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.006040793377906084\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.007559686433523893\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0016161577077582479\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.004901322070509195\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.009386264719069004\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0034861164167523384\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.005459489766508341\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.007667087018489838\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.003926257602870464\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.001147130853496492\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0016428963281214237\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0006590873235836625\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.004441052675247192\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.002424022415652871\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.003139855805784464\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.005021713674068451\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0009845132008194923\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.000810430443380028\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.002165650948882103\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0018959641456604004\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1642299890518188\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.00681145628914237\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.009560484439134598\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.006166134029626846\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.005076948553323746\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.007641076110303402\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0032272462267428637\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.007248640060424805\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.005765803623944521\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0009476117556914687\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.005805028602480888\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.009745662100613117\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0046698711812496185\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.002360477577894926\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.007415224798023701\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.007098999340087175\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0005065181176178157\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0045917583629488945\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0011066881706938148\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.00640272069722414\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0035310338716953993\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.001948855584487319\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.7526717185974121\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.004649412352591753\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.005628414452075958\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.010925685055553913\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.000862623390275985\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.00814197026193142\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.001652687438763678\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0033620959147810936\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0059160273522138596\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.00521627813577652\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.006898866035044193\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0025388963986188173\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.003943286836147308\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.00219854642637074\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.003587590530514717\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.004040624015033245\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.00452434690669179\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0006244746618904173\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.007650415413081646\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.002341012703254819\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.005082982592284679\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0023290093522518873\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2611470222473145\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0026812241412699223\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.004491496365517378\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.003975309897214174\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0030507820192724466\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0030740699730813503\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.005328469909727573\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0045357211492955685\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.007531886454671621\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.005136916413903236\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0005982475122436881\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013399961404502392\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.005279920529574156\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0014326979871839285\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.005552498623728752\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.004553234204649925\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.007957372814416885\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.003598092822358012\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0012634904123842716\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0016653294442221522\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0030144297052174807\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0015456931432709098\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.476792335510254\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.008210891857743263\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.011856531724333763\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.011175330728292465\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.01191964466124773\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.012274566106498241\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.006911238189786673\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.004561285953968763\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.001899154856801033\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.003895498812198639\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.015476597473025322\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.001888301339931786\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0011243686312809587\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0016281775897368789\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0017806870164349675\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0014959641266614199\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.00245400401763618\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.00566194299608469\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.008442332036793232\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0005938904359936714\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0038222153671085835\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.001484868349507451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ATE_BERT_Dropout_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "\n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses) \n",
    "    plt.savefig(f'../../../results/ATE/MAMS/plots/bert_ft_do_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.965927</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.986722</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.976127</td>\n",
       "      <td>1439.829337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999315</td>\n",
       "      <td>0.999315</td>\n",
       "      <td>0.971782</td>\n",
       "      <td>0.999315</td>\n",
       "      <td>0.981186</td>\n",
       "      <td>0.999315</td>\n",
       "      <td>0.976402</td>\n",
       "      <td>1436.228410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999175</td>\n",
       "      <td>0.999175</td>\n",
       "      <td>0.952399</td>\n",
       "      <td>0.999175</td>\n",
       "      <td>0.989980</td>\n",
       "      <td>0.999175</td>\n",
       "      <td>0.970631</td>\n",
       "      <td>1426.185487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999173</td>\n",
       "      <td>0.999173</td>\n",
       "      <td>0.952468</td>\n",
       "      <td>0.999173</td>\n",
       "      <td>0.989507</td>\n",
       "      <td>0.999173</td>\n",
       "      <td>0.970442</td>\n",
       "      <td>1404.081388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999252</td>\n",
       "      <td>0.999252</td>\n",
       "      <td>0.957936</td>\n",
       "      <td>0.999252</td>\n",
       "      <td>0.989869</td>\n",
       "      <td>0.999252</td>\n",
       "      <td>0.973501</td>\n",
       "      <td>1431.331285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999284</td>\n",
       "      <td>0.999284</td>\n",
       "      <td>0.956885</td>\n",
       "      <td>0.999284</td>\n",
       "      <td>0.990412</td>\n",
       "      <td>0.999284</td>\n",
       "      <td>0.973190</td>\n",
       "      <td>1437.399864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.999308</td>\n",
       "      <td>0.999308</td>\n",
       "      <td>0.962095</td>\n",
       "      <td>0.999308</td>\n",
       "      <td>0.989823</td>\n",
       "      <td>0.999308</td>\n",
       "      <td>0.975655</td>\n",
       "      <td>1487.773010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.958779</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.990315</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.974137</td>\n",
       "      <td>1426.274077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999273</td>\n",
       "      <td>0.999273</td>\n",
       "      <td>0.966096</td>\n",
       "      <td>0.999273</td>\n",
       "      <td>0.981161</td>\n",
       "      <td>0.999273</td>\n",
       "      <td>0.973508</td>\n",
       "      <td>1462.830671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.962474</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.984851</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.973458</td>\n",
       "      <td>1440.559458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.999298               0.999298               0.965927            0.999298   \n",
       "1  0.999315               0.999315               0.971782            0.999315   \n",
       "2  0.999175               0.999175               0.952399            0.999175   \n",
       "3  0.999173               0.999173               0.952468            0.999173   \n",
       "4  0.999252               0.999252               0.957936            0.999252   \n",
       "5  0.999284               0.999284               0.956885            0.999284   \n",
       "6  0.999308               0.999308               0.962095            0.999308   \n",
       "7  0.999244               0.999244               0.958779            0.999244   \n",
       "8  0.999273               0.999273               0.966096            0.999273   \n",
       "9  0.999237               0.999237               0.962474            0.999237   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.986722        0.999298        0.976127     1439.829337  \n",
       "1            0.981186        0.999315        0.976402     1436.228410  \n",
       "2            0.989980        0.999175        0.970631     1426.185487  \n",
       "3            0.989507        0.999173        0.970442     1404.081388  \n",
       "4            0.989869        0.999252        0.973501     1431.331285  \n",
       "5            0.990412        0.999284        0.973190     1437.399864  \n",
       "6            0.989823        0.999308        0.975655     1487.773010  \n",
       "7            0.990315        0.999244        0.974137     1426.274077  \n",
       "8            0.981161        0.999273        0.973508     1462.830671  \n",
       "9            0.984851        0.999237        0.973458     1440.559458  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
