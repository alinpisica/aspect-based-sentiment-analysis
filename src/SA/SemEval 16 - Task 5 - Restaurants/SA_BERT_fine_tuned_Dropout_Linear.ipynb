{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from semeval_reader import SemevalReader\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from models.BERT_Dropout_Linear import BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_list_for_polarity(polarity):\n",
    "    if polarity == 'positive':\n",
    "        return [0, 0, 1]\n",
    "    if polarity == 'negative':\n",
    "        return [1, 0, 0]\n",
    "    return [0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_reader = SemevalReader('../../../data/semeval16_restaurants_train.xml')\n",
    "\n",
    "reviews = semeval_reader.read_reviews()\n",
    "absolute_polarity_sentences = semeval_reader.get_absolute_polarity_sentences()\n",
    "\n",
    "df = pd.DataFrame(map(lambda x: (x.text, x.opinions[0].polarity), absolute_polarity_sentences))\n",
    "df.rename(columns={0: 'text'}, inplace=True)\n",
    "df['target_list'] = df.apply(lambda row: get_target_list_for_polarity(row[1]), axis=1)\n",
    "\n",
    "absolute_polarity_df = df.drop(columns=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/SA/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/SA/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned_dropout_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/SA/SemEval16 - Task 5 - Restaurants/stats/bert_fine_tuned_dropout_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6945077776908875\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.3229191303253174\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.22399303317070007\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.15816020965576172\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.1405986249446869\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.19062122702598572\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.11536185443401337\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.06736935675144196\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.08973481506109238\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.3357014060020447\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.05650490149855614\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.04045473784208298\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.04644361138343811\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.033130381256341934\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.04059445485472679\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.036855779588222504\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.05321710929274559\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.025032205507159233\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.057727158069610596\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.6119771003723145\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.26075565814971924\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.34804996848106384\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.7643680572509766\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.2480534315109253\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.44185662269592285\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.1576882153749466\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.12648174166679382\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.38849377632141113\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.10297577828168869\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.5462903380393982\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.37271952629089355\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.05251459777355194\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.21358434855937958\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.4903135299682617\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.05203552171587944\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.05345630645751953\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.049460552632808685\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.11039496958255768\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.04023868218064308\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.031065762042999268\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.029866686090826988\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.019897542893886566\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.03465970233082771\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.033216360956430435\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.7373549938201904\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.3951554298400879\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.2949485778808594\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.18503837287425995\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.13154658675193787\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.14080965518951416\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.477976530790329\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.0671469122171402\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.37229928374290466\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.06875475496053696\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.07504299283027649\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.5140749216079712\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.09994684159755707\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.031049728393554688\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.026921790093183517\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.02978166937828064\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.022347941994667053\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.05275265499949455\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.36296844482421875\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.15509608387947083\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.3702748417854309\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.0880480408668518\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.5391175746917725\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.22157275676727295\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.37571990489959717\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.4951431155204773\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.3915659785270691\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.05663813650608063\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.1019664853811264\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.05887150764465332\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.10502036660909653\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.08190268278121948\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.060592375695705414\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.19196254014968872\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.2621566653251648\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.06392544507980347\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.03584897518157959\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.4749222695827484\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.03068038448691368\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.03751745820045471\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.03167041018605232\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.13758036494255066\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.019423993304371834\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.6543927192687988\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6349282264709473\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.2864811420440674\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.43685901165008545\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.7645318508148193\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.44797706604003906\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.13137200474739075\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.10829123854637146\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.05603819712996483\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.06503559648990631\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.08823235332965851\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.06144458055496216\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.3907380700111389\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.0685899406671524\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.08428822457790375\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.024902908131480217\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.05140247941017151\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.03021678328514099\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.03799087554216385\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.026494041085243225\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.04216443747282028\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.03014454059302807\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.02715536206960678\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.7535623908042908\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.4084905982017517\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.30757683515548706\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.19403791427612305\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.12605418264865875\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.11908348649740219\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.08240695297718048\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.10303100943565369\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.11123563349246979\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.04466595873236656\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.09225648641586304\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.09346812963485718\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.36206087470054626\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.062203895300626755\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.03591805323958397\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.030002376064658165\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.05229336768388748\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.04219503328204155\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.018032044172286987\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.03276722505688667\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.029324010014533997\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.04653419181704521\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6152706742286682\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.36608394980430603\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.18978291749954224\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.15816983580589294\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.2437923401594162\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.09152670204639435\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.06184810400009155\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.05496113747358322\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.16021156311035156\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.08394088596105576\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.07930032908916473\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.08191539347171783\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.08092990517616272\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.438024640083313\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.02604706585407257\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.30438998341560364\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.05395437031984329\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.06351121515035629\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.023698357865214348\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.03508034721016884\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.09889056533575058\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.41064971685409546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.7134093046188354\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.3314184546470642\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.21228936314582825\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.1513461172580719\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.3745698928833008\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.09636692702770233\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.1138981431722641\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.3704026937484741\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.0727301687002182\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.5548577904701233\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.04964229092001915\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.053741034120321274\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.05015234649181366\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.5765897631645203\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.10813695192337036\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.4221479296684265\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.3398541212081909\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.022787705063819885\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.04613877460360527\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.02995213493704796\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.04932991787791252\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.03898058086633682\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.7075995206832886\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.43520963191986084\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.20984113216400146\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.12003310769796371\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.11975975334644318\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.26610490679740906\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.07425260543823242\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.059259604662656784\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.06112702935934067\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.05875405669212341\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.05538091063499451\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.049434758722782135\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.036404889076948166\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.08022373914718628\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.20921945571899414\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.041692279279232025\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.04572009667754173\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.0501147136092186\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.020475199446082115\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.11843764781951904\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.021927610039711\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.02382173202931881\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6649478673934937\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.5446168184280396\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.21101360023021698\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.16541405022144318\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.15359275043010712\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.5506733655929565\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.05755079910159111\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.23025064170360565\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.06288603693246841\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.063912034034729\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.3774510324001312\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.055487923324108124\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.055216073989868164\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.028572868555784225\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.1852944791316986\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.05766985937952995\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.03304930776357651\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.10250169038772583\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.4111484885215759\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.4133567810058594\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.03327770531177521\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.031178360804915428\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/10\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True\n",
    "    )\n",
    "\n",
    "    model = BERT_Dropout_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, no_out_labels=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    targets = np.argmax(targets, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.939103</td>\n",
       "      <td>0.939103</td>\n",
       "      <td>0.784160</td>\n",
       "      <td>0.939103</td>\n",
       "      <td>0.683917</td>\n",
       "      <td>0.939103</td>\n",
       "      <td>0.700570</td>\n",
       "      <td>214.968346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.947937</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.663808</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.687239</td>\n",
       "      <td>211.561142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.953672</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.662468</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.681514</td>\n",
       "      <td>205.901747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.943135</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.914692</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.927949</td>\n",
       "      <td>204.181634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.949062</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.704672</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.730427</td>\n",
       "      <td>208.950001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.956338</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.660940</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.681762</td>\n",
       "      <td>209.558999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.630906</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.646572</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.638542</td>\n",
       "      <td>208.894078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.813442</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.736887</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.762339</td>\n",
       "      <td>209.888987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.595703</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.631341</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>209.368890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.926282</td>\n",
       "      <td>0.926282</td>\n",
       "      <td>0.615683</td>\n",
       "      <td>0.926282</td>\n",
       "      <td>0.644112</td>\n",
       "      <td>0.926282</td>\n",
       "      <td>0.629559</td>\n",
       "      <td>208.753999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.939103               0.939103               0.784160            0.939103   \n",
       "1  0.932692               0.932692               0.947937            0.932692   \n",
       "2  0.932692               0.932692               0.953672            0.932692   \n",
       "3  0.980769               0.980769               0.943135            0.980769   \n",
       "4  0.929487               0.929487               0.949062            0.929487   \n",
       "5  0.929487               0.929487               0.956338            0.929487   \n",
       "6  0.958333               0.958333               0.630906            0.958333   \n",
       "7  0.929487               0.929487               0.813442            0.929487   \n",
       "8  0.913462               0.913462               0.595703            0.913462   \n",
       "9  0.926282               0.926282               0.615683            0.926282   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.683917        0.939103        0.700570      214.968346  \n",
       "1            0.663808        0.932692        0.687239      211.561142  \n",
       "2            0.662468        0.932692        0.681514      205.901747  \n",
       "3            0.914692        0.980769        0.927949      204.181634  \n",
       "4            0.704672        0.929487        0.730427      208.950001  \n",
       "5            0.660940        0.929487        0.681762      209.558999  \n",
       "6            0.646572        0.958333        0.638542      208.894078  \n",
       "7            0.736887        0.929487        0.762339      209.888987  \n",
       "8            0.631341        0.913462        0.611111      209.368890  \n",
       "9            0.644112        0.926282        0.629559      208.753999  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
