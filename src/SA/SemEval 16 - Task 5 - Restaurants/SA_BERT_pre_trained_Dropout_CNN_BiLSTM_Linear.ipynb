{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from semeval_reader import SemevalReader\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from models.BERT_Dropout_CNN_BiLSTM_Linear import BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_list_for_polarity(polarity):\n",
    "    if polarity == 'positive':\n",
    "        return [0, 0, 1]\n",
    "    if polarity == 'negative':\n",
    "        return [1, 0, 0]\n",
    "    return [0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_reader = SemevalReader('../../../data/semeval16_restaurants_train.xml')\n",
    "\n",
    "reviews = semeval_reader.read_reviews()\n",
    "absolute_polarity_sentences = semeval_reader.get_absolute_polarity_sentences()\n",
    "\n",
    "df = pd.DataFrame(map(lambda x: (x.text, x.opinions[0].polarity), absolute_polarity_sentences))\n",
    "df.rename(columns={0: 'text'}, inplace=True)\n",
    "df['target_list'] = df.apply(lambda row: get_target_list_for_polarity(row[1]), axis=1)\n",
    "\n",
    "absolute_polarity_df = df.drop(columns=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/SA/SemEval16 - Task 5 - Restaurants/models/bert_pre_trained_dropout_cnn_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/SA/SemEval16 - Task 5 - Restaurants/stats/bert_pre_trained_dropout_cnn_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6849984526634216\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.551174521446228\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.5675079822540283\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.5017633438110352\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.6461917757987976\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.13236531615257263\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.2261066734790802\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.3827422559261322\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.09966349601745605\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.4253297746181488\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.5212905406951904\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.8933717608451843\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.36494678258895874\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.07298640161752701\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.3547503352165222\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.027988139539957047\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.04060964658856392\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.10326582193374634\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.05279853194952011\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.06818564981222153\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.1943250298500061\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.011843038722872734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6887361407279968\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.6008433699607849\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.5504064559936523\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.39596879482269287\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.5702389478683472\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.31207895278930664\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.1390637755393982\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.38819608092308044\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.3798310458660126\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.5377541184425354\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.4577880799770355\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.11689735949039459\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.44993335008621216\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.07149426639080048\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.037890054285526276\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.034489747136831284\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.07911334931850433\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.38688403367996216\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.024788808077573776\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.05477605387568474\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.03409712761640549\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.042134709656238556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6794683337211609\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.5144062042236328\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.5319206714630127\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.3925364911556244\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.4637276828289032\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.5818487405776978\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.31780827045440674\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.2671831548213959\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.3931359648704529\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.5375869274139404\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.501971960067749\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.038523606956005096\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.0406625010073185\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.022803951054811478\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.07349824160337448\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.5086113810539246\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.07626689970493317\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.09992630779743195\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.012905745767056942\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.010693433694541454\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.05188615620136261\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.019984424114227295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6853570938110352\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.5731658935546875\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.764578104019165\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.5000852346420288\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.26460325717926025\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.28683924674987793\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.3892136216163635\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.27479779720306396\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.09317802637815475\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.12324162572622299\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.10074687004089355\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.12114866077899933\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.3328900933265686\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.7843090295791626\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.015208887867629528\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.013263355009257793\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.015111085027456284\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.01012833695858717\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.6005643606185913\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.11201785504817963\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.07423917949199677\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.04101293534040451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6964221000671387\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.7109543085098267\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.6685733199119568\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.2759927213191986\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.46100112795829773\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.22034752368927002\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.14681535959243774\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.1864432543516159\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.09403541684150696\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.24148350954055786\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.20678339898586273\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.15376776456832886\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.08428364247083664\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.07747279107570648\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.4345700740814209\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.4478917121887207\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.07287810742855072\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.04779994487762451\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.11290089786052704\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.02939257025718689\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.07206352055072784\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.25588542222976685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.7044438123703003\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.6262228488922119\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.5435106754302979\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.41057726740837097\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.3754904866218567\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.7301740050315857\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.4045577943325043\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.28121405839920044\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.35082051157951355\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.5005247592926025\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.12198790162801743\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.059068065136671066\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.07050080597400665\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.40532276034355164\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.05079053342342377\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.06243818253278732\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.07432855665683746\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.7296657562255859\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.027968037873506546\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.10244335979223251\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.0393546000123024\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.03389086574316025\n",
      "Run 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6871427297592163\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.50816810131073\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.4279990792274475\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.3820075988769531\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.5867719054222107\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.5182000398635864\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.10027050971984863\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.5084753632545471\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.16644249856472015\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.06543536484241486\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.2661551237106323\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.12457741796970367\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.41237181425094604\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.07049304246902466\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.0515303760766983\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.06481412053108215\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.12359599769115448\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.04705725237727165\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.038673821836709976\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.20932498574256897\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.06959018111228943\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.061314038932323456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6830505728721619\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.5739259123802185\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.6856088042259216\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.24600490927696228\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.43721431493759155\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.5217752456665039\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.5001397728919983\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.10887718200683594\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.3200184106826782\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.3526468873023987\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.5949265956878662\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.14439657330513\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.05566447973251343\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.052916690707206726\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.09872026741504669\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.035538699477910995\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.44568049907684326\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.07895755022764206\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.029135283082723618\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.349226713180542\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.052046604454517365\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.03163421154022217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6904184222221375\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.5874440670013428\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.4215044379234314\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.3365277647972107\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.3176775276660919\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.6553312540054321\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.1882181167602539\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.3597099184989929\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.19706891477108002\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.12855221331119537\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.13188572227954865\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.07984905689954758\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.29229772090911865\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.11548858880996704\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.4149824380874634\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.20395159721374512\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.033904194831848145\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.0729997381567955\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.04619817063212395\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.06481700390577316\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.383950412273407\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.6621028184890747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6866952180862427\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.5388302206993103\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.4090738296508789\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.3749626874923706\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.38168400526046753\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.1961364597082138\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.28114205598831177\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.34564799070358276\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.3629702925682068\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.31423425674438477\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.5280775427818298\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.09572979807853699\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.03257439285516739\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.10746961832046509\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.4115232825279236\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.060619812458753586\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.0416710339486599\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.2757192552089691\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.021781161427497864\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.7606687545776367\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.011194700375199318\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.02052496001124382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/10\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True\n",
    "    )\n",
    "\n",
    "    model = BERT_Dropout_CNN_BiLSTM_Linear(bert=BertModel.from_pretrained('bert-base-uncased'), bert_seq_len=512, dropout=0.3, bilstm_in_features=256, no_out_labels=3, conv_out_channels=54, conv_kernel_size=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    targets = np.argmax(targets, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.579088</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.619409</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.596886</td>\n",
       "      <td>201.170180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.593324</td>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.633655</td>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.612474</td>\n",
       "      <td>198.677062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.613640</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.629980</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.621599</td>\n",
       "      <td>198.567633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.587527</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.615704</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.601267</td>\n",
       "      <td>198.563250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.605887</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.610451</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.608052</td>\n",
       "      <td>199.355402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.912312</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.642195</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.657977</td>\n",
       "      <td>211.377116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.609488</td>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.615217</td>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.611223</td>\n",
       "      <td>208.473276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.571759</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.589461</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>207.678398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.576813</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.616791</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.595046</td>\n",
       "      <td>208.089441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.591811</td>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.616598</td>\n",
       "      <td>0.907051</td>\n",
       "      <td>0.603947</td>\n",
       "      <td>210.883662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.894231               0.894231               0.579088            0.894231   \n",
       "1  0.907051               0.907051               0.593324            0.907051   \n",
       "2  0.923077               0.923077               0.613640            0.923077   \n",
       "3  0.903846               0.903846               0.587527            0.903846   \n",
       "4  0.916667               0.916667               0.605887            0.916667   \n",
       "5  0.891026               0.891026               0.912312            0.891026   \n",
       "6  0.907051               0.907051               0.609488            0.907051   \n",
       "7  0.875000               0.875000               0.571759            0.875000   \n",
       "8  0.903846               0.903846               0.576813            0.903846   \n",
       "9  0.907051               0.907051               0.591811            0.907051   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.619409        0.894231        0.596886      201.170180  \n",
       "1            0.633655        0.907051        0.612474      198.677062  \n",
       "2            0.629980        0.923077        0.621599      198.567633  \n",
       "3            0.615704        0.903846        0.601267      198.563250  \n",
       "4            0.610451        0.916667        0.608052      199.355402  \n",
       "5            0.642195        0.891026        0.657977      211.377116  \n",
       "6            0.615217        0.907051        0.611223      208.473276  \n",
       "7            0.589461        0.875000        0.580357      207.678398  \n",
       "8            0.616791        0.903846        0.595046      208.089441  \n",
       "9            0.616598        0.907051        0.603947      210.883662  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
