{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from semeval_reader import SemevalReader\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from sa_models.SA_BERT_Dropout_CNN_BiLSTM_Linear import SA_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_list_for_polarity(polarity):\n",
    "    if polarity == 'positive':\n",
    "        return [0, 0, 1]\n",
    "    if polarity == 'negative':\n",
    "        return [1, 0, 0]\n",
    "    return [0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_reader = SemevalReader('../../../data/semeval16_restaurants_train.xml')\n",
    "\n",
    "reviews = semeval_reader.read_reviews()\n",
    "absolute_polarity_sentences = semeval_reader.get_absolute_polarity_sentences()\n",
    "\n",
    "df = pd.DataFrame(map(lambda x: (x.text, x.opinions[0].polarity), absolute_polarity_sentences))\n",
    "df.rename(columns={0: 'text'}, inplace=True)\n",
    "df['target_list'] = df.apply(lambda row: get_target_list_for_polarity(row[1]), axis=1)\n",
    "\n",
    "absolute_polarity_df = df.drop(columns=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/SA/SemEval16 - Task 5 - Restaurants/models/bert_pre_trained_dropout_cnn_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/SA/SemEval16 - Task 5 - Restaurants/stats/bert_pre_trained_dropout_cnn_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6874613761901855\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.6216678619384766\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.5358039736747742\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.2977152466773987\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.3557056784629822\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.5149423480033875\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.37484925985336304\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.06865265965461731\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.5727523565292358\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.23172152042388916\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.09408882260322571\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.0561547577381134\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.10869568586349487\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.03591163083910942\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.06960925459861755\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.13841183483600616\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.030344031751155853\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.018795784562826157\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.07282781600952148\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.014008983969688416\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.017621971666812897\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.020386669784784317\n",
      "Run 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.7154633402824402\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.618417501449585\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.44464391469955444\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.29741400480270386\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.24994191527366638\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.36358147859573364\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.399749755859375\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.11229853332042694\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.11725249141454697\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.48750731348991394\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.09451286494731903\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.45425403118133545\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.05628611147403717\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.08714979887008667\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.05240461230278015\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.15310057997703552\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.08278203755617142\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.12393645942211151\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.07127564400434494\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.20268969237804413\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.48829683661460876\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.40319058299064636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6784725189208984\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.6468285918235779\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.5042349696159363\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.5352164506912231\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.47054433822631836\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.2887949049472809\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.19461993873119354\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.49358123540878296\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.3922000825405121\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.12829820811748505\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.14438985288143158\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.07314185053110123\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.6096878051757812\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.6356380581855774\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.03971702978014946\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.12151290476322174\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.034514110535383224\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.09342437237501144\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.04985713213682175\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.6620180010795593\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.02882145345211029\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.013670064508914948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6915306448936462\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.5008850693702698\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.43643781542778015\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.40850120782852173\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.3932937979698181\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.49668335914611816\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.18283526599407196\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.22553515434265137\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.11554718017578125\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.10422860085964203\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.04256674647331238\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.4518810510635376\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.047374874353408813\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.45230862498283386\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.03088327869772911\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.07235540449619293\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.4554923474788666\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.025385309010744095\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.47408974170684814\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.10396911948919296\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.020688669756054878\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.0156802237033844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6960047483444214\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.6021063923835754\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 1.0069389343261719\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.41011083126068115\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.2672017216682434\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.48662716150283813\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.9991106986999512\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.1091393530368805\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.1254575252532959\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.0761856809258461\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.2077949047088623\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.5903880000114441\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.13890618085861206\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.06468860805034637\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.03186438977718353\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.05669192597270012\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.042034875601530075\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.059852421283721924\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.101986363530159\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.2039155662059784\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.3685467839241028\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.04335886985063553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.7048889994621277\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.6119951009750366\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.531725287437439\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.4031209647655487\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.7964865565299988\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.46224260330200195\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.13961440324783325\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.4360681474208832\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.2515283226966858\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.1599084436893463\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.09676627814769745\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.19408546388149261\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.08685772120952606\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.3969612717628479\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.05424705147743225\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.40960773825645447\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.0343082956969738\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.410038560628891\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.045730870217084885\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.027755871415138245\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.042975954711437225\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.03481569513678551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.689222514629364\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.5568697452545166\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.43063777685165405\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.7757756114006042\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.37763065099716187\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.3439243733882904\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.2519928216934204\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.32056301832199097\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.5668320655822754\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.12064292281866074\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.1275022029876709\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.4036598205566406\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.05753746256232262\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.0642387866973877\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.35002702474594116\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.7082979679107666\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.41685253381729126\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.9279353022575378\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.03888038545846939\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.2485237568616867\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.08116918802261353\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.047532595694065094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.7031309008598328\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.491115927696228\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.4360962510108948\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.20849382877349854\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.36028608679771423\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.5691620111465454\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.7282012701034546\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.1018361747264862\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.12036987394094467\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.302007257938385\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.22022515535354614\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.05869429558515549\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.07032941281795502\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.04013147950172424\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.035613421350717545\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.6353021860122681\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.013124758377671242\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.05441413074731827\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.3823172450065613\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.030213896185159683\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.2951480746269226\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.4234694242477417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.694473922252655\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.608627438545227\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.2334899604320526\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.45565494894981384\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.2307671308517456\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.2500508427619934\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.2508369982242584\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.23538559675216675\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.2121790498495102\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.05845759063959122\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.2977002263069153\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.385844886302948\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.030874555930495262\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.04154539108276367\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.09882613271474838\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.02437276765704155\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.01254332810640335\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.028273750096559525\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.08698958158493042\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.027177851647138596\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.024918539449572563\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.02633090876042843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2, Batch: 0/313, Loss: 0.6832401156425476\n",
      "Epoch: 0/2, Batch: 31/313, Loss: 0.5110331773757935\n",
      "Epoch: 0/2, Batch: 62/313, Loss: 0.4355607032775879\n",
      "Epoch: 0/2, Batch: 93/313, Loss: 0.630267858505249\n",
      "Epoch: 0/2, Batch: 124/313, Loss: 0.47408056259155273\n",
      "Epoch: 0/2, Batch: 155/313, Loss: 0.317720890045166\n",
      "Epoch: 0/2, Batch: 186/313, Loss: 0.29666051268577576\n",
      "Epoch: 0/2, Batch: 217/313, Loss: 0.4744132459163666\n",
      "Epoch: 0/2, Batch: 248/313, Loss: 0.3339642882347107\n",
      "Epoch: 0/2, Batch: 279/313, Loss: 0.047195132821798325\n",
      "Epoch: 0/2, Batch: 310/313, Loss: 0.26396793127059937\n",
      "Epoch: 1/2, Batch: 0/313, Loss: 0.19994090497493744\n",
      "Epoch: 1/2, Batch: 31/313, Loss: 0.1214948520064354\n",
      "Epoch: 1/2, Batch: 62/313, Loss: 0.030105775222182274\n",
      "Epoch: 1/2, Batch: 93/313, Loss: 0.04274573177099228\n",
      "Epoch: 1/2, Batch: 124/313, Loss: 0.1157049685716629\n",
      "Epoch: 1/2, Batch: 155/313, Loss: 0.3847389817237854\n",
      "Epoch: 1/2, Batch: 186/313, Loss: 0.06596709787845612\n",
      "Epoch: 1/2, Batch: 217/313, Loss: 0.043254654854536057\n",
      "Epoch: 1/2, Batch: 248/313, Loss: 0.12645837664604187\n",
      "Epoch: 1/2, Batch: 279/313, Loss: 0.2429969757795334\n",
      "Epoch: 1/2, Batch: 310/313, Loss: 0.017847266048192978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/10\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True\n",
    "    )\n",
    "\n",
    "    model = SA_BERT_Dropout_CNN_BiLSTM_Linear(bert=BertModel.from_pretrained('bert-base-uncased'), bert_seq_len=512, dropout=0.3, bilstm_in_features=256, no_out_labels=3, conv_out_channels=54, conv_kernel_size=3, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    targets = np.argmax(targets, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.588618</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.612703</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.600383</td>\n",
       "      <td>209.204387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.602125</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.610051</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.605597</td>\n",
       "      <td>208.909574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.580676</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.608747</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.594010</td>\n",
       "      <td>209.502670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.560620</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.610296</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.579609</td>\n",
       "      <td>208.938160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.586048</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.588858</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.585939</td>\n",
       "      <td>206.405250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.589864</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.624877</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.605352</td>\n",
       "      <td>204.803477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.579521</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.622783</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.598367</td>\n",
       "      <td>207.881433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.887821</td>\n",
       "      <td>0.887821</td>\n",
       "      <td>0.584738</td>\n",
       "      <td>0.887821</td>\n",
       "      <td>0.616201</td>\n",
       "      <td>0.887821</td>\n",
       "      <td>0.599777</td>\n",
       "      <td>203.828193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.596669</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.626885</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.610923</td>\n",
       "      <td>203.669993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.603366</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.624560</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.613777</td>\n",
       "      <td>203.716484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.891026               0.891026               0.588618            0.891026   \n",
       "1  0.903846               0.903846               0.602125            0.903846   \n",
       "2  0.894231               0.894231               0.580676            0.894231   \n",
       "3  0.878205               0.878205               0.560620            0.878205   \n",
       "4  0.878205               0.878205               0.586048            0.878205   \n",
       "5  0.910256               0.910256               0.589864            0.910256   \n",
       "6  0.897436               0.897436               0.579521            0.897436   \n",
       "7  0.887821               0.887821               0.584738            0.887821   \n",
       "8  0.916667               0.916667               0.596669            0.916667   \n",
       "9  0.913462               0.913462               0.603366            0.913462   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.612703        0.891026        0.600383      209.204387  \n",
       "1            0.610051        0.903846        0.605597      208.909574  \n",
       "2            0.608747        0.894231        0.594010      209.502670  \n",
       "3            0.610296        0.878205        0.579609      208.938160  \n",
       "4            0.588858        0.878205        0.585939      206.405250  \n",
       "5            0.624877        0.910256        0.605352      204.803477  \n",
       "6            0.622783        0.897436        0.598367      207.881433  \n",
       "7            0.616201        0.887821        0.599777      203.828193  \n",
       "8            0.626885        0.916667        0.610923      203.669993  \n",
       "9            0.624560        0.913462        0.613777      203.716484  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
