{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_CNN_BiLSTM_Linear import ABSA_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_SemEval16_Restaurants_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>[Judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>[We, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>[They, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>[The, food, was, lousy, -, too, sweet, or, too...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Judging from previous posts this used to be a ...   \n",
       "1  We, there were four of us, arrived at noon - t...   \n",
       "2  They never brought us complimentary noodles, i...   \n",
       "3  The food was lousy - too sweet or too salty an...   \n",
       "4  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Judging, from, previous, posts, this, used, t...   \n",
       "1  [We, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [They, never, brought, us, complimentary, nood...   \n",
       "3  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "4  [The, food, was, lousy, -, too, sweet, or, too...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ABSA/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/SemEval16 - Task 5 - Restaurants/models/bert_fine_tuned_dropout_cnn_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/SemEval16 - Task 5 - Restaurants/stats/bert_fine_tuned_dropout_cnn_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3791313171386719\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.7832396626472473\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.25685885548591614\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.16308583319187164\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.07236038148403168\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.053919460624456406\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.055026598274707794\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.03827543929219246\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.04102105274796486\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06114530563354492\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.0535225048661232\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.03238680958747864\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.0463700145483017\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.02261694334447384\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.0643007904291153\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.035171955823898315\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.048358090221881866\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.04689274728298187\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.03355842083692551\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.018842751160264015\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.030310027301311493\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.010604733601212502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3641107082366943\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.8239360451698303\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.31828704476356506\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.16850361227989197\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.11828004568815231\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.09411289542913437\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.0803120881319046\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.05951399728655815\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.07759344577789307\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.06753435730934143\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.042851801961660385\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.05055856704711914\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.04003523290157318\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.05058004707098007\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.06702111661434174\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.046212080866098404\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.03653896227478981\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.045877035707235336\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.04148229584097862\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.04450536519289017\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.02788243629038334\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.051784489303827286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3971620798110962\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.6693621277809143\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1958797723054886\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.11131821572780609\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.06754472851753235\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05422203615307808\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.07840702682733536\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.034936219453811646\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.030279647558927536\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.017139554023742676\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.026397010311484337\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.03282485157251358\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.03176300972700119\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.030062565580010414\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.0178560558706522\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.015436606481671333\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.008604682981967926\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.027908919379115105\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.019933033734560013\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.010901074856519699\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.01407429575920105\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.0071443370543420315\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3759942054748535\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.6145530939102173\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.1890699863433838\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.09356549382209778\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.06024505943059921\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.05707227811217308\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03864336386322975\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.030549542978405952\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.02893415093421936\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.027721723541617393\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.028823520988225937\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.017474109306931496\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.024479907006025314\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.026125000789761543\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.016810808330774307\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.026944095268845558\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.024180090054869652\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.010907353833317757\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.015781113877892494\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.05671720579266548\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.010990656912326813\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.017846928909420967\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.3288021087646484\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 1.1250958442687988\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.34509241580963135\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.11349646747112274\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.052389636635780334\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.033426184207201004\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.03391922637820244\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.02501101605594158\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.0350249782204628\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.029813792556524277\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.014166760258376598\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.022020800039172173\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.06589243561029434\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.03700666129589081\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.02574761025607586\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.02042020671069622\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.024061063304543495\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.020877357572317123\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.022052235901355743\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.012942325323820114\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.006051623262465\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.017550745978951454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.423202395439148\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.9547160863876343\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.369901567697525\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.1928642839193344\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.1265455037355423\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.10181789100170135\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.07552804797887802\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.06070491299033165\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.05331553891301155\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.05608479678630829\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.04431859403848648\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.04429730027914047\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.034737616777420044\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.034711193293333054\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.04101503640413284\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.042126771062612534\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.026310313493013382\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.028388500213623047\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.03107151947915554\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.031138593330979347\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.026620831340551376\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.03163694590330124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.4553301334381104\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 1.0866166353225708\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.4185488224029541\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.18107494711875916\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.12448842078447342\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.07927144318819046\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.06825843453407288\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.07052507996559143\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.05284758284687996\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.050642289221286774\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.056155432015657425\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.050901468843221664\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.04834197461605072\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.04309484362602234\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.03750821202993393\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.031653404235839844\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.04852695018053055\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.03347618132829666\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.0331820584833622\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.03645135462284088\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.03220590576529503\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.03703387454152107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.4070982933044434\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.6454086899757385\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.21360935270786285\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.09324800223112106\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.06844113767147064\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.03397832438349724\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.032687027007341385\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0403105653822422\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.02016087993979454\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.07819543033838272\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.022587209939956665\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.02245577983558178\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.023781055584549904\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.028993265703320503\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.030787330120801926\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.015860142186284065\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.031248364597558975\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.013999651186168194\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.019355857744812965\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.016401413828134537\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.024642493575811386\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.026385365054011345\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.4012635946273804\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.798592209815979\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.23199215531349182\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.12359057366847992\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.06252966821193695\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.041480306535959244\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.04064248129725456\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.04009825736284256\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.032084815204143524\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.030222205445170403\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.03247006610035896\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.020597873255610466\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.027915265411138535\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.021821320056915283\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.03371003642678261\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.031382445245981216\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.015489323996007442\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.013117147609591484\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.05288447067141533\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.025684760883450508\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.013576598837971687\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.01985940895974636\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/501, Loss: 1.341726303100586\n",
      "Epoch: 0/2, Batch: 50/501, Loss: 0.7334640026092529\n",
      "Epoch: 0/2, Batch: 100/501, Loss: 0.22304096817970276\n",
      "Epoch: 0/2, Batch: 150/501, Loss: 0.09718289971351624\n",
      "Epoch: 0/2, Batch: 200/501, Loss: 0.06522771716117859\n",
      "Epoch: 0/2, Batch: 250/501, Loss: 0.044909343123435974\n",
      "Epoch: 0/2, Batch: 300/501, Loss: 0.047009725123643875\n",
      "Epoch: 0/2, Batch: 350/501, Loss: 0.0389801450073719\n",
      "Epoch: 0/2, Batch: 400/501, Loss: 0.02764223888516426\n",
      "Epoch: 0/2, Batch: 450/501, Loss: 0.021825570613145828\n",
      "Epoch: 0/2, Batch: 500/501, Loss: 0.02710687555372715\n",
      "Epoch: 1/2, Batch: 0/501, Loss: 0.028512077406048775\n",
      "Epoch: 1/2, Batch: 50/501, Loss: 0.027549302205443382\n",
      "Epoch: 1/2, Batch: 100/501, Loss: 0.030616093426942825\n",
      "Epoch: 1/2, Batch: 150/501, Loss: 0.021268345415592194\n",
      "Epoch: 1/2, Batch: 200/501, Loss: 0.02535199001431465\n",
      "Epoch: 1/2, Batch: 250/501, Loss: 0.01652650348842144\n",
      "Epoch: 1/2, Batch: 300/501, Loss: 0.02831178903579712\n",
      "Epoch: 1/2, Batch: 350/501, Loss: 0.019713107496500015\n",
      "Epoch: 1/2, Batch: 400/501, Loss: 0.024747220799326897\n",
      "Epoch: 1/2, Batch: 450/501, Loss: 0.029164571315050125\n",
      "Epoch: 1/2, Batch: 500/501, Loss: 0.022869180887937546\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_CNN_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994969</td>\n",
       "      <td>0.994969</td>\n",
       "      <td>0.248751</td>\n",
       "      <td>0.994969</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.994969</td>\n",
       "      <td>0.249374</td>\n",
       "      <td>499.831584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.248782</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.249390</td>\n",
       "      <td>494.516235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995098</td>\n",
       "      <td>0.995098</td>\n",
       "      <td>0.483781</td>\n",
       "      <td>0.995098</td>\n",
       "      <td>0.255878</td>\n",
       "      <td>0.995098</td>\n",
       "      <td>0.260355</td>\n",
       "      <td>486.927129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994664</td>\n",
       "      <td>0.994664</td>\n",
       "      <td>0.373733</td>\n",
       "      <td>0.994664</td>\n",
       "      <td>0.250226</td>\n",
       "      <td>0.994664</td>\n",
       "      <td>0.249850</td>\n",
       "      <td>468.425592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.995074</td>\n",
       "      <td>0.995074</td>\n",
       "      <td>0.436274</td>\n",
       "      <td>0.995074</td>\n",
       "      <td>0.252411</td>\n",
       "      <td>0.995074</td>\n",
       "      <td>0.254151</td>\n",
       "      <td>403.872148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.994848</td>\n",
       "      <td>0.994848</td>\n",
       "      <td>0.248712</td>\n",
       "      <td>0.994848</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.994848</td>\n",
       "      <td>0.249354</td>\n",
       "      <td>356.734167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.994957</td>\n",
       "      <td>0.994957</td>\n",
       "      <td>0.248739</td>\n",
       "      <td>0.994957</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.994957</td>\n",
       "      <td>0.249368</td>\n",
       "      <td>356.549546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.995184</td>\n",
       "      <td>0.995184</td>\n",
       "      <td>0.515532</td>\n",
       "      <td>0.995184</td>\n",
       "      <td>0.277121</td>\n",
       "      <td>0.995184</td>\n",
       "      <td>0.271461</td>\n",
       "      <td>356.531179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.994891</td>\n",
       "      <td>0.994891</td>\n",
       "      <td>0.470177</td>\n",
       "      <td>0.994891</td>\n",
       "      <td>0.257213</td>\n",
       "      <td>0.994891</td>\n",
       "      <td>0.263361</td>\n",
       "      <td>444.991704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.994871</td>\n",
       "      <td>0.994871</td>\n",
       "      <td>0.248727</td>\n",
       "      <td>0.994871</td>\n",
       "      <td>0.249999</td>\n",
       "      <td>0.994871</td>\n",
       "      <td>0.249362</td>\n",
       "      <td>491.557349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.994969               0.994969               0.248751            0.994969   \n",
       "1  0.995129               0.995129               0.248782            0.995129   \n",
       "2  0.995098               0.995098               0.483781            0.995098   \n",
       "3  0.994664               0.994664               0.373733            0.994664   \n",
       "4  0.995074               0.995074               0.436274            0.995074   \n",
       "5  0.994848               0.994848               0.248712            0.994848   \n",
       "6  0.994957               0.994957               0.248739            0.994957   \n",
       "7  0.995184               0.995184               0.515532            0.995184   \n",
       "8  0.994891               0.994891               0.470177            0.994891   \n",
       "9  0.994871               0.994871               0.248727            0.994871   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.250000        0.994969        0.249374      499.831584  \n",
       "1            0.250000        0.995129        0.249390      494.516235  \n",
       "2            0.255878        0.995098        0.260355      486.927129  \n",
       "3            0.250226        0.994664        0.249850      468.425592  \n",
       "4            0.252411        0.995074        0.254151      403.872148  \n",
       "5            0.250000        0.994848        0.249354      356.734167  \n",
       "6            0.250000        0.994957        0.249368      356.549546  \n",
       "7            0.277121        0.995184        0.271461      356.531179  \n",
       "8            0.257213        0.994891        0.263361      444.991704  \n",
       "9            0.249999        0.994871        0.249362      491.557349  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
