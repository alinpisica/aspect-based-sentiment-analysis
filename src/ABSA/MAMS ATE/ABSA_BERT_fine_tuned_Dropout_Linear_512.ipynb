{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_Linear import ABSA_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ABSA/MAMS/models/bert_fine_tuned_512.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_fine_tuned_dropout_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_fine_tuned_dropout_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.9492353200912476\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.008134338073432446\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.019667260348796844\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.009682302363216877\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.009305421262979507\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.00721031054854393\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.010070200078189373\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.009894106537103653\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.003577489173039794\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.004022869281470776\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.006201484240591526\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0031398627907037735\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0035414008889347315\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0009759792010299861\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01182918343693018\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.004381000995635986\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0018560822354629636\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.005700790323317051\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0013193851336836815\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0023716760333627462\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.011162514798343182\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0038517082575708628\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0891257524490356\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.013059663586318493\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.010595728643238544\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.018392939120531082\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.008979805745184422\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0040612961165606976\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.008878310211002827\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.010495878756046295\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.009578547440469265\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0070087602362036705\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0007886948878876865\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.016170671209692955\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.007239981088787317\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.004967967513948679\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.007913526147603989\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.002911881310865283\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0064393021166324615\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0055069345980882645\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.004645439796149731\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0020992893259972334\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.005071152932941914\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.006075617391616106\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0354536771774292\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0050476714968681335\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.020058657974004745\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0061199418269097805\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0076346201822161674\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.005337613169103861\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.010920497588813305\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.009486622177064419\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.005880553741008043\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01094506960362196\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.003580845659598708\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.006270295009016991\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.008965288288891315\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01371290534734726\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0019702089484781027\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.007574001792818308\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004565899726003408\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.009506645612418652\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.003753518918529153\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.002669069916009903\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.004545003175735474\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.003697131760418415\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.452362298965454\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.010472544468939304\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.009893840178847313\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.012149033136665821\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0036985871847718954\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.013787902891635895\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.005462553817778826\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.014235730282962322\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.008700371719896793\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.003132760990411043\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.01097485888749361\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.007744195405393839\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0023911013267934322\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.009049712680280209\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0012668175622820854\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.007973739877343178\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.005456435959786177\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.007927107624709606\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0026761775370687246\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0024888634216040373\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.005272355861961842\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0009039444266818464\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.5874429941177368\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.010191422887146473\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.007268467918038368\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.020107513293623924\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.005134493112564087\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.01666237786412239\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.008014332503080368\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.011304144747555256\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.018361933529376984\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.006461340468376875\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.005991726648062468\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.004759974777698517\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0035586024168878794\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.007611013017594814\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.00782813224941492\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0022118636406958103\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0107589615508914\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.004907344467937946\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01251126267015934\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.012345653958618641\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.012043487280607224\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.007748277857899666\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.474556803703308\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.010225473903119564\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.009663291275501251\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.008418384939432144\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.01036014687269926\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.008695709519088268\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.004862530622631311\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.006243822164833546\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.00640698429197073\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.013751205056905746\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.004275115206837654\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.005301137920469046\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.012152335606515408\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.001874163863249123\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.004334344994276762\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0053162528201937675\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.00418401462957263\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0013623700942844152\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0025934746954590082\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.004689471330493689\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.00288989394903183\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0022427102085202932\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.6030919551849365\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.017728980630636215\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0078077130019664764\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.01510681863874197\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.008495642803609371\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.007617650553584099\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.009344645775854588\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.009319161996245384\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.005324350204318762\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.005492941476404667\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0023173075169324875\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.004284475930035114\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0061594764702022076\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.004296473693102598\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0030660333577543497\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0068185278214514256\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0017470575403422117\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.008177093230187893\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0032159346155822277\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.004191989544779062\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.005033207591623068\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0030221492052078247\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.9742105603218079\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.008678569458425045\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.01011514663696289\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.003651405917480588\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.010813082568347454\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.00896676629781723\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.006653773598372936\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.012249049730598927\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.016002338379621506\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.005867348052561283\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.010264451615512371\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.008758100681006908\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.00527816079556942\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0034052140545099974\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.011969533748924732\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.004527377896010876\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.005917506758123636\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.002018074505031109\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.005663025192916393\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0030093006789684296\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0033863934222608805\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.008188030682504177\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2549335956573486\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.007602219004184008\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.009377975016832352\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.00460863159969449\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.013726219534873962\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.003485511988401413\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.009482845664024353\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.005499192047864199\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.00623284000903368\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.00769495964050293\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.002799903741106391\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.009644939564168453\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0054185651242733\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.006158751901239157\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.00458698533475399\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.005411869380623102\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0034283639397472143\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0013675352092832327\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0049041397869586945\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.001744799199514091\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.002254955004900694\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0037319434341043234\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.538814663887024\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.011165274307131767\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.006879714317619801\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.007717435713857412\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.01363425049930811\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.009273712523281574\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.004287026356905699\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.007201294414699078\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.009660388343036175\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.005977733060717583\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.006544416770339012\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.005588590167462826\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.006142936646938324\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0065208240412175655\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0028157487977296114\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.002324227709323168\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.012153970077633858\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.005469976458698511\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.003345129080116749\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.002336363773792982\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.004415449686348438\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.004562528803944588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "    \n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.savefig(f'../../../results/ABSA/MAMS/plots/bert_ft_do_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998557</td>\n",
       "      <td>0.998557</td>\n",
       "      <td>0.907584</td>\n",
       "      <td>0.998557</td>\n",
       "      <td>0.963008</td>\n",
       "      <td>0.998557</td>\n",
       "      <td>0.933751</td>\n",
       "      <td>1434.817833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998665</td>\n",
       "      <td>0.998665</td>\n",
       "      <td>0.919109</td>\n",
       "      <td>0.998665</td>\n",
       "      <td>0.956070</td>\n",
       "      <td>0.998665</td>\n",
       "      <td>0.936779</td>\n",
       "      <td>1435.621739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998869</td>\n",
       "      <td>0.998869</td>\n",
       "      <td>0.931395</td>\n",
       "      <td>0.998869</td>\n",
       "      <td>0.959647</td>\n",
       "      <td>0.998869</td>\n",
       "      <td>0.945217</td>\n",
       "      <td>1437.770232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998546</td>\n",
       "      <td>0.998546</td>\n",
       "      <td>0.904496</td>\n",
       "      <td>0.998546</td>\n",
       "      <td>0.967929</td>\n",
       "      <td>0.998546</td>\n",
       "      <td>0.934716</td>\n",
       "      <td>1437.730833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.909838</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.957769</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.932813</td>\n",
       "      <td>1437.382670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.998713</td>\n",
       "      <td>0.998713</td>\n",
       "      <td>0.913395</td>\n",
       "      <td>0.998713</td>\n",
       "      <td>0.964741</td>\n",
       "      <td>0.998713</td>\n",
       "      <td>0.938016</td>\n",
       "      <td>1436.981961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.916870</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.958164</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.936869</td>\n",
       "      <td>1436.782191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.925666</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.954150</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.939527</td>\n",
       "      <td>1438.160596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.998712</td>\n",
       "      <td>0.998712</td>\n",
       "      <td>0.921537</td>\n",
       "      <td>0.998712</td>\n",
       "      <td>0.956357</td>\n",
       "      <td>0.998712</td>\n",
       "      <td>0.938464</td>\n",
       "      <td>1436.276098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.998453</td>\n",
       "      <td>0.998453</td>\n",
       "      <td>0.893681</td>\n",
       "      <td>0.998453</td>\n",
       "      <td>0.966312</td>\n",
       "      <td>0.998453</td>\n",
       "      <td>0.927974</td>\n",
       "      <td>1438.753707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.998557               0.998557               0.907584            0.998557   \n",
       "1  0.998665               0.998665               0.919109            0.998665   \n",
       "2  0.998869               0.998869               0.931395            0.998869   \n",
       "3  0.998546               0.998546               0.904496            0.998546   \n",
       "4  0.998588               0.998588               0.909838            0.998588   \n",
       "5  0.998713               0.998713               0.913395            0.998713   \n",
       "6  0.998702               0.998702               0.916870            0.998702   \n",
       "7  0.998766               0.998766               0.925666            0.998766   \n",
       "8  0.998712               0.998712               0.921537            0.998712   \n",
       "9  0.998453               0.998453               0.893681            0.998453   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.963008        0.998557        0.933751     1434.817833  \n",
       "1            0.956070        0.998665        0.936779     1435.621739  \n",
       "2            0.959647        0.998869        0.945217     1437.770232  \n",
       "3            0.967929        0.998546        0.934716     1437.730833  \n",
       "4            0.957769        0.998588        0.932813     1437.382670  \n",
       "5            0.964741        0.998713        0.938016     1436.981961  \n",
       "6            0.958164        0.998702        0.936869     1436.782191  \n",
       "7            0.954150        0.998766        0.939527     1438.160596  \n",
       "8            0.956357        0.998712        0.938464     1436.276098  \n",
       "9            0.966312        0.998453        0.927974     1438.753707  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
