{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_CNN_BiLSTM_Linear import ABSA_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_pre_trained_dropout_cnn_bilstm_lineard_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_pre_trained_dropout_cnn_bilstm_lineard_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3810385465621948\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.11806484311819077\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.06962115317583084\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.07488050311803818\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.043225545436143875\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.08972860872745514\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0416538268327713\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.040256280452013016\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.038887545466423035\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02781376987695694\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.035010747611522675\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.047987200319767\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.030896145850419998\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.026505807414650917\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02999955229461193\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.032928839325904846\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03691432252526283\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.039105843752622604\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.04440140724182129\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.024345263838768005\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.037189383059740067\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02312479168176651\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4254533052444458\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.09346413612365723\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.07124502211809158\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04899629205465317\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07030703127384186\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.030732091516256332\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.027337631210684776\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03722239285707474\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.027962738648056984\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.027295654639601707\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.031196456402540207\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03373457491397858\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.053012218326330185\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.038052190095186234\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.018666930496692657\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.025738678872585297\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03461034968495369\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.03304162621498108\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.026968101039528847\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.026362847536802292\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.014126039110124111\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.026784783229231834\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3838074207305908\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06712137907743454\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05908351019024849\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.10240937769412994\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.06697259098291397\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04797056317329407\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03133363649249077\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.041339211165905\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.034338872879743576\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.035008206963539124\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.032803624868392944\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.049739763140678406\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.034949615597724915\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02810889668762684\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02653777040541172\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.028693370521068573\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.04527932405471802\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.024658795446157455\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.030251972377300262\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03701194003224373\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03253429755568504\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02436171844601631\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4655847549438477\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06871597468852997\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05175527557730675\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03726159781217575\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.035098616033792496\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03602975606918335\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03471672162413597\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.027736641466617584\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.024690240621566772\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02330869808793068\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03700613975524902\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03411465138196945\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.022244995459914207\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.023536065593361855\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03938891366124153\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.020572880282998085\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0382259339094162\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02437022142112255\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.024032210931181908\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01957225427031517\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.05511392652988434\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02252698503434658\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4575557708740234\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.07825229316949844\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.035767875611782074\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04349265620112419\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03121590055525303\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0276668518781662\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03424999862909317\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03767988830804825\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.05238499864935875\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02647310495376587\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03854953125119209\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.029151445254683495\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02456914819777012\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.036297496408224106\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02931884676218033\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04441782459616661\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.035562869161367416\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.03562088683247566\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.027695126831531525\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.016082582995295525\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.04605213180184364\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.03376057371497154\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4080334901809692\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06497728824615479\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03529210388660431\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05063828080892563\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03516019880771637\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03205035999417305\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.05062153562903404\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02883363515138626\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0462801568210125\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02851421758532524\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.030894508585333824\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.05472758412361145\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.027194838970899582\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04344310238957405\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02613372728228569\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.06553445756435394\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.04793769493699074\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.025698957964777946\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.024228699505329132\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03218255564570427\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03312795236706734\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.026240834966301918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3801367282867432\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04888852313160896\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.06096489354968071\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.06252757459878922\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03638799488544464\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.050394680351018906\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02429851144552231\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.030864883214235306\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0326482318341732\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.030942702665925026\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03563748672604561\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02383790723979473\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03582192584872246\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01922081969678402\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.027861153706908226\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.027220452204346657\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.029779627919197083\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.024828799068927765\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.027005096897482872\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.021482914686203003\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03461148217320442\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.024198461323976517\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3446427583694458\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.09002193063497543\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.042237814515829086\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.046878375113010406\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0398983471095562\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.036115292459726334\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.021714452654123306\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.022148655727505684\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.039672717452049255\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.018878474831581116\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02919241413474083\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02390795387327671\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03911144658923149\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02552015148103237\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01745414175093174\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.05604533478617668\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.025512728840112686\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02882523275911808\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.015097424387931824\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02265818975865841\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02485448122024536\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.03698243945837021\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.412337064743042\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.07400830090045929\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0417432002723217\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03927013650536537\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.028258642181754112\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03135087341070175\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03634331375360489\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03470411151647568\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03425613418221474\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.04384107515215874\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.023523833602666855\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03430098295211792\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03274312987923622\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04283100739121437\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03336768224835396\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03776315972208977\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.028508298099040985\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.03901558369398117\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.037960484623909\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.024327179417014122\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0343010276556015\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.03712336719036102\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3755556344985962\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06637553125619888\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03175264969468117\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05559002608060837\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07102678716182709\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.028377745300531387\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.028766024857759476\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03441072627902031\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.07735832035541534\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03816113993525505\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03544941544532776\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.030880508944392204\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.027009498327970505\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.030777709558606148\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.020062755793333054\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.039896659553050995\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0267780851572752\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.017642725259065628\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0275979395955801\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02286149375140667\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.026569517329335213\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.027736442163586617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_CNN_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "    \n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.savefig(f'../../../results/ABSA/MAMS/plots/bert_pt_do_cnn_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990475</td>\n",
       "      <td>0.990475</td>\n",
       "      <td>0.519778</td>\n",
       "      <td>0.990475</td>\n",
       "      <td>0.254938</td>\n",
       "      <td>0.990475</td>\n",
       "      <td>0.258384</td>\n",
       "      <td>1634.117587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.991221</td>\n",
       "      <td>0.991221</td>\n",
       "      <td>0.494974</td>\n",
       "      <td>0.991221</td>\n",
       "      <td>0.321372</td>\n",
       "      <td>0.991221</td>\n",
       "      <td>0.352615</td>\n",
       "      <td>1607.252399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.990807</td>\n",
       "      <td>0.990807</td>\n",
       "      <td>0.460009</td>\n",
       "      <td>0.990807</td>\n",
       "      <td>0.265559</td>\n",
       "      <td>0.990807</td>\n",
       "      <td>0.277826</td>\n",
       "      <td>1614.728349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.991501</td>\n",
       "      <td>0.991501</td>\n",
       "      <td>0.571700</td>\n",
       "      <td>0.991501</td>\n",
       "      <td>0.359086</td>\n",
       "      <td>0.991501</td>\n",
       "      <td>0.381725</td>\n",
       "      <td>1613.170038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.991117</td>\n",
       "      <td>0.991117</td>\n",
       "      <td>0.480494</td>\n",
       "      <td>0.991117</td>\n",
       "      <td>0.304158</td>\n",
       "      <td>0.991117</td>\n",
       "      <td>0.330279</td>\n",
       "      <td>1607.119688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.991199</td>\n",
       "      <td>0.991199</td>\n",
       "      <td>0.497940</td>\n",
       "      <td>0.991199</td>\n",
       "      <td>0.315595</td>\n",
       "      <td>0.991199</td>\n",
       "      <td>0.336799</td>\n",
       "      <td>1608.175640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.991614</td>\n",
       "      <td>0.991614</td>\n",
       "      <td>0.520299</td>\n",
       "      <td>0.991614</td>\n",
       "      <td>0.356965</td>\n",
       "      <td>0.991614</td>\n",
       "      <td>0.378564</td>\n",
       "      <td>1608.077399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.991555</td>\n",
       "      <td>0.991555</td>\n",
       "      <td>0.489681</td>\n",
       "      <td>0.991555</td>\n",
       "      <td>0.321453</td>\n",
       "      <td>0.991555</td>\n",
       "      <td>0.351634</td>\n",
       "      <td>1609.333060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.991022</td>\n",
       "      <td>0.991022</td>\n",
       "      <td>0.456188</td>\n",
       "      <td>0.991022</td>\n",
       "      <td>0.334296</td>\n",
       "      <td>0.991022</td>\n",
       "      <td>0.356058</td>\n",
       "      <td>1609.790253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.991180</td>\n",
       "      <td>0.991180</td>\n",
       "      <td>0.474421</td>\n",
       "      <td>0.991180</td>\n",
       "      <td>0.318962</td>\n",
       "      <td>0.991180</td>\n",
       "      <td>0.332242</td>\n",
       "      <td>1610.190985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.990475               0.990475               0.519778            0.990475   \n",
       "1  0.991221               0.991221               0.494974            0.991221   \n",
       "2  0.990807               0.990807               0.460009            0.990807   \n",
       "3  0.991501               0.991501               0.571700            0.991501   \n",
       "4  0.991117               0.991117               0.480494            0.991117   \n",
       "5  0.991199               0.991199               0.497940            0.991199   \n",
       "6  0.991614               0.991614               0.520299            0.991614   \n",
       "7  0.991555               0.991555               0.489681            0.991555   \n",
       "8  0.991022               0.991022               0.456188            0.991022   \n",
       "9  0.991180               0.991180               0.474421            0.991180   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.254938        0.990475        0.258384     1634.117587  \n",
       "1            0.321372        0.991221        0.352615     1607.252399  \n",
       "2            0.265559        0.990807        0.277826     1614.728349  \n",
       "3            0.359086        0.991501        0.381725     1613.170038  \n",
       "4            0.304158        0.991117        0.330279     1607.119688  \n",
       "5            0.315595        0.991199        0.336799     1608.175640  \n",
       "6            0.356965        0.991614        0.378564     1608.077399  \n",
       "7            0.321453        0.991555        0.351634     1609.333060  \n",
       "8            0.334296        0.991022        0.356058     1609.790253  \n",
       "9            0.318962        0.991180        0.332242     1610.190985  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
