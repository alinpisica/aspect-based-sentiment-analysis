{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_CNN_BiLSTM_Linear import ABSA_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ABSA/MAMS/models/bert_fine_tuned_512.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_fine_tuned_dropout_cnn_bilstm_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_fine_tuned_dropout_cnn_bilstm_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3803099393844604\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0768231526017189\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03971938416361809\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04627726599574089\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.06225328892469406\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.058220360428094864\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.04164021834731102\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02869509346783161\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.025328686460852623\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03221874311566353\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.01737811416387558\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.04337086156010628\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.022622648626565933\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.024187779054045677\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0333479642868042\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.02607640065252781\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.021782856434583664\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.019570453092455864\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.025353794917464256\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03720773756504059\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.01978791318833828\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02218228206038475\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.348757266998291\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06070501357316971\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03447973355650902\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0335453599691391\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.026778221130371094\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03273991122841835\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.031317099928855896\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.027026064693927765\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02554442174732685\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.020244013518095016\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03484835475683212\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.027896923944354057\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.029333051294088364\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02386513538658619\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.020964663475751877\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.02463521808385849\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.017865756526589394\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.019872071221470833\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.021163422614336014\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.019215138629078865\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.018190806731581688\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01862572319805622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4402340650558472\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.1319902241230011\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.07094556093215942\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.042434707283973694\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03480217605829239\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0378134623169899\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.045462146401405334\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0371064729988575\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.036315858364105225\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.032301656901836395\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02697656862437725\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.036575715988874435\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.033089734613895416\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02898130565881729\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.027515701949596405\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.020675258710980415\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.023575514554977417\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.020893922075629234\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02627251297235489\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.031058816239237785\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.020913228392601013\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.023420283570885658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4142146110534668\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06044238805770874\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.041091401129961014\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03636251762509346\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.028049683198332787\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03447793424129486\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02237161062657833\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02564460225403309\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02102893404662609\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01863938197493553\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02164643630385399\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.019479334354400635\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.031192444264888763\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01534025464206934\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02649114839732647\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.014657991006970406\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01967111974954605\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.032611921429634094\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.018232401460409164\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.019381780177354813\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03544735535979271\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.011997873894870281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.340345859527588\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0587984062731266\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03605768829584122\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04394996166229248\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.035256776958703995\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04529440402984619\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.05555619299411774\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.029902007430791855\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02429429441690445\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0500834584236145\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02486586570739746\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06501729786396027\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.026982784271240234\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02136586420238018\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.034475553780794144\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.028757765889167786\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.027234189212322235\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.017305828630924225\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.04182056337594986\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.013925619423389435\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03700237721204758\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.017831088975071907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4504588842391968\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.08682814985513687\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03825804963707924\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03280135244131088\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.02680203504860401\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04336237162351608\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03448481112718582\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.07036522030830383\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.022621147334575653\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01624879240989685\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.021233398467302322\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.034653156995773315\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.027386115863919258\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.015081459656357765\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04100402817130089\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03774920105934143\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03357301279902458\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.022777443751692772\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01577386073768139\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.020136935636401176\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.016928281635046005\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.013483565300703049\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3961124420166016\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.08795900642871857\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05807014927268028\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.041249316185712814\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.02848747931420803\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.054046593606472015\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.050458136945962906\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03555171191692352\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.04639596864581108\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03259028121829033\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03659844771027565\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.030940549448132515\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.032192230224609375\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.019588006660342216\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0501350536942482\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03365405648946762\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03006148897111416\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.021136878058314323\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.020754264667630196\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.043200861662626266\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.013896316289901733\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.015870891511440277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3732680082321167\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06076937913894653\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04994494095444679\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03667077422142029\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.04617001488804817\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03946513310074806\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02843860164284706\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02297278307378292\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.027824850752949715\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.048174191266298294\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02388974279165268\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.027286406606435776\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0478542223572731\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0282632764428854\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04132716730237007\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.016588812693953514\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.024623656645417213\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02166808769106865\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.04033610597252846\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0411171056330204\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.020359976217150688\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02481376938521862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3801532983779907\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.09290043264627457\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.039691053330898285\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05040424317121506\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03913619741797447\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.034096263349056244\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.032583002001047134\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.023557696491479874\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.031479932367801666\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.04015032947063446\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02951274998486042\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02226678468286991\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.022654790431261063\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03305863216519356\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.024952178820967674\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.027399688959121704\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.024063821882009506\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02816598303616047\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.015822891145944595\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02065872959792614\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02215626835823059\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.03623117506504059\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.369490623474121\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.060623202472925186\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04105269908905029\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.033983662724494934\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03138936311006546\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04697730764746666\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.035043131560087204\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.023441625759005547\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.034477587789297104\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02450692653656006\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02878512069582939\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.026469623669981956\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04582741856575012\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.032067108899354935\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04694448411464691\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.020129147917032242\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.029440822079777718\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0185070913285017\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02519741840660572\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01993590220808983\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.014955328777432442\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.030368972569704056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_CNN_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "    \n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.savefig(f'../../../results/ABSA/MAMS/plots/bert_ft_do_cnn_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992334</td>\n",
       "      <td>0.992334</td>\n",
       "      <td>0.588042</td>\n",
       "      <td>0.992334</td>\n",
       "      <td>0.439498</td>\n",
       "      <td>0.992334</td>\n",
       "      <td>0.448710</td>\n",
       "      <td>1616.284519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.991831</td>\n",
       "      <td>0.991831</td>\n",
       "      <td>0.436373</td>\n",
       "      <td>0.991831</td>\n",
       "      <td>0.420946</td>\n",
       "      <td>0.991831</td>\n",
       "      <td>0.398167</td>\n",
       "      <td>1610.596591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.472917</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.376213</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.391380</td>\n",
       "      <td>1610.118611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.991696</td>\n",
       "      <td>0.991696</td>\n",
       "      <td>0.440245</td>\n",
       "      <td>0.991696</td>\n",
       "      <td>0.407540</td>\n",
       "      <td>0.991696</td>\n",
       "      <td>0.417682</td>\n",
       "      <td>1611.414271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992149</td>\n",
       "      <td>0.992149</td>\n",
       "      <td>0.439640</td>\n",
       "      <td>0.992149</td>\n",
       "      <td>0.409157</td>\n",
       "      <td>0.992149</td>\n",
       "      <td>0.400025</td>\n",
       "      <td>1611.455272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.992199</td>\n",
       "      <td>0.992199</td>\n",
       "      <td>0.707548</td>\n",
       "      <td>0.992199</td>\n",
       "      <td>0.408342</td>\n",
       "      <td>0.992199</td>\n",
       "      <td>0.429811</td>\n",
       "      <td>1611.618326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.991987</td>\n",
       "      <td>0.991987</td>\n",
       "      <td>0.435053</td>\n",
       "      <td>0.991987</td>\n",
       "      <td>0.396933</td>\n",
       "      <td>0.991987</td>\n",
       "      <td>0.406565</td>\n",
       "      <td>1611.704263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.991880</td>\n",
       "      <td>0.991880</td>\n",
       "      <td>0.461722</td>\n",
       "      <td>0.991880</td>\n",
       "      <td>0.383317</td>\n",
       "      <td>0.991880</td>\n",
       "      <td>0.382939</td>\n",
       "      <td>1611.732044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.991951</td>\n",
       "      <td>0.991951</td>\n",
       "      <td>0.470718</td>\n",
       "      <td>0.991951</td>\n",
       "      <td>0.398134</td>\n",
       "      <td>0.991951</td>\n",
       "      <td>0.409666</td>\n",
       "      <td>1614.435424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.991604</td>\n",
       "      <td>0.991604</td>\n",
       "      <td>0.457174</td>\n",
       "      <td>0.991604</td>\n",
       "      <td>0.401530</td>\n",
       "      <td>0.991604</td>\n",
       "      <td>0.389872</td>\n",
       "      <td>1612.886770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.992334               0.992334               0.588042            0.992334   \n",
       "1  0.991831               0.991831               0.436373            0.991831   \n",
       "2  0.991800               0.991800               0.472917            0.991800   \n",
       "3  0.991696               0.991696               0.440245            0.991696   \n",
       "4  0.992149               0.992149               0.439640            0.992149   \n",
       "5  0.992199               0.992199               0.707548            0.992199   \n",
       "6  0.991987               0.991987               0.435053            0.991987   \n",
       "7  0.991880               0.991880               0.461722            0.991880   \n",
       "8  0.991951               0.991951               0.470718            0.991951   \n",
       "9  0.991604               0.991604               0.457174            0.991604   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.439498        0.992334        0.448710     1616.284519  \n",
       "1            0.420946        0.991831        0.398167     1610.596591  \n",
       "2            0.376213        0.991800        0.391380     1610.118611  \n",
       "3            0.407540        0.991696        0.417682     1611.414271  \n",
       "4            0.409157        0.992149        0.400025     1611.455272  \n",
       "5            0.408342        0.992199        0.429811     1611.618326  \n",
       "6            0.396933        0.991987        0.406565     1611.704263  \n",
       "7            0.383317        0.991880        0.382939     1611.732044  \n",
       "8            0.398134        0.991951        0.409666     1614.435424  \n",
       "9            0.401530        0.991604        0.389872     1612.886770  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
