{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_BiLSTM_Linear import ABSA_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ABSA/MAMS/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_fine_tuned_dropout_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_fine_tuned_dropout_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3336554765701294\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.08637931197881699\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.13602620363235474\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.07754103094339371\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.09061302244663239\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.07328848540782928\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.04255019128322601\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0979384258389473\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.10406870394945145\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.044342055916786194\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.04861941188573837\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06442344933748245\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.032411232590675354\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02288845181465149\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04603860527276993\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.012880904600024223\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.029135985299944878\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.04947225749492645\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.08500095456838608\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.09381432831287384\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03878472372889519\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.035210225731134415\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4434446096420288\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.19079430401325226\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.08492004871368408\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.20565128326416016\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07734771072864532\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.08044955879449844\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.055164553225040436\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02934456616640091\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.08811149001121521\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.10726931691169739\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.051413394510746\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.045157697051763535\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.07375605404376984\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.10083868354558945\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.06758135557174683\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0303965974599123\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.018302228301763535\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.029822083190083504\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0761839970946312\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.05722293257713318\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.020913779735565186\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0524502769112587\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.394864559173584\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.2101057916879654\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.07232021540403366\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.09875893592834473\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.029850376769900322\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.11810801923274994\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.057419221848249435\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.043721336871385574\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.05164512246847153\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.013748052529990673\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.015668513253331184\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.09883827716112137\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.10127192735671997\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02559822052717209\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.013071409426629543\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.009863538667559624\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.007225609850138426\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.011761209927499294\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.029837269335985184\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.009601275436580181\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03935327008366585\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.04077959433197975\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3496010303497314\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.08556708693504333\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.14526395499706268\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.08812059462070465\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.11929403990507126\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.06770569086074829\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.08393922448158264\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.04510752484202385\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03610553219914436\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.033727992326021194\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.17760202288627625\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03766293451189995\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03958994150161743\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.025540733709931374\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03937535732984543\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04208224266767502\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01993347890675068\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.043185263872146606\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.09888868033885956\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.04140956327319145\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.027865374460816383\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0598728209733963\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3813079595565796\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.11784432083368301\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.11015020310878754\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.06608808785676956\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03811366856098175\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.09289585798978806\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.11181339621543884\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03387556970119476\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.057485807687044144\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.09792080521583557\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02721056155860424\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.04960958659648895\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.047634731978178024\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.05336322635412216\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.05998456850647926\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0357021763920784\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.04771225526928902\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.06172366067767143\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03908989205956459\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03873566910624504\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.1451091319322586\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.06753285974264145\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3381301164627075\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.23990985751152039\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.1407729685306549\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.09898655116558075\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.10686499625444412\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.13436949253082275\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0993073433637619\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.11572311818599701\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.05666845664381981\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.10527771711349487\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.15856659412384033\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.014222033321857452\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.08230064809322357\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.05499676987528801\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.10482355207204819\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03229504078626633\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.039017729461193085\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.007588711101561785\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.05360830947756767\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.018741779029369354\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02263553999364376\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.024573249742388725\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.5289324522018433\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.15317228436470032\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.10974252969026566\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.09857059270143509\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.09821712970733643\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0470292754471302\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.06958874315023422\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.037990011274814606\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.08627033978700638\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.16034886240959167\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.07409032434225082\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.10919997096061707\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03520451486110687\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.05059894919395447\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.06494635343551636\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.00754738412797451\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.020700078457593918\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.04130177199840546\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03256839141249657\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.13227908313274384\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.037558313459157944\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.028508992865681648\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4293335676193237\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.1928185224533081\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.11831725388765335\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.09314823895692825\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.15767863392829895\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.06012637913227081\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.08656632155179977\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.052619583904743195\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.04420829564332962\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.05204432085156441\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.03229176253080368\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.07322824001312256\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.05437714606523514\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03937182202935219\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.07157120108604431\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.07481247186660767\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0622522346675396\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.021432824432849884\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.010021364316344261\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03080856427550316\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03931171074509621\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.020114293321967125\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.368937611579895\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.19622693955898285\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.08498351275920868\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.07000001519918442\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07725405693054199\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04964856430888176\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.11376583576202393\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.05994727835059166\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.04199409857392311\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0530569851398468\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.07188592106103897\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0583387166261673\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.11914772540330887\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0634622797369957\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03022097982466221\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04724518954753876\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.04212949424982071\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.012452872470021248\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.05245159566402435\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03862743452191353\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.06397270411252975\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.03921094909310341\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4277966022491455\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.14569182693958282\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.09508661180734634\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.08142083138227463\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.15752820670604706\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.1087755560874939\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03945139795541763\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.09305959939956665\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03669244796037674\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.11271443963050842\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.05238988995552063\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.05490335449576378\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01766910031437874\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.038099657744169235\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04566814750432968\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.026825357228517532\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01893463172018528\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02506238967180252\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.05982284992933273\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.08675405383110046\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.037355806678533554\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.06252608448266983\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, bilstm_in_features=256, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.989472</td>\n",
       "      <td>0.989472</td>\n",
       "      <td>0.941245</td>\n",
       "      <td>0.989472</td>\n",
       "      <td>0.975720</td>\n",
       "      <td>0.989472</td>\n",
       "      <td>0.958016</td>\n",
       "      <td>477.156030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.988838</td>\n",
       "      <td>0.988838</td>\n",
       "      <td>0.933412</td>\n",
       "      <td>0.988838</td>\n",
       "      <td>0.978002</td>\n",
       "      <td>0.988838</td>\n",
       "      <td>0.954913</td>\n",
       "      <td>435.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.991587</td>\n",
       "      <td>0.991587</td>\n",
       "      <td>0.956949</td>\n",
       "      <td>0.991587</td>\n",
       "      <td>0.974645</td>\n",
       "      <td>0.991587</td>\n",
       "      <td>0.965648</td>\n",
       "      <td>435.151032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.989905</td>\n",
       "      <td>0.989905</td>\n",
       "      <td>0.944794</td>\n",
       "      <td>0.989905</td>\n",
       "      <td>0.970061</td>\n",
       "      <td>0.989905</td>\n",
       "      <td>0.957131</td>\n",
       "      <td>435.005001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.991964</td>\n",
       "      <td>0.991964</td>\n",
       "      <td>0.954584</td>\n",
       "      <td>0.991964</td>\n",
       "      <td>0.977851</td>\n",
       "      <td>0.991964</td>\n",
       "      <td>0.965957</td>\n",
       "      <td>435.411002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.989844</td>\n",
       "      <td>0.989844</td>\n",
       "      <td>0.936903</td>\n",
       "      <td>0.989844</td>\n",
       "      <td>0.972217</td>\n",
       "      <td>0.989844</td>\n",
       "      <td>0.953740</td>\n",
       "      <td>435.573913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.990600</td>\n",
       "      <td>0.990600</td>\n",
       "      <td>0.947744</td>\n",
       "      <td>0.990600</td>\n",
       "      <td>0.972702</td>\n",
       "      <td>0.990600</td>\n",
       "      <td>0.959979</td>\n",
       "      <td>434.405051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.988944</td>\n",
       "      <td>0.988944</td>\n",
       "      <td>0.937864</td>\n",
       "      <td>0.988944</td>\n",
       "      <td>0.974274</td>\n",
       "      <td>0.988944</td>\n",
       "      <td>0.955549</td>\n",
       "      <td>434.590972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.990322</td>\n",
       "      <td>0.990322</td>\n",
       "      <td>0.940830</td>\n",
       "      <td>0.990322</td>\n",
       "      <td>0.978803</td>\n",
       "      <td>0.990322</td>\n",
       "      <td>0.959192</td>\n",
       "      <td>436.977995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.988357</td>\n",
       "      <td>0.988357</td>\n",
       "      <td>0.929172</td>\n",
       "      <td>0.988357</td>\n",
       "      <td>0.976782</td>\n",
       "      <td>0.988357</td>\n",
       "      <td>0.951915</td>\n",
       "      <td>437.477998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.989472               0.989472               0.941245            0.989472   \n",
       "1  0.988838               0.988838               0.933412            0.988838   \n",
       "2  0.991587               0.991587               0.956949            0.991587   \n",
       "3  0.989905               0.989905               0.944794            0.989905   \n",
       "4  0.991964               0.991964               0.954584            0.991964   \n",
       "5  0.989844               0.989844               0.936903            0.989844   \n",
       "6  0.990600               0.990600               0.947744            0.990600   \n",
       "7  0.988944               0.988944               0.937864            0.988944   \n",
       "8  0.990322               0.990322               0.940830            0.990322   \n",
       "9  0.988357               0.988357               0.929172            0.988357   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.975720        0.989472        0.958016      477.156030  \n",
       "1            0.978002        0.988838        0.954913      435.882000  \n",
       "2            0.974645        0.991587        0.965648      435.151032  \n",
       "3            0.970061        0.989905        0.957131      435.005001  \n",
       "4            0.977851        0.991964        0.965957      435.411002  \n",
       "5            0.972217        0.989844        0.953740      435.573913  \n",
       "6            0.972702        0.990600        0.959979      434.405051  \n",
       "7            0.974274        0.988944        0.955549      434.590972  \n",
       "8            0.978803        0.990322        0.959192      436.977995  \n",
       "9            0.976782        0.988357        0.951915      437.477998  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
