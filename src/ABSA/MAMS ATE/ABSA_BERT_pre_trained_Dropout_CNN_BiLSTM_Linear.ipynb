{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_CNN_BiLSTM_Linear import ABSA_BERT_Dropout_CNN_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_pre_trained_dropout_cnn_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_pre_trained_dropout_cnn_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3631865978240967\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.10931700468063354\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.07141556590795517\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.08653442561626434\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.04957825690507889\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0416187085211277\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03671260550618172\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03618067502975464\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03889615461230278\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.038617756217718124\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.021549008786678314\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.05912388116121292\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.033196769654750824\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.036182940006256104\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04589039459824562\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03612091392278671\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0384889580309391\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.04909728094935417\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.023954030126333237\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03986688330769539\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.04443906247615814\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.030024893581867218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3882732391357422\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06157192960381508\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.043841928243637085\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03719068318605423\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03641417250037193\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.029159951955080032\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.042774736881256104\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.04848344996571541\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02744126319885254\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02402782253921032\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.027957776561379433\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.025321662425994873\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.048340052366256714\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0188961923122406\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04096396639943123\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03271034359931946\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0288045946508646\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.03600601106882095\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03724700212478638\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.046760398894548416\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.031449347734451294\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.017716526985168457\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4550366401672363\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.08572451025247574\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.09278484433889389\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05569501966238022\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.04886656254529953\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03916202113032341\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03141012787818909\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.029711689800024033\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03467307239770889\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.03293580934405327\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.04221590235829353\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.05254197493195534\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.024570271372795105\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04170088842511177\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.020970597863197327\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04741717875003815\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02484247088432312\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02572520077228546\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03419916704297066\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02495719864964485\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.024316802620887756\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.022295765578746796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4361921548843384\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.08529439568519592\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05542869120836258\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.053058311343193054\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.04335419461131096\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03710734844207764\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0324615016579628\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.055429600179195404\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03513846546411514\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02990262396633625\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02595461905002594\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.036542199552059174\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.022168118506669998\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02820616029202938\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.043523967266082764\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.029882553964853287\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.029850227758288383\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.023715758696198463\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01905331201851368\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.027691148221492767\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02209426648914814\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.029268180951476097\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3821887969970703\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06438963860273361\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.043918970972299576\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03629732504487038\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.027727169916033745\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04175572097301483\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.04410457983613014\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.0451270155608654\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02826712653040886\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.028017597272992134\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.04533805698156357\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03374262526631355\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.023366069421172142\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03143696114420891\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.051074475049972534\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03683343157172203\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.05202862620353699\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01739843748509884\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03413888439536095\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0223395936191082\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03423988074064255\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.017994750291109085\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3751863241195679\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06288325041532516\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03799547627568245\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03326467424631119\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.034325189888477325\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04354122653603554\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.038699470460414886\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.03535933047533035\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03220434486865997\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.035158392041921616\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.029377406463027\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01944572664797306\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02354283817112446\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.027454402297735214\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02793472446501255\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0313328318297863\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.07356507331132889\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02817583829164505\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02153891511261463\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.017774654552340508\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.024179765954613686\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02054368518292904\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3403291702270508\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.059121664613485336\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.07162316888570786\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.032442837953567505\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.02889317087829113\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04685297980904579\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0320948101580143\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.05086894333362579\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.031180687248706818\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02976338565349579\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.022066788747906685\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.020932110026478767\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02118302881717682\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.02258750982582569\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01792076975107193\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03624424338340759\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.026818828657269478\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02419193461537361\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02164584957063198\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.026086067780852318\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.030996007844805717\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.029190240427851677\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.375952959060669\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06404655426740646\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04654619097709656\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03895771503448486\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03156597167253494\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.05386277288198471\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.0335136279463768\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.07783400267362595\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03192261606454849\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02977488376200199\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02685116045176983\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.030384989455342293\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.025721319019794464\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03505538031458855\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03481036797165871\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.034446150064468384\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.026497352868318558\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.045975737273693085\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.03871195763349533\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.04366843402385712\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.024772003293037415\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.022497665137052536\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3666672706604004\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.07432495057582855\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0688154324889183\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.039724718779325485\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.035307496786117554\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03454373776912689\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.028711138293147087\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.026617197319865227\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.021452613174915314\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.034022606909275055\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02750556543469429\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.029943788424134254\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.029019255191087723\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03100801445543766\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02586381509900093\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04279342293739319\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03396859019994736\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.029839374125003815\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02262856438755989\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03153686225414276\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.04532528296113014\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.019162500277161598\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.354116678237915\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05251184105873108\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.049427781254053116\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04139192774891853\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.035755813121795654\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03312709555029869\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03997936099767685\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.07216227799654007\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03871668130159378\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02907794527709484\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.027934523299336433\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.03672162815928459\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.03807569667696953\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03127885237336159\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02806968055665493\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03135498985648155\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.026508964598178864\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.030192814767360687\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02774767577648163\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02280573360621929\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.029329517856240273\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.020174629986286163\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_CNN_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), bert_seq_len=SEQ_LEN, dropout=0.3, bilstm_in_features=256, conv_out_channels=SEQ_LEN, conv_kernel_size=3, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990441</td>\n",
       "      <td>0.990441</td>\n",
       "      <td>0.442078</td>\n",
       "      <td>0.990441</td>\n",
       "      <td>0.253916</td>\n",
       "      <td>0.990441</td>\n",
       "      <td>0.256493</td>\n",
       "      <td>2099.871010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.517035</td>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.323365</td>\n",
       "      <td>0.991296</td>\n",
       "      <td>0.354625</td>\n",
       "      <td>2027.496546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.991084</td>\n",
       "      <td>0.991084</td>\n",
       "      <td>0.503016</td>\n",
       "      <td>0.991084</td>\n",
       "      <td>0.281170</td>\n",
       "      <td>0.991084</td>\n",
       "      <td>0.301803</td>\n",
       "      <td>1689.650854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.991426</td>\n",
       "      <td>0.991426</td>\n",
       "      <td>0.474129</td>\n",
       "      <td>0.991426</td>\n",
       "      <td>0.331309</td>\n",
       "      <td>0.991426</td>\n",
       "      <td>0.353523</td>\n",
       "      <td>1585.099316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.991547</td>\n",
       "      <td>0.991547</td>\n",
       "      <td>0.722087</td>\n",
       "      <td>0.991547</td>\n",
       "      <td>0.332457</td>\n",
       "      <td>0.991547</td>\n",
       "      <td>0.347023</td>\n",
       "      <td>1582.259509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.991180</td>\n",
       "      <td>0.991180</td>\n",
       "      <td>0.470892</td>\n",
       "      <td>0.991180</td>\n",
       "      <td>0.324101</td>\n",
       "      <td>0.991180</td>\n",
       "      <td>0.355393</td>\n",
       "      <td>1582.108990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.991232</td>\n",
       "      <td>0.991232</td>\n",
       "      <td>0.464236</td>\n",
       "      <td>0.991232</td>\n",
       "      <td>0.305426</td>\n",
       "      <td>0.991232</td>\n",
       "      <td>0.332749</td>\n",
       "      <td>1582.286159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.991531</td>\n",
       "      <td>0.991531</td>\n",
       "      <td>0.428022</td>\n",
       "      <td>0.991531</td>\n",
       "      <td>0.333354</td>\n",
       "      <td>0.991531</td>\n",
       "      <td>0.342430</td>\n",
       "      <td>1582.126265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.991049</td>\n",
       "      <td>0.991049</td>\n",
       "      <td>0.501598</td>\n",
       "      <td>0.991049</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.991049</td>\n",
       "      <td>0.322350</td>\n",
       "      <td>1581.916067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.991440</td>\n",
       "      <td>0.991440</td>\n",
       "      <td>0.597788</td>\n",
       "      <td>0.991440</td>\n",
       "      <td>0.323723</td>\n",
       "      <td>0.991440</td>\n",
       "      <td>0.339569</td>\n",
       "      <td>1657.568510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.990441               0.990441               0.442078            0.990441   \n",
       "1  0.991296               0.991296               0.517035            0.991296   \n",
       "2  0.991084               0.991084               0.503016            0.991084   \n",
       "3  0.991426               0.991426               0.474129            0.991426   \n",
       "4  0.991547               0.991547               0.722087            0.991547   \n",
       "5  0.991180               0.991180               0.470892            0.991180   \n",
       "6  0.991232               0.991232               0.464236            0.991232   \n",
       "7  0.991531               0.991531               0.428022            0.991531   \n",
       "8  0.991049               0.991049               0.501598            0.991049   \n",
       "9  0.991440               0.991440               0.597788            0.991440   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.253916        0.990441        0.256493     2099.871010  \n",
       "1            0.323365        0.991296        0.354625     2027.496546  \n",
       "2            0.281170        0.991084        0.301803     1689.650854  \n",
       "3            0.331309        0.991426        0.353523     1585.099316  \n",
       "4            0.332457        0.991547        0.347023     1582.259509  \n",
       "5            0.324101        0.991180        0.355393     1582.108990  \n",
       "6            0.305426        0.991232        0.332749     1582.286159  \n",
       "7            0.333354        0.991531        0.342430     1582.126265  \n",
       "8            0.305600        0.991049        0.322350     1581.916067  \n",
       "9            0.323723        0.991440        0.339569     1657.568510  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
