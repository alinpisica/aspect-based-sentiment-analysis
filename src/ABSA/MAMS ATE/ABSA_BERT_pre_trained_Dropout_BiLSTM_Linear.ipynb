{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_BiLSTM_Linear import ABSA_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_pre_trained_dropout_bilstm_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_pre_trained_dropout_bilstm_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3831692934036255\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.30869072675704956\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.17631593346595764\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.43559253215789795\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.2408280372619629\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.15919789671897888\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.2964566946029663\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.2248377799987793\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.15100184082984924\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06231212615966797\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.14944525063037872\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.14583805203437805\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.08851128816604614\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.11292742192745209\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.053483009338378906\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.13002857565879822\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.128607839345932\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.24498945474624634\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.065106600522995\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.18872129917144775\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.11447392404079437\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.16477546095848083\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4759585857391357\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.48100560903549194\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.18065574765205383\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.2543410360813141\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.28393879532814026\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.23575694859027863\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.11849767714738846\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.12477166950702667\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.14442892372608185\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.11190393567085266\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.2005370557308197\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.11065888404846191\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.19407416880130768\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.06472842395305634\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.08902226388454437\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.15028463304042816\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.12098583579063416\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.14210891723632812\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.13279195129871368\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.08956592530012131\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.10323601216077805\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.23208409547805786\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3337265253067017\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.22197076678276062\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.18306420743465424\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.15839338302612305\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.2192411869764328\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.2524053454399109\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.13828790187835693\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.3239305019378662\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.22892114520072937\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.060166481882333755\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.1278332620859146\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0847414880990982\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0868595540523529\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.1156730204820633\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.07640337198972702\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.15233533084392548\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0831553265452385\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.08559192717075348\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.048917680978775024\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.11112669110298157\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.07310488820075989\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.08208037167787552\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3476425409317017\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.34406906366348267\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.22021892666816711\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.2665574252605438\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.24211443960666656\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.19143590331077576\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.09970966726541519\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.21082964539527893\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.13509689271450043\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.24593637883663177\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.2859792113304138\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.1447136104106903\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.13690316677093506\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.11347969621419907\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.1338900476694107\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.11203987151384354\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.1366163045167923\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.11544886976480484\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.12056601792573929\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0803542509675026\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.21096941828727722\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.158954918384552\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4122194051742554\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.3194822072982788\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.17873714864253998\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.1518125683069229\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.25390729308128357\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.34383732080459595\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.23269939422607422\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.1674288809299469\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.07744931429624557\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.19811664521694183\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.2439318299293518\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06348944455385208\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.05576172471046448\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.18851980566978455\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.2606141269207001\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.16421936452388763\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.07995370775461197\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.10626109689474106\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0829269215464592\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.09096098691225052\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.11347073316574097\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.08328545093536377\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3311383724212646\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.27310270071029663\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.25105202198028564\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.22578594088554382\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.19081102311611176\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.29852768778800964\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.29257842898368835\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.24930623173713684\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.11232259124517441\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.1588725447654724\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.10662727057933807\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.13013504445552826\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0977243036031723\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.11374133825302124\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.06927459686994553\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.10898562520742416\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.1293291598558426\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.04871136322617531\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.10136735439300537\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.10332781821489334\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0519268624484539\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.13583551347255707\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3191657066345215\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.3966245651245117\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.24273699522018433\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.38783150911331177\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.2595081925392151\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.1885509490966797\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.2371465563774109\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.1565956324338913\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.22171024978160858\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.2385740876197815\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.19829167425632477\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.10984384268522263\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.11190692335367203\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.1324588656425476\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.05910487473011017\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.07108350098133087\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.19968962669372559\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.1553230881690979\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.15380601584911346\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.06747394800186157\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.1410408467054367\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.16584162414073944\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4342572689056396\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.4035191833972931\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.2702668309211731\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.31586962938308716\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.21961478888988495\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.17998471856117249\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.15179169178009033\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.2047586590051651\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.20053713023662567\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.09675406664609909\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.12946021556854248\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.12915092706680298\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.1359308958053589\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.17767393589019775\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.1707441657781601\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.15712039172649384\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.03591582551598549\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.06067797169089317\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.07351243495941162\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.11837800592184067\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0387093611061573\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.04705154150724411\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4010132551193237\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.29287371039390564\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.17841856181621552\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.2047901600599289\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.22498318552970886\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.12871943414211273\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.3412126898765564\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.10283481329679489\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.2686803936958313\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06011749058961868\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.10868853330612183\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.13273723423480988\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.08994609862565994\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.07478327304124832\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.15221557021141052\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.12401708215475082\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.08091305196285248\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.09655004739761353\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.1265864223241806\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.11912443488836288\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0491824746131897\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.10575540363788605\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.402011513710022\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.24587301909923553\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.22544658184051514\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.29617753624916077\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.2308943271636963\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.2446216195821762\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.1266815960407257\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.1642545610666275\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.22503326833248138\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.11712732911109924\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.15355883538722992\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.13131046295166016\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.08723986893892288\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.15804234147071838\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.10650445520877838\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.21313628554344177\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.10025881975889206\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.06826860457658768\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.08664582669734955\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.04705406725406647\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.041899293661117554\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.06919488310813904\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, bilstm_in_features=256, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.969338</td>\n",
       "      <td>0.969338</td>\n",
       "      <td>0.843482</td>\n",
       "      <td>0.969338</td>\n",
       "      <td>0.906365</td>\n",
       "      <td>0.969338</td>\n",
       "      <td>0.872969</td>\n",
       "      <td>499.511999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.965525</td>\n",
       "      <td>0.965525</td>\n",
       "      <td>0.812255</td>\n",
       "      <td>0.965525</td>\n",
       "      <td>0.906358</td>\n",
       "      <td>0.965525</td>\n",
       "      <td>0.852922</td>\n",
       "      <td>492.238224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.969544</td>\n",
       "      <td>0.969544</td>\n",
       "      <td>0.853579</td>\n",
       "      <td>0.969544</td>\n",
       "      <td>0.893324</td>\n",
       "      <td>0.969544</td>\n",
       "      <td>0.872303</td>\n",
       "      <td>497.431457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.967291</td>\n",
       "      <td>0.967291</td>\n",
       "      <td>0.838480</td>\n",
       "      <td>0.967291</td>\n",
       "      <td>0.901756</td>\n",
       "      <td>0.967291</td>\n",
       "      <td>0.867565</td>\n",
       "      <td>498.653585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.967573</td>\n",
       "      <td>0.967573</td>\n",
       "      <td>0.835216</td>\n",
       "      <td>0.967573</td>\n",
       "      <td>0.906720</td>\n",
       "      <td>0.967573</td>\n",
       "      <td>0.868560</td>\n",
       "      <td>487.667144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.970788</td>\n",
       "      <td>0.970788</td>\n",
       "      <td>0.857319</td>\n",
       "      <td>0.970788</td>\n",
       "      <td>0.922418</td>\n",
       "      <td>0.970788</td>\n",
       "      <td>0.888012</td>\n",
       "      <td>440.269203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.965710</td>\n",
       "      <td>0.965710</td>\n",
       "      <td>0.828253</td>\n",
       "      <td>0.965710</td>\n",
       "      <td>0.900834</td>\n",
       "      <td>0.965710</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>488.172769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.962550</td>\n",
       "      <td>0.962550</td>\n",
       "      <td>0.809245</td>\n",
       "      <td>0.962550</td>\n",
       "      <td>0.906574</td>\n",
       "      <td>0.962550</td>\n",
       "      <td>0.853415</td>\n",
       "      <td>488.568213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.969045</td>\n",
       "      <td>0.969045</td>\n",
       "      <td>0.841615</td>\n",
       "      <td>0.969045</td>\n",
       "      <td>0.903127</td>\n",
       "      <td>0.969045</td>\n",
       "      <td>0.870674</td>\n",
       "      <td>484.503408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.969881</td>\n",
       "      <td>0.969881</td>\n",
       "      <td>0.839314</td>\n",
       "      <td>0.969881</td>\n",
       "      <td>0.919748</td>\n",
       "      <td>0.969881</td>\n",
       "      <td>0.876117</td>\n",
       "      <td>477.340663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.969338               0.969338               0.843482            0.969338   \n",
       "1  0.965525               0.965525               0.812255            0.965525   \n",
       "2  0.969544               0.969544               0.853579            0.969544   \n",
       "3  0.967291               0.967291               0.838480            0.967291   \n",
       "4  0.967573               0.967573               0.835216            0.967573   \n",
       "5  0.970788               0.970788               0.857319            0.970788   \n",
       "6  0.965710               0.965710               0.828253            0.965710   \n",
       "7  0.962550               0.962550               0.809245            0.962550   \n",
       "8  0.969045               0.969045               0.841615            0.969045   \n",
       "9  0.969881               0.969881               0.839314            0.969881   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.906365        0.969338        0.872969      499.511999  \n",
       "1            0.906358        0.965525        0.852922      492.238224  \n",
       "2            0.893324        0.969544        0.872303      497.431457  \n",
       "3            0.901756        0.967291        0.867565      498.653585  \n",
       "4            0.906720        0.967573        0.868560      487.667144  \n",
       "5            0.922418        0.970788        0.888012      440.269203  \n",
       "6            0.900834        0.965710        0.861736      488.172769  \n",
       "7            0.906574        0.962550        0.853415      488.568213  \n",
       "8            0.903127        0.969045        0.870674      484.503408  \n",
       "9            0.919748        0.969881        0.876117      477.340663  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
