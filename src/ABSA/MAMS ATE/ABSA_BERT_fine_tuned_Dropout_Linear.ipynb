{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_Linear import ABSA_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ABSA/MAMS/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_fine_tuned_dropout_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_fine_tuned_dropout_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.5455280542373657\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.10102585703134537\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.07099210470914841\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.06743618845939636\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.18148551881313324\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04130833223462105\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.05735116824507713\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.045030977576971054\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.024665703997015953\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.09214869141578674\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.09857123345136642\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.007777941878885031\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.033046115189790726\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.014097967185080051\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03192324936389923\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04130838066339493\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02443869784474373\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.027148425579071045\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.06553737074136734\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.006495527923107147\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03322691470384598\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0062872907146811485\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4970009326934814\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03637510538101196\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.027568327262997627\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.13179804384708405\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.09195652604103088\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.09735674411058426\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.11488567292690277\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.020183123648166656\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.05643947049975395\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.054316964000463486\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.012392540462315083\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.024742484092712402\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.12638835608959198\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.04972296208143234\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.05833349749445915\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0021377899684011936\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0064954692497849464\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.10818029195070267\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.008055265992879868\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.040581975132226944\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.04825802892446518\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.020171072334051132\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.703810453414917\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0997849628329277\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.08105327934026718\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.039271511137485504\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07632216811180115\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03688938543200493\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02114119753241539\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.02726982720196247\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.04439416155219078\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.08714030683040619\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.012131861411035061\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06628528237342834\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02646522969007492\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01818830706179142\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04729903116822243\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.07780922204256058\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.10728666186332703\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006729746703058481\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.005156024359166622\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.013875101692974567\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.017096523195505142\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.030494367703795433\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1122971773147583\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04983869194984436\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0330549031496048\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.053816311061382294\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07279356569051743\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.034955963492393494\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.007536888122558594\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.06177428364753723\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03661800175905228\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0717480406165123\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.038009028881788254\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.14808927476406097\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.020346948876976967\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.025578176602721214\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03880083188414574\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03101680614054203\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.06660670042037964\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.013808037154376507\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.024111537262797356\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.014161055907607079\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.052325621247291565\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.09337423741817474\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 2.0353620052337646\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.020792657509446144\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05874020606279373\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.0906536728143692\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.1023307666182518\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.09299734234809875\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.11265293508768082\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.09311378747224808\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.08788515627384186\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.030441123992204666\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.033242058008909225\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.016983650624752045\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.010925685055553913\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.051983606070280075\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.03509524464607239\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03162524849176407\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0510360412299633\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01754765771329403\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01662302017211914\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.002039635321125388\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.021408481523394585\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.013108033686876297\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0513139963150024\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.08345438539981842\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.039631474763154984\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.05162996053695679\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03875569626688957\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.13925868272781372\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.09768597781658173\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.05786487087607384\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0625326931476593\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.05318004637956619\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.07678115367889404\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.051139719784259796\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.019576532766222954\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.07834898680448532\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04903435334563255\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.03574931249022484\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.07054141163825989\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.011235790327191353\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.10143665224313736\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.007429512217640877\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.07637057453393936\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01400411780923605\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.0929744243621826\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.09074465185403824\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.08476177603006363\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04242240637540817\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0442294217646122\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.10727457702159882\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.05122658982872963\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.029379861429333687\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.1303757280111313\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.031834203749895096\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.09496369957923889\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02170262672007084\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.08486778289079666\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01592394709587097\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.021570924669504166\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.029627535492181778\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.09553240239620209\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.042569853365421295\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.08413984626531601\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.05262838676571846\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.039644598960876465\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.016447700560092926\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.5335028171539307\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.027094678953289986\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.040356121957302094\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.08012279868125916\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07024668157100677\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04311503469944\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.10755738615989685\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.07374712824821472\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.026755189523100853\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.08662112057209015\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.009479902684688568\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.031153034418821335\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0347219780087471\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.004231328144669533\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.040717821568250656\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04378291964530945\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.05776755139231682\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.042408209294080734\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.012448928318917751\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02465902641415596\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.026821047067642212\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.01968478038907051\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 0.961886465549469\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04268907010555267\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05044357851147652\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.04796493425965309\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.07258418202400208\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.039125099778175354\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.033051490783691406\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.06756390631198883\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.06123010069131851\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.028228238224983215\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.026417860761284828\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.048490364104509354\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.11749236285686493\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.08461350202560425\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.04359738528728485\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.06074882298707962\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01613364927470684\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006831620819866657\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.07535082846879959\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.042678751051425934\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.013334031216800213\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0484551377594471\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.1898866891860962\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.19563846290111542\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.05568427965044975\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.08747167140245438\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.1435634046792984\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.021894926205277443\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02951216697692871\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.042114969342947006\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.12617550790309906\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.06386047601699829\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.07551344484090805\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02389579452574253\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.025628961622714996\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.03097139485180378\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.08631768822669983\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.05082361772656441\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.02814944088459015\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.04339263588190079\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.006550594232976437\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.032248981297016144\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.060288578271865845\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0543341301381588\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992613</td>\n",
       "      <td>0.992613</td>\n",
       "      <td>0.962803</td>\n",
       "      <td>0.992613</td>\n",
       "      <td>0.973975</td>\n",
       "      <td>0.992613</td>\n",
       "      <td>0.968336</td>\n",
       "      <td>464.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.944517</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.979396</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.961365</td>\n",
       "      <td>416.577434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.991591</td>\n",
       "      <td>0.991591</td>\n",
       "      <td>0.954819</td>\n",
       "      <td>0.991591</td>\n",
       "      <td>0.973405</td>\n",
       "      <td>0.991591</td>\n",
       "      <td>0.963946</td>\n",
       "      <td>440.190033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992543</td>\n",
       "      <td>0.992543</td>\n",
       "      <td>0.956839</td>\n",
       "      <td>0.992543</td>\n",
       "      <td>0.982128</td>\n",
       "      <td>0.992543</td>\n",
       "      <td>0.969218</td>\n",
       "      <td>405.618003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992367</td>\n",
       "      <td>0.992367</td>\n",
       "      <td>0.964203</td>\n",
       "      <td>0.992367</td>\n",
       "      <td>0.974645</td>\n",
       "      <td>0.992367</td>\n",
       "      <td>0.969345</td>\n",
       "      <td>406.012999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.949835</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.980030</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.964510</td>\n",
       "      <td>393.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.993266</td>\n",
       "      <td>0.993266</td>\n",
       "      <td>0.969144</td>\n",
       "      <td>0.993266</td>\n",
       "      <td>0.974776</td>\n",
       "      <td>0.993266</td>\n",
       "      <td>0.971919</td>\n",
       "      <td>397.856289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.992163</td>\n",
       "      <td>0.992163</td>\n",
       "      <td>0.959331</td>\n",
       "      <td>0.992163</td>\n",
       "      <td>0.979696</td>\n",
       "      <td>0.992163</td>\n",
       "      <td>0.969353</td>\n",
       "      <td>405.157401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.992273</td>\n",
       "      <td>0.992273</td>\n",
       "      <td>0.954830</td>\n",
       "      <td>0.992273</td>\n",
       "      <td>0.975802</td>\n",
       "      <td>0.992273</td>\n",
       "      <td>0.965011</td>\n",
       "      <td>418.478840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.959962</td>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.980314</td>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.969970</td>\n",
       "      <td>451.383970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.992613               0.992613               0.962803            0.992613   \n",
       "1  0.990632               0.990632               0.944517            0.990632   \n",
       "2  0.991591               0.991591               0.954819            0.991591   \n",
       "3  0.992543               0.992543               0.956839            0.992543   \n",
       "4  0.992367               0.992367               0.964203            0.992367   \n",
       "5  0.991582               0.991582               0.949835            0.991582   \n",
       "6  0.993266               0.993266               0.969144            0.993266   \n",
       "7  0.992163               0.992163               0.959331            0.992163   \n",
       "8  0.992273               0.992273               0.954830            0.992273   \n",
       "9  0.992621               0.992621               0.959962            0.992621   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.973975        0.992613        0.968336      464.131000  \n",
       "1            0.979396        0.990632        0.961365      416.577434  \n",
       "2            0.973405        0.991591        0.963946      440.190033  \n",
       "3            0.982128        0.992543        0.969218      405.618003  \n",
       "4            0.974645        0.992367        0.969345      406.012999  \n",
       "5            0.980030        0.991582        0.964510      393.047100  \n",
       "6            0.974776        0.993266        0.971919      397.856289  \n",
       "7            0.979696        0.992163        0.969353      405.157401  \n",
       "8            0.975802        0.992273        0.965011      418.478840  \n",
       "9            0.980314        0.992621        0.969970      451.383970  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
