{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_Linear import ABSA_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_OUTPUT = '../../../results/ABSA/MAMS/models/bert_fine_tuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_pre_trained_dropout_linear.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_pre_trained_dropout_linear.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2526299953460693\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.2885449528694153\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.25791382789611816\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.24232974648475647\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.147434264421463\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0882880836725235\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.1354515105485916\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.08040063083171844\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.09888941794633865\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.1163133829832077\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.19088421761989594\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.07820041477680206\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.1257300078868866\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.11559046804904938\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.20295719802379608\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.10481714457273483\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.09265143424272537\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.2153022736310959\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.06823992729187012\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.03687797114253044\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.11422789096832275\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.04328354820609093\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3375189304351807\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.24999377131462097\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.12909480929374695\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.24412091076374054\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.2875880300998688\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.13713687658309937\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.09037364274263382\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.07854811102151871\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.25643905997276306\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.196878120303154\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.06259071081876755\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0958595722913742\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.05206745117902756\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.14066120982170105\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.06236933171749115\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.12913843989372253\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0442548543214798\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.07283850014209747\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.025645149871706963\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.08081791549921036\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.11667986214160919\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.055560048669576645\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3627949953079224\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.3129556477069855\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.13242149353027344\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.2006109654903412\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.12846672534942627\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.1122320145368576\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.19593828916549683\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.12100567668676376\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.12576162815093994\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.13254095613956451\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.14690648019313812\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.1440526396036148\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.16032594442367554\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.07418917864561081\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0709942877292633\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.047559238970279694\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.17505282163619995\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.13517357409000397\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.16164134442806244\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.13205356895923615\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.08645245432853699\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.07089459896087646\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.5255146026611328\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.22782333195209503\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.20907364785671234\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.15132984519004822\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.22870416939258575\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.13756407797336578\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.15119698643684387\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.1689082831144333\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.07314569503068924\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.20746135711669922\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.06946021318435669\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.14383257925510406\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.17101933062076569\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.05567340925335884\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.09731901437044144\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.20308884978294373\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.12583109736442566\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.09431879967451096\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.2623961865901947\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.09211573749780655\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.16300392150878906\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.1796053946018219\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.494645357131958\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.5937363505363464\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.23503173887729645\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.16981230676174164\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.1561817079782486\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.2752498388290405\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.2155832052230835\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.1717945784330368\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.08771449327468872\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.1993524581193924\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.21169281005859375\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.11117426306009293\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.075229711830616\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0917232558131218\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.1302131861448288\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.04296717792749405\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.07977001368999481\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.1414860486984253\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.11170777678489685\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.22525660693645477\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.11908253282308578\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.09293036162853241\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.287811040878296\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.165397047996521\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.38749924302101135\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.2141561061143875\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.12698215246200562\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.1149856224656105\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.10640440136194229\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.4292336106300354\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.12054489552974701\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.14777003228664398\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.07152947783470154\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.06416037678718567\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.052553702145814896\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.12153353542089462\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.09446508437395096\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.1207205206155777\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.13479925692081451\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.12027699500322342\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.02522975020110607\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.09232889860868454\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.05576201155781746\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.04501887038350105\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.571466326713562\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.3046130836009979\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.1519452929496765\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.3155481219291687\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.1846698671579361\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.11767574399709702\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.28937864303588867\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.17857523262500763\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.2487822026014328\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.12633179128170013\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.13335654139518738\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.11851473897695541\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04352208599448204\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.07714993506669998\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.031944047659635544\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.06524914503097534\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.08279050141572952\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.11922398954629898\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.05386870354413986\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.08140808343887329\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.15296146273612976\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.16756314039230347\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.50091552734375\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.28136900067329407\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.2776910364627838\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.4212469458580017\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.28288406133651733\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.2551549971103668\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.27862027287483215\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.16343724727630615\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.08341775089502335\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.08604379743337631\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.11649656295776367\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.07076561450958252\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.16607365012168884\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.08072018623352051\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.15478865802288055\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.15997731685638428\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.13406434655189514\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0777471661567688\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.16448579728603363\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.06166214123368263\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.1331726312637329\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.06634615361690521\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.428419589996338\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.22488917410373688\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.21533726155757904\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.09850621223449707\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.13390299677848816\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.12011973559856415\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.25259023904800415\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.29996147751808167\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.12272587418556213\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.12523700296878815\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.2756144404411316\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.12336212396621704\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0586387924849987\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.10730982571840286\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.060731999576091766\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.09196586906909943\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0995132252573967\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.053315382450819016\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.1336057484149933\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0822465643286705\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.10937035828828812\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.06520599871873856\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.324198842048645\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.33837059140205383\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.19307850301265717\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.14162318408489227\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.41496384143829346\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.18482811748981476\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.09967201203107834\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.2252100557088852\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.1870366334915161\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.10392902791500092\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.28700363636016846\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.11754060536623001\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.2014608234167099\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.14901800453662872\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.05966226011514664\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.07579650729894638\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.058674462139606476\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.10062818974256516\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.09550353139638901\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.05480137839913368\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.03240993991494179\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.047996118664741516\n"
     ]
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, BERT_FINE_TUNED_OUTPUT)\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973690</td>\n",
       "      <td>0.973690</td>\n",
       "      <td>0.871392</td>\n",
       "      <td>0.973690</td>\n",
       "      <td>0.909867</td>\n",
       "      <td>0.973690</td>\n",
       "      <td>0.889834</td>\n",
       "      <td>462.124252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973332</td>\n",
       "      <td>0.973332</td>\n",
       "      <td>0.869429</td>\n",
       "      <td>0.973332</td>\n",
       "      <td>0.918997</td>\n",
       "      <td>0.973332</td>\n",
       "      <td>0.892897</td>\n",
       "      <td>463.156325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972621</td>\n",
       "      <td>0.972621</td>\n",
       "      <td>0.869215</td>\n",
       "      <td>0.972621</td>\n",
       "      <td>0.914541</td>\n",
       "      <td>0.972621</td>\n",
       "      <td>0.890712</td>\n",
       "      <td>463.697592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.967149</td>\n",
       "      <td>0.967149</td>\n",
       "      <td>0.834844</td>\n",
       "      <td>0.967149</td>\n",
       "      <td>0.911839</td>\n",
       "      <td>0.967149</td>\n",
       "      <td>0.870066</td>\n",
       "      <td>457.009927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973043</td>\n",
       "      <td>0.973043</td>\n",
       "      <td>0.869519</td>\n",
       "      <td>0.973043</td>\n",
       "      <td>0.911786</td>\n",
       "      <td>0.973043</td>\n",
       "      <td>0.889424</td>\n",
       "      <td>457.945434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.973236</td>\n",
       "      <td>0.973236</td>\n",
       "      <td>0.870956</td>\n",
       "      <td>0.973236</td>\n",
       "      <td>0.919035</td>\n",
       "      <td>0.973236</td>\n",
       "      <td>0.893985</td>\n",
       "      <td>463.727643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.827584</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.931433</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.874699</td>\n",
       "      <td>466.763255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.971559</td>\n",
       "      <td>0.971559</td>\n",
       "      <td>0.862795</td>\n",
       "      <td>0.971559</td>\n",
       "      <td>0.920731</td>\n",
       "      <td>0.971559</td>\n",
       "      <td>0.889100</td>\n",
       "      <td>465.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.971458</td>\n",
       "      <td>0.971458</td>\n",
       "      <td>0.861057</td>\n",
       "      <td>0.971458</td>\n",
       "      <td>0.907090</td>\n",
       "      <td>0.971458</td>\n",
       "      <td>0.881851</td>\n",
       "      <td>470.280651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.973858</td>\n",
       "      <td>0.973858</td>\n",
       "      <td>0.871088</td>\n",
       "      <td>0.973858</td>\n",
       "      <td>0.926488</td>\n",
       "      <td>0.973858</td>\n",
       "      <td>0.897417</td>\n",
       "      <td>474.953734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.973690               0.973690               0.871392            0.973690   \n",
       "1  0.973332               0.973332               0.869429            0.973332   \n",
       "2  0.972621               0.972621               0.869215            0.972621   \n",
       "3  0.967149               0.967149               0.834844            0.967149   \n",
       "4  0.973043               0.973043               0.869519            0.973043   \n",
       "5  0.973236               0.973236               0.870956            0.973236   \n",
       "6  0.965870               0.965870               0.827584            0.965870   \n",
       "7  0.971559               0.971559               0.862795            0.971559   \n",
       "8  0.971458               0.971458               0.861057            0.971458   \n",
       "9  0.973858               0.973858               0.871088            0.973858   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.909867        0.973690        0.889834      462.124252  \n",
       "1            0.918997        0.973332        0.892897      463.156325  \n",
       "2            0.914541        0.972621        0.890712      463.697592  \n",
       "3            0.911839        0.967149        0.870066      457.009927  \n",
       "4            0.911786        0.973043        0.889424      457.945434  \n",
       "5            0.919035        0.973236        0.893985      463.727643  \n",
       "6            0.931433        0.965870        0.874699      466.763255  \n",
       "7            0.920731        0.971559        0.889100      465.903500  \n",
       "8            0.907090        0.971458        0.881851      470.280651  \n",
       "9            0.926488        0.973858        0.897417      474.953734  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f74211c8095d4d69bea747ac312f2fd52777f7ee1c791c3155581964756685"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
