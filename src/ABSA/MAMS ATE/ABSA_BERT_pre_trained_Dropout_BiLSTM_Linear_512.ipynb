{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_BiLSTM_Linear import ABSA_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_pre_trained_dropout_bilstm_lineard_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_pre_trained_dropout_bilstm_lineard_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3989964723587036\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.051186900585889816\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04581970348954201\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.037601277232170105\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.018607890233397484\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03269530460238457\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.015670863911509514\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.011706496588885784\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.015482071787118912\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.012338041327893734\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.01471719890832901\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01775175891816616\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01095061656087637\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01135783176869154\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0169964712113142\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.012493333779275417\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.030924564227461815\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.011901117861270905\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.014268591068685055\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.017544258385896683\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.007460349705070257\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.02365766651928425\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2254143953323364\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.10972627997398376\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.037546299397945404\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.02997608296573162\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.017185479402542114\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.017041482031345367\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.013730410486459732\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.014378117397427559\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.029850922524929047\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.008149844594299793\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.010643325746059418\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.011747241020202637\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.032214488834142685\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.013362335041165352\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.011476079002022743\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.011748184449970722\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.010164521634578705\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.028667842969298363\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.019689325243234634\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.01958325505256653\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.013391814194619656\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.014560605399310589\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.407193660736084\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05394158139824867\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.02823598124086857\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.022741561755537987\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.028418265283107758\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.022159487009048462\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03219893202185631\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.028510240837931633\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.03865176811814308\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02715490385890007\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.015788862481713295\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013814142905175686\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02280024252831936\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.014146621339023113\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.02051629312336445\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.01250491663813591\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01528653223067522\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.011495283804833889\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01108070369809866\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0208902508020401\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.02092658169567585\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.00897048320621252\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.437765121459961\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.08725452423095703\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04233332350850105\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03420952335000038\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.030483044683933258\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.034431617707014084\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.020564783364534378\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.025211311876773834\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.014503316953778267\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01330399140715599\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02549181878566742\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.019629372283816338\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.013133397325873375\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.022328956052660942\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.015289039351046085\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.024000560864806175\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.010010926052927971\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.015181046910583973\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.012802977114915848\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.008863676339387894\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.01592196710407734\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.010238459333777428\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4074699878692627\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.09127793461084366\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03210081905126572\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.02696327678859234\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.021271351724863052\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02737320028245449\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.020994946360588074\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.019126906991004944\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.029260702431201935\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.02595030702650547\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02423354797065258\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02856936678290367\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.04054954648017883\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.012049240060150623\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01314771082252264\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.020901812240481377\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.009615510702133179\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.028450584039092064\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.013283565640449524\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.007267532404512167\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.020366640761494637\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.020420806482434273\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.459651231765747\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.04202660173177719\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.042985279113054276\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.039956625550985336\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.017334697768092155\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.01673721708357334\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02242833562195301\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.028135431930422783\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.014551867730915546\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.015983039513230324\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.013292058371007442\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01705973781645298\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0244329571723938\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.024236386641860008\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01554687600582838\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.01459786668419838\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.00904911383986473\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.024024154990911484\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.011700793169438839\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0144667262211442\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.018651628866791725\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.009574344381690025\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4364943504333496\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05168379098176956\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03394697606563568\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.031093358993530273\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.027885429561138153\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.04836142435669899\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02217312529683113\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.034468960016965866\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.014524076133966446\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01148905698210001\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.013423378579318523\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.015679344534873962\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.010690572671592236\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.018981104716658592\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.014954430051147938\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.01257610134780407\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.012975622899830341\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.010249746963381767\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.016347181051969528\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.010080898180603981\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.006137304939329624\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.015218366868793964\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4256720542907715\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05770270153880119\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.032361362129449844\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03502345085144043\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.021350378170609474\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.026255521923303604\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.03462185338139534\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.015328334644436836\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.019261781126260757\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01482012216001749\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.018982209265232086\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.011505373753607273\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.006834798958152533\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.029643604531884193\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.010272277519106865\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.015261058695614338\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01914329268038273\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.008414608426392078\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.005624718964099884\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.014320974238216877\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.008414586074650288\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.009832524694502354\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.5083407163619995\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03775814548134804\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.03370681405067444\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.02761612832546234\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03302557021379471\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.03769112750887871\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.021423345431685448\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.025855088606476784\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.015767280012369156\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.012992528267204762\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.014530953019857407\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.018046097829937935\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.015126248821616173\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.016287177801132202\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.009634265676140785\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.009516101330518723\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.015143629163503647\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.012166743166744709\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.009615197777748108\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.016301259398460388\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.014325239695608616\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.007923767901957035\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2722336053848267\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.06790249794721603\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.033617258071899414\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.023466147482395172\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.024172240868210793\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.022580036893486977\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.015051363036036491\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.015229467302560806\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.014557748101651669\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0334373377263546\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.01520512718707323\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.016934774816036224\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.00832628645002842\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.026953652501106262\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.009706462733447552\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.007468400988727808\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.014860637485980988\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.013593753799796104\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.012406847439706326\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.009989566169679165\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.00929414201527834\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.007746190298348665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_BiLSTM_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, bilstm_in_features=256, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "    \n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.savefig(f'../../../results/ABSA/MAMS/plots/bert_pt_do_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995794</td>\n",
       "      <td>0.995794</td>\n",
       "      <td>0.772965</td>\n",
       "      <td>0.995794</td>\n",
       "      <td>0.862529</td>\n",
       "      <td>0.995794</td>\n",
       "      <td>0.811492</td>\n",
       "      <td>1595.514750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996216</td>\n",
       "      <td>0.996216</td>\n",
       "      <td>0.788374</td>\n",
       "      <td>0.996216</td>\n",
       "      <td>0.871659</td>\n",
       "      <td>0.996216</td>\n",
       "      <td>0.826308</td>\n",
       "      <td>1593.540165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.996075</td>\n",
       "      <td>0.996075</td>\n",
       "      <td>0.771309</td>\n",
       "      <td>0.996075</td>\n",
       "      <td>0.848901</td>\n",
       "      <td>0.996075</td>\n",
       "      <td>0.806808</td>\n",
       "      <td>1595.283751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.996235</td>\n",
       "      <td>0.996235</td>\n",
       "      <td>0.787885</td>\n",
       "      <td>0.996235</td>\n",
       "      <td>0.843890</td>\n",
       "      <td>0.996235</td>\n",
       "      <td>0.813825</td>\n",
       "      <td>1595.031279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.787099</td>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.844247</td>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.812319</td>\n",
       "      <td>1593.649881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.996204</td>\n",
       "      <td>0.996204</td>\n",
       "      <td>0.766569</td>\n",
       "      <td>0.996204</td>\n",
       "      <td>0.860715</td>\n",
       "      <td>0.996204</td>\n",
       "      <td>0.809628</td>\n",
       "      <td>1595.722036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.743078</td>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.795546</td>\n",
       "      <td>1598.468870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.996449</td>\n",
       "      <td>0.996449</td>\n",
       "      <td>0.797532</td>\n",
       "      <td>0.996449</td>\n",
       "      <td>0.837778</td>\n",
       "      <td>0.996449</td>\n",
       "      <td>0.816529</td>\n",
       "      <td>1594.962350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.805585</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.876410</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.838639</td>\n",
       "      <td>1594.095161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.996442</td>\n",
       "      <td>0.996442</td>\n",
       "      <td>0.787731</td>\n",
       "      <td>0.996442</td>\n",
       "      <td>0.874746</td>\n",
       "      <td>0.996442</td>\n",
       "      <td>0.826864</td>\n",
       "      <td>1593.432523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.995794               0.995794               0.772965            0.995794   \n",
       "1  0.996216               0.996216               0.788374            0.996216   \n",
       "2  0.996075               0.996075               0.771309            0.996075   \n",
       "3  0.996235               0.996235               0.787885            0.996235   \n",
       "4  0.996461               0.996461               0.787099            0.996461   \n",
       "5  0.996204               0.996204               0.766569            0.996204   \n",
       "6  0.995617               0.995617               0.743078            0.995617   \n",
       "7  0.996449               0.996449               0.797532            0.996449   \n",
       "8  0.996774               0.996774               0.805585            0.996774   \n",
       "9  0.996442               0.996442               0.787731            0.996442   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.862529        0.995794        0.811492     1595.514750  \n",
       "1            0.871659        0.996216        0.826308     1593.540165  \n",
       "2            0.848901        0.996075        0.806808     1595.283751  \n",
       "3            0.843890        0.996235        0.813825     1595.031279  \n",
       "4            0.844247        0.996461        0.812319     1593.649881  \n",
       "5            0.860715        0.996204        0.809628     1595.722036  \n",
       "6            0.863158        0.995617        0.795546     1598.468870  \n",
       "7            0.837778        0.996449        0.816529     1594.962350  \n",
       "8            0.876410        0.996774        0.838639     1594.095161  \n",
       "9            0.874746        0.996442        0.826864     1593.432523  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
