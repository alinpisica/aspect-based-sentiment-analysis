{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_Linear import ABSA_BERT_Dropout_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_OUTPUT = '../../../results/ABSA/MAMS/models/bert_fine_tuned_512.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_pre_trained_dropout_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_pre_trained_dropout_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.416115641593933\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.023767583072185516\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.04055728763341904\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.007156846113502979\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.04735390841960907\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.009222711436450481\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.023472990840673447\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.011701035313308239\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.010298486799001694\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.015238752588629723\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.012346215546131134\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.016065219417214394\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.006589624099433422\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01175089180469513\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.007044200319796801\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.006776570342481136\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.016653714701533318\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006623073946684599\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.011777987703680992\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.010882250964641571\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.00837006326764822\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.008219941519200802\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.434044361114502\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.026882024481892586\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.02996402233839035\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.025140633806586266\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.03435293585062027\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.012341855093836784\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.011684522964060307\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.014168002642691135\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.012888545170426369\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.013488597236573696\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.007341758348047733\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.01365024782717228\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02229158766567707\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.009572457522153854\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.01172605063766241\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.016106313094496727\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004506360273808241\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.014590132981538773\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.008175885304808617\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.005656330846250057\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.017342504113912582\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.014253049157559872\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3188862800598145\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.02159733511507511\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.016605401411652565\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.029832877218723297\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.020013555884361267\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.017350053414702415\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.01982295699417591\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.028057921677827835\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02567264623939991\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.023154212161898613\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.014774573966860771\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.011945796199142933\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01067393273115158\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01849888265132904\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.0148535231128335\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.011035692878067493\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.009497941471636295\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006447807885706425\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.007645842153578997\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.009287686087191105\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.007607095409184694\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.013670604676008224\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3575215339660645\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05157271772623062\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0318712443113327\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.027845125645399094\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.013174621388316154\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.027563748881220818\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.02810780704021454\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.020631056278944016\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.036039385944604874\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.015453641302883625\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.028053971007466316\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0072747087106108665\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02337317168712616\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01692351885139942\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.006914626341313124\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.007201626896858215\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.010672176256775856\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006414460949599743\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.011394788511097431\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.00737293204292655\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.004859185311943293\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.008299142122268677\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2007657289505005\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.026367101818323135\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0214906744658947\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.01617773249745369\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.015571603551506996\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.016081297770142555\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.01880991831421852\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.007974415086209774\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.008311992511153221\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01236505713313818\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.010030685923993587\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.0076179965399205685\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.0076379720121622086\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.00975250918418169\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.009906121529638767\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.011397520080208778\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.006728329695761204\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.01225959975272417\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.009230297990143299\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.00618457468226552\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0030517918057739735\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0036129425279796124\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.5785142183303833\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.05336606502532959\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.021891212090849876\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.02643408440053463\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.023904386907815933\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.00971264112740755\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.021731559187173843\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.018501147627830505\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.009132255800068378\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01701280288398266\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.008190215565264225\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.007521140389144421\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.02683236449956894\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.013912501744925976\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.016028575599193573\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.010115448385477066\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.01156609132885933\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.02162478305399418\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01354575902223587\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.02223707176744938\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.01012458000332117\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.004088266286998987\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4208024740219116\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03284938260912895\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.023436399176716805\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.019585948437452316\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.013624697923660278\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.0261934082955122\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.040814511477947235\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.016995524987578392\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.014654249884188175\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01626446843147278\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.012332538142800331\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.013896359130740166\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.011830326169729233\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01392458938062191\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.009950011968612671\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.020466338843107224\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.007924441248178482\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.020117731764912605\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.01470690593123436\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.009145689196884632\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.008723808452486992\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.00680841039866209\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.646239161491394\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.0458395853638649\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.02094065025448799\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.026028934866189957\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.043606020510196686\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.019163809716701508\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.012129929848015308\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.017911644652485847\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.02820557914674282\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.012676459737122059\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.02143855020403862\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.012902694754302502\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01760033331811428\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.012947558425366879\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.010026355274021626\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.009351243264973164\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.005636125802993774\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.015371140092611313\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.012290260754525661\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0071857827715575695\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.011907734908163548\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.015602520667016506\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3805875778198242\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.024078592658042908\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.027801400050520897\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.024105634540319443\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.028708599507808685\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.02283484674990177\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.030354823917150497\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.014459507539868355\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.04404585063457489\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0359005443751812\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.011352079920470715\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.00587877631187439\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.017049048095941544\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.01329963468015194\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.005542663857340813\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.008796608075499535\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.027952119708061218\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006741776131093502\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0115458182990551\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.011772708967328072\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.010177281685173512\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.007341152988374233\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4151227474212646\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.041296351701021194\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.0336492620408535\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.03201315924525261\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.020459018647670746\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.016965335234999657\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.027498502284288406\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.028577903285622597\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.023310653865337372\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.015538928098976612\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.005948712583631277\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.008241250179708004\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.01646760106086731\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.015221607871353626\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.004331372678279877\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.009268290363252163\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.024460047483444214\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006433241534978151\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.009637763723731041\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.011594357900321484\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.016602005809545517\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.004185121972113848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_Linear(BertModel.from_pretrained('bert-base-uncased'), dropout=0.3, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "    \n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.savefig(f'../../../results/ABSA/MAMS/plots/bert_pt_do_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.bert, BERT_FINE_TUNED_OUTPUT)\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.996994</td>\n",
       "      <td>0.996994</td>\n",
       "      <td>0.838570</td>\n",
       "      <td>0.996994</td>\n",
       "      <td>0.875333</td>\n",
       "      <td>0.996994</td>\n",
       "      <td>0.856020</td>\n",
       "      <td>1482.374664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.811909</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.891605</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.848475</td>\n",
       "      <td>1442.701033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997205</td>\n",
       "      <td>0.997205</td>\n",
       "      <td>0.841765</td>\n",
       "      <td>0.997205</td>\n",
       "      <td>0.877980</td>\n",
       "      <td>0.997205</td>\n",
       "      <td>0.859144</td>\n",
       "      <td>1442.824428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.996892</td>\n",
       "      <td>0.996892</td>\n",
       "      <td>0.810081</td>\n",
       "      <td>0.996892</td>\n",
       "      <td>0.902223</td>\n",
       "      <td>0.996892</td>\n",
       "      <td>0.852215</td>\n",
       "      <td>1441.767841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.996617</td>\n",
       "      <td>0.996617</td>\n",
       "      <td>0.809211</td>\n",
       "      <td>0.996617</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>0.996617</td>\n",
       "      <td>0.854058</td>\n",
       "      <td>1442.243243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.996910</td>\n",
       "      <td>0.996910</td>\n",
       "      <td>0.854435</td>\n",
       "      <td>0.996910</td>\n",
       "      <td>0.832235</td>\n",
       "      <td>0.996910</td>\n",
       "      <td>0.842122</td>\n",
       "      <td>1441.566827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.996906</td>\n",
       "      <td>0.996906</td>\n",
       "      <td>0.828040</td>\n",
       "      <td>0.996906</td>\n",
       "      <td>0.888249</td>\n",
       "      <td>0.996906</td>\n",
       "      <td>0.856319</td>\n",
       "      <td>1442.615625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.997012</td>\n",
       "      <td>0.997012</td>\n",
       "      <td>0.832308</td>\n",
       "      <td>0.997012</td>\n",
       "      <td>0.883518</td>\n",
       "      <td>0.997012</td>\n",
       "      <td>0.854664</td>\n",
       "      <td>1441.468551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.996675</td>\n",
       "      <td>0.996675</td>\n",
       "      <td>0.812887</td>\n",
       "      <td>0.996675</td>\n",
       "      <td>0.865251</td>\n",
       "      <td>0.996675</td>\n",
       "      <td>0.837636</td>\n",
       "      <td>1440.953821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.829250</td>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.895961</td>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.859759</td>\n",
       "      <td>1441.345638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.996994               0.996994               0.838570            0.996994   \n",
       "1  0.996897               0.996897               0.811909            0.996897   \n",
       "2  0.997205               0.997205               0.841765            0.997205   \n",
       "3  0.996892               0.996892               0.810081            0.996892   \n",
       "4  0.996617               0.996617               0.809211            0.996617   \n",
       "5  0.996910               0.996910               0.854435            0.996910   \n",
       "6  0.996906               0.996906               0.828040            0.996906   \n",
       "7  0.997012               0.997012               0.832308            0.997012   \n",
       "8  0.996675               0.996675               0.812887            0.996675   \n",
       "9  0.996981               0.996981               0.829250            0.996981   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.875333        0.996994        0.856020     1482.374664  \n",
       "1            0.891605        0.996897        0.848475     1442.701033  \n",
       "2            0.877980        0.997205        0.859144     1442.824428  \n",
       "3            0.902223        0.996892        0.852215     1441.767841  \n",
       "4            0.908100        0.996617        0.854058     1442.243243  \n",
       "5            0.832235        0.996910        0.842122     1441.566827  \n",
       "6            0.888249        0.996906        0.856319     1442.615625  \n",
       "7            0.883518        0.997012        0.854664     1441.468551  \n",
       "8            0.865251        0.996675        0.837636     1440.953821  \n",
       "9            0.895961        0.996981        0.859759     1441.345638  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
