{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from InputDataset import InputDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from absa_models.ABSA_BERT_Dropout_BiLSTM_Linear import ABSA_BERT_Dropout_BiLSTM_Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory: 8 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ABSA_MAMS_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json.load(open(DATASET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>absa_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>[The, decor, is, not, special, at, all, but, t...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>[when, tables, opened, up, ,, the, manager, sa...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The decor is not special at all but their food...   \n",
       "1  The decor is not special at all but their food...   \n",
       "2  The decor is not special at all but their food...   \n",
       "3  when tables opened up, the manager sat another...   \n",
       "4  when tables opened up, the manager sat another...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, decor, is, not, special, at, all, but, t...   \n",
       "1  [The, decor, is, not, special, at, all, but, t...   \n",
       "2  [The, decor, is, not, special, at, all, but, t...   \n",
       "3  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "4  [when, tables, opened, up, ,, the, manager, sa...   \n",
       "\n",
       "                                           absa_tags  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, ...  \n",
       "3            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4            [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NO_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_FINE_TUNED_PATH = '../../../results/ABSA/MAMS/models/bert_fine_tuned_512.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT = '../../../results/ABSA/MAMS/models/bert_fine_tuned_dropout_bilstm_linear_512.pth'\n",
    "STATS_OUTPUT = '../../../results/ABSA/MAMS/stats/bert_fine_tuned_dropout_bilstm_linear_512.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(ids_tensors[0])), 0)(ids_tensors[0])\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True).to(device)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors[0] = torch.nn.ConstantPad1d((0, SEQ_LEN - len(tags_tensors[0])), 0)(tags_tensors[0])\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long).to(device)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1).to(device)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    dataloader_len = len(dataloader)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _,data in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids_tensors, tags_tensors, masks_tensors = data\n",
    "\n",
    "        outputs = model(ids_tensors, masks_tensors)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, 4), tags_tensors.view(-1))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if _ % (dataloader_len // 10) == 0:\n",
    "            print(f\"Epoch: {epoch}/{EPOCHS}, Batch: {_}/{dataloader_len}, Loss: {loss.item()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids_tensors, tags_tensors, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(device)\n",
    "            tags_tensors = tags_tensors.to(device)\n",
    "            masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "            outputs = model(ids_tensors, masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            fin_outputs += list([int(p) for pred in predictions for p in pred])\n",
    "            fin_targets += list([int(tag) for tags_tensor in tags_tensors for tag in tags_tensor])\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['accuracy','precision_score_micro','precision_score_macro','recall_score_micro','recall_score_macro','f1_score_micro','f1_score_macro', 'execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3756438493728638\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03791637718677521\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.01581696607172489\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.011949518695473671\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.01785103976726532\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.013424722477793694\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.013390550389885902\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.016701027750968933\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.004123473074287176\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.011402109637856483\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.005645963829010725\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.008947817608714104\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.005234266631305218\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0026117926463484764\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.004476022440940142\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.007618011441081762\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.005058764014393091\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.00832890160381794\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.004095661919564009\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.002314563374966383\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.005335908383131027\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0087056215852499\n",
      "Run 2/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.2438709735870361\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.030721131712198257\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.014600617811083794\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.015723278746008873\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.0212003942579031\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.009118779562413692\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.007883007638156414\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.014101680368185043\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.010712460614740849\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.005899384617805481\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.0076243337243795395\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.010099555365741253\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.008496821857988834\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.004388763103634119\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.005610997322946787\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.009049559943377972\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.013037816621363163\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.00949921552091837\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.0050065782852470875\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.004652818664908409\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.0035782300401479006\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.004346185829490423\n",
      "Run 3/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.5476166009902954\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.037408992648124695\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.01518436148762703\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.01597565971314907\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.011723934672772884\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.013806648552417755\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.020978640764951706\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.013180532492697239\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.012520510703325272\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.006937691941857338\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.010621431283652782\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.009324794635176659\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.007589648477733135\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.009020758792757988\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.012410533614456654\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0043312665075063705\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0025294723454862833\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.00570645509287715\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.003526694141328335\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.005979903507977724\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.003559361444786191\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.005708134267479181\n",
      "Run 4/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4960310459136963\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03311140462756157\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.029813779518008232\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.014667592942714691\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.011615295894443989\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.007881402038037777\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.017835792154073715\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.009767532348632812\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.00609571672976017\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.008311158046126366\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.009730515070259571\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.008891145698726177\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.00592305650934577\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.007272324059158564\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.010187041945755482\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.006174731999635696\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.00640706904232502\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006173625122755766\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.007054946385324001\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.006986105814576149\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.01550954021513462\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.005166549235582352\n",
      "Run 5/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4932889938354492\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.037466373294591904\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.021323779597878456\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.019693603739142418\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.01635805331170559\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.01779908314347267\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.010306848213076591\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.008101148530840874\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.0072799017652869225\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.007363911718130112\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.006087802350521088\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.008347872644662857\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.003234854666516185\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.0036079632118344307\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.005130257457494736\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.014464041218161583\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.0029449898283928633\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.004440563730895519\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.005100779701024294\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.006689455825835466\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.003868671366944909\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.004816838074475527\n",
      "Run 6/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.289080023765564\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.027964597567915916\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.022869640961289406\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.010588428005576134\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.010381476953625679\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.015169734135270119\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.011817431077361107\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.008511360734701157\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.016133246943354607\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.006628483068197966\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.006557990796864033\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.007116623688489199\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.008594322949647903\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.002625980181619525\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.003130665048956871\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.008237693458795547\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.009296105243265629\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.002941116690635681\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.007910482585430145\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0036457811947911978\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.006246326025575399\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.0062766307964921\n",
      "Run 7/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.4556888341903687\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.03756697103381157\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.016757836565375328\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.018341688439249992\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.016201449558138847\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.010022501461207867\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.014442858286201954\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.007918464951217175\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.014454233460128307\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0068074543960392475\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.006025850772857666\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.02373776212334633\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.00582753773778677\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.009546542540192604\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.005016184411942959\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.010065448470413685\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.004985906649380922\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.012630954384803772\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.007654279936105013\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.006154023110866547\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.007957176305353642\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.009190495125949383\n",
      "Run 8/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3994317054748535\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.036340437829494476\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.027453085407614708\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.01800091378390789\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.012147965840995312\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.007737434469163418\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.011619112454354763\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.011178561486303806\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.012475159950554371\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.0033168636728078127\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.012071787379682064\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.008531402796506882\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.005203376989811659\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.00510500930249691\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.009133064188063145\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.005973257124423981\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.017472218722105026\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.006311677861958742\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.008564502000808716\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0038802488707005978\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.012990317307412624\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.005230882205069065\n",
      "Run 9/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.3501956462860107\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.023770703002810478\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.021931318566203117\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.018258364871144295\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.011197814717888832\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.01667892187833786\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.013814494013786316\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.013119939714670181\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.009046372957527637\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.005390954669564962\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.00605876324698329\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.011493407189846039\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.00415349705144763\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.006959730293601751\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.00758094759657979\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.0033293645828962326\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.005155075341463089\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.0027956601697951555\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.004762194585055113\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.011413476429879665\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.003652707440778613\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.003251685295253992\n",
      "Run 10/10\n",
      "Epoch: 0/2, Batch: 0/2237, Loss: 1.276262879371643\n",
      "Epoch: 0/2, Batch: 223/2237, Loss: 0.02160954289138317\n",
      "Epoch: 0/2, Batch: 446/2237, Loss: 0.016375575214624405\n",
      "Epoch: 0/2, Batch: 669/2237, Loss: 0.013225319795310497\n",
      "Epoch: 0/2, Batch: 892/2237, Loss: 0.009098188020288944\n",
      "Epoch: 0/2, Batch: 1115/2237, Loss: 0.010113043710589409\n",
      "Epoch: 0/2, Batch: 1338/2237, Loss: 0.008294861763715744\n",
      "Epoch: 0/2, Batch: 1561/2237, Loss: 0.006479702889919281\n",
      "Epoch: 0/2, Batch: 1784/2237, Loss: 0.007985740900039673\n",
      "Epoch: 0/2, Batch: 2007/2237, Loss: 0.01420255471020937\n",
      "Epoch: 0/2, Batch: 2230/2237, Loss: 0.012946018949151039\n",
      "Epoch: 1/2, Batch: 0/2237, Loss: 0.008161681704223156\n",
      "Epoch: 1/2, Batch: 223/2237, Loss: 0.005487839691340923\n",
      "Epoch: 1/2, Batch: 446/2237, Loss: 0.004995779134333134\n",
      "Epoch: 1/2, Batch: 669/2237, Loss: 0.003765517147257924\n",
      "Epoch: 1/2, Batch: 892/2237, Loss: 0.007513912860304117\n",
      "Epoch: 1/2, Batch: 1115/2237, Loss: 0.010270464234054089\n",
      "Epoch: 1/2, Batch: 1338/2237, Loss: 0.016274917870759964\n",
      "Epoch: 1/2, Batch: 1561/2237, Loss: 0.004706850741058588\n",
      "Epoch: 1/2, Batch: 1784/2237, Loss: 0.0078016966581344604\n",
      "Epoch: 1/2, Batch: 2007/2237, Loss: 0.002221760107204318\n",
      "Epoch: 1/2, Batch: 2230/2237, Loss: 0.002105000661686063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(NO_RUNS):\n",
    "    # clear cache cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Run {i + 1}/{NO_RUNS}\")\n",
    "\n",
    "    train_dataset = df.sample(frac=TRAIN_SPLIT)\n",
    "    test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = InputDataset(train_dataset, tokenizer)\n",
    "    testing_set = InputDataset(test_dataset, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        training_set,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = TRAIN_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        testing_set,\n",
    "        sampler = SequentialSampler(testing_set),\n",
    "        batch_size = VALID_BATCH_SIZE,\n",
    "        drop_last = True,\n",
    "        collate_fn=create_mini_batch\n",
    "    )\n",
    "\n",
    "    model = ABSA_BERT_Dropout_BiLSTM_Linear(torch.load(BERT_FINE_TUNED_PATH), dropout=0.3, bilstm_in_features=256, no_out_labels=4, device=device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = train(epoch, model, loss_fn, optimizer, train_dataloader)\n",
    "\n",
    "        train_losses += losses\n",
    "    \n",
    "    plt.title(f'Train Loss for run {i + 1}/{NO_RUNS}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.savefig(f'../../../results/ABSA/MAMS/plots/bert_ft_do_bilstm_lin/train_loss_run_{i + 1}.png')\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    outputs, targets = validation(model, validation_dataloader)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, outputs)\n",
    "    precision_score_micro = precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    results.loc[i] = [accuracy,precision_score_micro,precision_score_macro,recall_score_micro,recall_score_macro,f1_score_micro,f1_score_macro, execution_time]\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, MODEL_OUTPUT)\n",
    "\n",
    "    del train_dataset\n",
    "    del test_dataset\n",
    "    del training_set\n",
    "    del testing_set\n",
    "    del model\n",
    "    del loss_fn\n",
    "    del optimizer\n",
    "    del outputs\n",
    "    del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>recall_score_micro</th>\n",
       "      <th>recall_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998560</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>0.905641</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>0.960182</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>0.931797</td>\n",
       "      <td>1582.908360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998354</td>\n",
       "      <td>0.998354</td>\n",
       "      <td>0.889387</td>\n",
       "      <td>0.998354</td>\n",
       "      <td>0.958459</td>\n",
       "      <td>0.998354</td>\n",
       "      <td>0.921768</td>\n",
       "      <td>1585.678306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998277</td>\n",
       "      <td>0.998277</td>\n",
       "      <td>0.883886</td>\n",
       "      <td>0.998277</td>\n",
       "      <td>0.948774</td>\n",
       "      <td>0.998277</td>\n",
       "      <td>0.914724</td>\n",
       "      <td>1585.441738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998635</td>\n",
       "      <td>0.998635</td>\n",
       "      <td>0.921840</td>\n",
       "      <td>0.998635</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>0.998635</td>\n",
       "      <td>0.930964</td>\n",
       "      <td>1584.645123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998260</td>\n",
       "      <td>0.998260</td>\n",
       "      <td>0.877391</td>\n",
       "      <td>0.998260</td>\n",
       "      <td>0.960848</td>\n",
       "      <td>0.998260</td>\n",
       "      <td>0.915963</td>\n",
       "      <td>1584.638819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.998428</td>\n",
       "      <td>0.998428</td>\n",
       "      <td>0.894380</td>\n",
       "      <td>0.998428</td>\n",
       "      <td>0.953210</td>\n",
       "      <td>0.998428</td>\n",
       "      <td>0.922461</td>\n",
       "      <td>1584.429439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.879600</td>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.952311</td>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.913039</td>\n",
       "      <td>1585.094118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.998452</td>\n",
       "      <td>0.998452</td>\n",
       "      <td>0.897714</td>\n",
       "      <td>0.998452</td>\n",
       "      <td>0.945328</td>\n",
       "      <td>0.998452</td>\n",
       "      <td>0.920475</td>\n",
       "      <td>1584.311128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.998432</td>\n",
       "      <td>0.998432</td>\n",
       "      <td>0.888305</td>\n",
       "      <td>0.998432</td>\n",
       "      <td>0.961689</td>\n",
       "      <td>0.998432</td>\n",
       "      <td>0.922548</td>\n",
       "      <td>1583.135825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.998572</td>\n",
       "      <td>0.998572</td>\n",
       "      <td>0.903771</td>\n",
       "      <td>0.998572</td>\n",
       "      <td>0.947487</td>\n",
       "      <td>0.998572</td>\n",
       "      <td>0.924525</td>\n",
       "      <td>1582.900379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision_score_micro  precision_score_macro  recall_score_micro  \\\n",
       "0  0.998560               0.998560               0.905641            0.998560   \n",
       "1  0.998354               0.998354               0.889387            0.998354   \n",
       "2  0.998277               0.998277               0.883886            0.998277   \n",
       "3  0.998635               0.998635               0.921840            0.998635   \n",
       "4  0.998260               0.998260               0.877391            0.998260   \n",
       "5  0.998428               0.998428               0.894380            0.998428   \n",
       "6  0.998305               0.998305               0.879600            0.998305   \n",
       "7  0.998452               0.998452               0.897714            0.998452   \n",
       "8  0.998432               0.998432               0.888305            0.998432   \n",
       "9  0.998572               0.998572               0.903771            0.998572   \n",
       "\n",
       "   recall_score_macro  f1_score_micro  f1_score_macro  execution_time  \n",
       "0            0.960182        0.998560        0.931797     1582.908360  \n",
       "1            0.958459        0.998354        0.921768     1585.678306  \n",
       "2            0.948774        0.998277        0.914724     1585.441738  \n",
       "3            0.940349        0.998635        0.930964     1584.645123  \n",
       "4            0.960848        0.998260        0.915963     1584.638819  \n",
       "5            0.953210        0.998428        0.922461     1584.429439  \n",
       "6            0.952311        0.998305        0.913039     1585.094118  \n",
       "7            0.945328        0.998452        0.920475     1584.311128  \n",
       "8            0.961689        0.998432        0.922548     1583.135825  \n",
       "9            0.947487        0.998572        0.924525     1582.900379  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(STATS_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7b70e5470c1caae179898b9726485c258ababa2b5a8278be4bb974e3d5af9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
